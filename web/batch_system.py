# â”€â”€ web/batch_system.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°°ì¹˜ ì‹œìŠ¤í…œ + ë°°ì¹˜ ì²´ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
# arm_server.py P5 ë¦¬íŒ©í† ë§ìœ¼ë¡œ ë¶„ë¦¬ (2026-02-28)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import asyncio
import json
import logging
import os
import sys
from datetime import datetime

from state import app_state
from config_loader import KST, MODEL_MAX_TOKENS_MAP, _log, logger
from db import (
    create_task, get_today_cost, load_setting,
    save_activity_log, save_archive, save_setting, update_task,
)
from ws_manager import wm

try:
    from ai_handler import (
        ask_ai, batch_check, batch_retrieve, batch_submit,
        batch_submit_grouped, get_available_providers, select_model,
    )
except ImportError:
    pass

from fastapi import APIRouter, Request

batch_router = APIRouter(tags=["batch"])


def _ms():
    """arm_server ëª¨ë“ˆ ì°¸ì¡° (ìˆœí™˜ import ë°©ì§€)."""
    return sys.modules.get("arm_server") or sys.modules.get("web.arm_server")


# â”€â”€ ë°°ì¹˜ ëª¨ë“œ ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ‘ë¯¸ì‚¬ (ë„êµ¬ í˜¸ì¶œ ë°©ì§€) â”€â”€
_BATCH_MODE_SUFFIX = (
    "\n\n[ë°°ì¹˜ ëª¨ë“œ ì•ˆë‚´] ì´ ìš”ì²­ì€ ë°°ì¹˜ ì²˜ë¦¬ì…ë‹ˆë‹¤. "
    "ë„êµ¬(í•¨ìˆ˜)ë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
    "ë³´ìœ í•œ ì§€ì‹ê³¼ ë¶„ì„ ëŠ¥ë ¥ë§Œìœ¼ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. "
    "ì½”ë“œ ë¸”ë¡ì´ë‚˜ í•¨ìˆ˜ í˜¸ì¶œ í˜•íƒœ(ì˜ˆ: await, function() ë“±)ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”."
)


# â”€â”€ ë°°ì¹˜ ëª…ë ¹ (ì—¬ëŸ¬ ëª…ë ¹ í•œë²ˆì— ì‹¤í–‰) â”€â”€

# â†’ app_stateë¡œ ì´ë™. alias (listëŠ” ê³µìœ  ì°¸ì¡°)
_batch_queue = app_state.batch_queue
_batch_api_queue = app_state.batch_api_queue
# app_state.batch_runningì€ primitive â†’ app_state.batch_running ì§ì ‘ ì‚¬ìš©


@batch_router.get("/api/batch/queue")
async def get_batch_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ ì¡°íšŒ."""
    return {"queue": _batch_queue, "running": app_state.batch_running}


@batch_router.post("/api/batch")
async def submit_batch(request: Request):
    """ë°°ì¹˜ ëª…ë ¹ ì œì¶œ â€” ì—¬ëŸ¬ ëª…ë ¹ì„ í•œë²ˆì— ì ‘ìˆ˜í•©ë‹ˆë‹¤."""
    body = await request.json()
    commands = body.get("commands", [])
    mode = body.get("mode", "sequential")  # sequential ë˜ëŠ” parallel

    if not commands:
        return {"success": False, "error": "ëª…ë ¹ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

    batch_id = f"batch_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}"
    batch_items = []
    for i, cmd in enumerate(commands):
        item = {
            "batch_id": batch_id,
            "index": i,
            "command": cmd if isinstance(cmd, str) else cmd.get("command", ""),
            "status": "pending",
            "result": None,
            "task_id": None,
        }
        batch_items.append(item)
        _batch_queue.append(item)

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°°ì¹˜ ì‹¤í–‰
    asyncio.create_task(_run_batch(batch_id, batch_items, mode))

    return {"success": True, "batch_id": batch_id, "count": len(commands), "mode": mode}


async def _run_batch(batch_id: str, items: list, mode: str):
    """ë°°ì¹˜ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""

    app_state.batch_running = True

    try:
        if mode == "parallel":
            # ë³‘ë ¬ ì‹¤í–‰
            tasks = []
            for item in items:
                tasks.append(_run_batch_item(item))
            await asyncio.gather(*tasks)
        else:
            # ìˆœì°¨ ì‹¤í–‰
            for item in items:
                await _run_batch_item(item)
    finally:
        app_state.batch_running = False
        # ì™„ë£Œëœ ë°°ì¹˜ í•­ëª©ì€ 10ë¶„ í›„ ì •ë¦¬
        await asyncio.sleep(600)
        for item in items:
            if item in _batch_queue:
                _batch_queue.remove(item)


async def _run_batch_item(item: dict):
    """ë°°ì¹˜ ë‚´ ê°œë³„ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    item["status"] = "running"
    try:
        task = create_task(item["command"], source="batch")
        item["task_id"] = task["task_id"]

        # AI ì²˜ë¦¬
        result = await _ms()._process_ai_command(item["command"], task["task_id"])

        item["status"] = "completed"
        item["result"] = result.get("content", "")[:200] if isinstance(result, dict) else str(result)[:200]
        # R-3: ì „ë ¥ë¶„ì„ ë°ì´í„°ìš© agent_id ê¸°ë¡
        agent_id = result.get("agent_id", "chief_of_staff") if isinstance(result, dict) else "chief_of_staff"
        update_task(task["task_id"], agent_id=agent_id)
    except Exception as e:
        item["status"] = "failed"
        item["result"] = str(e)[:200]


@batch_router.delete("/api/batch/queue")
async def clear_batch_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ì„ ë¹„ì›ë‹ˆë‹¤."""
    _batch_queue[:] = [item for item in _batch_queue if item.get("status") == "running"]
    return {"success": True}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€ AI Batch API ì‹œìŠ¤í…œ (PENDING ì¶”ì  + ìë™ ê²°ê³¼ ìˆ˜ì§‘) â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# CEOê°€ ì—¬ëŸ¬ ëª…ë ¹ì„ AI Batch APIë¡œ ë³´ë‚´ë©´:
#   1) ê° ëª…ë ¹ì´ PENDING ìƒíƒœë¡œ DBì— ì €ì¥ë¨
#   2) í”„ë¡œë°”ì´ë”ì˜ Batch APIì— í•œêº¼ë²ˆì— ì œì¶œ (ì‹¤ì‹œê°„ë³´ë‹¤ ~50% ì €ë ´)
#   3) ë°±ê·¸ë¼ìš´ë“œ í´ëŸ¬ê°€ 60ì´ˆë§ˆë‹¤ ìƒíƒœë¥¼ í™•ì¸
#   4) ì™„ë£Œë˜ë©´ ìë™ìœ¼ë¡œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„í•˜ì—¬ ë³´ê³ ì„œ ì‘ì„±
#   5) WebSocketìœ¼ë¡œ CEOì—ê²Œ ì‹¤ì‹œê°„ ì•Œë¦¼

# app_state.batch_poller_task â†’ app_state.batch_poller_task ì§ì ‘ ì‚¬ìš©


@batch_router.post("/api/batch/ai")
async def submit_ai_batch(request: Request):
    """AI Batch APIë¡œ ì—¬ëŸ¬ ìš”ì²­ì„ í•œêº¼ë²ˆì— ì œì¶œí•©ë‹ˆë‹¤.

    ìš”ì²­ body:
    {
        "requests": [
            {"message": "ì‚¼ì„±ì „ì ë¶„ì„í•´ì¤˜", "system_prompt": "...", "agent_id": "cio_manager"},
            {"message": "íŠ¹í—ˆ ê²€ìƒ‰í•´ì¤˜", "system_prompt": "...", "agent_id": "clo_manager"},
        ],
        "model": "claude-sonnet-4-6",  // ê¸°ë³¸ ëª¨ë¸ (ì„ íƒ)
        "auto_delegate": true  // ê²°ê³¼ë¥¼ ì—ì´ì „íŠ¸ì—ê²Œ ìë™ ìœ„ì„í• ì§€ (ê¸°ë³¸: true)
    }

    ì‘ë‹µ: {"batch_id": "...", "count": N, "status": "submitted"}
    """
    body = await request.json()
    requests_list = body.get("requests", [])
    model = body.get("model")
    auto_delegate = body.get("auto_delegate", True)

    if not requests_list:
        return {"success": False, "error": "ìš”ì²­ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

    # ê° ìš”ì²­ì— custom_id ìë™ ë¶€ì—¬
    now_str = datetime.now(KST).strftime('%Y%m%d_%H%M%S')
    for i, req in enumerate(requests_list):
        if "custom_id" not in req:
            req["custom_id"] = f"batch_{now_str}_{i}"
        # ì—ì´ì „íŠ¸ ì†Œìš¸(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)ì„ ìë™ìœ¼ë¡œ ë¡œë“œ
        agent_id = req.get("agent_id")
        if agent_id and not req.get("system_prompt"):
            req["system_prompt"] = _ms()._load_agent_prompt(agent_id)

    # Batch API ì œì¶œ
    result = await batch_submit(requests_list, model=model)

    if "error" in result:
        return {"success": False, "error": result["error"]}

    batch_id = result["batch_id"]
    provider = result["provider"]

    # DBì— PENDING ìƒíƒœë¡œ ì €ì¥
    pending_data = {
        "batch_id": batch_id,
        "provider": provider,
        "model": model,
        "status": "pending",
        "auto_delegate": auto_delegate,
        "submitted_at": datetime.now(KST).isoformat(),
        "requests": [
            {
                "custom_id": r.get("custom_id"),
                "message": r.get("message", "")[:200],
                "agent_id": r.get("agent_id", ""),
            }
            for r in requests_list
        ],
        "results": [],
    }

    # ê¸°ì¡´ pending_batches ëª©ë¡ì— ì¶”ê°€
    pending_batches = load_setting("pending_batches") or []
    pending_batches.append(pending_data)
    save_setting("pending_batches", pending_batches)

    # ê° ìš”ì²­ì„ taskë¡œë„ ìƒì„± (PENDING ìƒíƒœ)
    for req in requests_list:
        task = create_task(
            req.get("message", "ë°°ì¹˜ ìš”ì²­"),
            source="batch_api",
            agent_id=req.get("agent_id", "chief_of_staff"),
        )
        update_task(task["task_id"], status="pending",
                    result_summary=f"[PENDING] ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ (batch_id: {batch_id[:20]}...)")

    # WebSocket ì•Œë¦¼
    await wm.broadcast("batch_submitted", {
        "batch_id": batch_id,
        "provider": provider,
        "count": len(requests_list),
    })

    _log(f"[BATCH] AI ë°°ì¹˜ ì œì¶œ ì™„ë£Œ: {batch_id} ({len(requests_list)}ê°œ ìš”ì²­, {provider})")

    # í´ëŸ¬ê°€ ì•ˆ ëŒê³  ìˆìœ¼ë©´ ì‹œì‘
    _ensure_batch_poller()

    return {
        "success": True,
        "batch_id": batch_id,
        "provider": provider,
        "count": len(requests_list),
        "status": "submitted",
    }


@batch_router.get("/api/batch/pending")
async def get_pending_batches():
    """PENDING ìƒíƒœì¸ ë°°ì¹˜ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    pending_batches = load_setting("pending_batches") or []
    # pendingê³¼ processingë§Œ ë°˜í™˜
    active = [b for b in pending_batches if b.get("status") in ("pending", "processing")]
    return {"pending": active, "total": len(pending_batches)}


@batch_router.post("/api/batch/check/{batch_id}")
async def check_batch_status(batch_id: str):
    """íŠ¹ì • ë°°ì¹˜ì˜ ìƒíƒœë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤."""
    pending_batches = load_setting("pending_batches") or []
    batch_info = next((b for b in pending_batches if b["batch_id"] == batch_id), None)

    if not batch_info:
        return {"error": f"ë°°ì¹˜ '{batch_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

    provider = batch_info["provider"]
    status_result = await batch_check(batch_id, provider)

    if "error" in status_result:
        return status_result

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    batch_info["status"] = status_result["status"]
    batch_info["progress"] = status_result.get("progress", {})
    save_setting("pending_batches", pending_batches)

    # ì™„ë£Œë˜ì—ˆìœ¼ë©´ ê²°ê³¼ ìˆ˜ì§‘
    if status_result["status"] == "completed":
        await _collect_batch_results(batch_info, pending_batches)

    return status_result


@batch_router.post("/api/batch/resume")
async def resume_all_pending():
    """ëª¨ë“  PENDING ë°°ì¹˜ì˜ ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , ì™„ë£Œëœ ê²ƒì€ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    pending_batches = load_setting("pending_batches") or []
    active = [b for b in pending_batches if b.get("status") in ("pending", "processing")]

    if not active:
        return {"message": "ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤", "checked": 0}

    checked = 0
    collected = 0
    for batch_info in active:
        batch_id = batch_info["batch_id"]
        provider = batch_info["provider"]

        status_result = await batch_check(batch_id, provider)
        if "error" not in status_result:
            batch_info["status"] = status_result["status"]
            batch_info["progress"] = status_result.get("progress", {})
            checked += 1

            if status_result["status"] == "completed":
                await _collect_batch_results(batch_info, pending_batches)
                collected += 1

    save_setting("pending_batches", pending_batches)
    return {"checked": checked, "collected": collected, "remaining": len(active) - collected}


@batch_router.get("/api/batch/history")
async def get_batch_history():
    """ëª¨ë“  ë°°ì¹˜ì˜ íˆìŠ¤í† ë¦¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì™„ë£Œëœ ê²ƒ í¬í•¨)."""
    all_batches = load_setting("pending_batches") or []
    return {"batches": all_batches[-50:], "total": len(all_batches)}  # ìµœê·¼ 50ê°œë§Œ


async def _collect_batch_results(batch_info: dict, all_batches: list):
    """ì™„ë£Œëœ ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³ , í•„ìš”ì‹œ ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„í•©ë‹ˆë‹¤."""
    batch_id = batch_info["batch_id"]
    provider = batch_info["provider"]

    _log(f"[BATCH] ê²°ê³¼ ìˆ˜ì§‘ ì‹œì‘: {batch_id}")

    # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    result = await batch_retrieve(batch_id, provider)
    if "error" in result:
        _log(f"[BATCH] ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {result['error']}")
        return

    results = result.get("results", [])
    batch_info["results"] = results
    batch_info["status"] = "completed"
    batch_info["completed_at"] = datetime.now(KST).isoformat()

    # ì´ ë¹„ìš© ê³„ì‚°
    total_cost = sum(r.get("cost_usd", 0) for r in results if r.get("cost_usd"))
    batch_info["total_cost_usd"] = round(total_cost, 6)

    save_setting("pending_batches", all_batches)

    # ì—ì´ì „íŠ¸ì—ê²Œ ìë™ ìœ„ì„ (auto_delegate=trueì¸ ê²½ìš°)
    if batch_info.get("auto_delegate"):
        req_map = {r["custom_id"]: r for r in batch_info.get("requests", [])}
        for res in results:
            if res.get("error"):
                continue
            custom_id = res.get("custom_id", "")
            req_info = req_map.get(custom_id, {})
            agent_id = req_info.get("agent_id")
            message = req_info.get("message", "")

            if agent_id and res.get("content"):
                # ê²°ê³¼ë¥¼ í™œë™ ë¡œê·¸ì— ê¸°ë¡
                agent_name = _ms()._AGENT_NAMES.get(agent_id, agent_id)
                log_entry = save_activity_log(
                    agent_id,
                    f"[ë°°ì¹˜ ì™„ë£Œ] {agent_name}: {message[:40]}... â†’ {res['content'][:60]}..."
                )
                await wm.send_activity_log(log_entry)

                # ì•„ì¹´ì´ë¸Œì— ì €ì¥
                division = _ms()._AGENT_DIVISION.get(agent_id, "secretary")
                now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
                archive_content = f"# [ë°°ì¹˜] [{agent_name}] {message[:60]}\n\n{res['content']}"
                save_archive(
                    division=division,
                    filename=f"batch_{agent_id}_{now_str}.md",
                    content=archive_content,
                    agent_id=agent_id,
                )

    # WebSocketìœ¼ë¡œ ì™„ë£Œ ì•Œë¦¼
    await wm.broadcast("batch_completed", {
        "batch_id": batch_id,
        "provider": provider,
        "count": len(results),
        "total_cost_usd": total_cost,
        "succeeded": sum(1 for r in results if not r.get("error")),
        "failed": sum(1 for r in results if r.get("error")),
    })

    _log(f"[BATCH] ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ: {batch_id} ({len(results)}ê°œ, ${total_cost:.4f})")


async def _flush_batch_api_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ì— ìŒ“ì¸ ìš”ì²­ì„ Batch APIì— ì œì¶œí•©ë‹ˆë‹¤."""
    if not _batch_api_queue:
        return {"message": "ëŒ€ê¸°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

    queue_copy = list(_batch_api_queue)
    _batch_api_queue.clear()

    _log(f"[BATCH] ëŒ€ê¸°ì—´ {len(queue_copy)}ê±´ â†’ Batch API ì œì¶œ ì¤‘...")

    # ê° ìš”ì²­ì— ì—ì´ì „íŠ¸ ë¼ìš°íŒ… (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê²°ì •)
    for req in queue_copy:
        if not req.get("system_prompt"):
            routing = await _ms()._route_task(req.get("message", ""))
            agent_id = routing.get("agent_id", "chief_of_staff")
            req["agent_id"] = agent_id
            req["system_prompt"] = _ms()._load_agent_prompt(agent_id)

    # Batch API ì œì¶œ â€” í”„ë¡œë°”ì´ë”ë³„ë¡œ ìë™ ê·¸ë£¹í™” (Claude/GPT/Gemini ìš”ì²­ì´ ì„ì—¬ë„ ê°ê° ì˜¬ë°”ë¥¸ APIë¡œ ì „ì†¡)
    batch_results = await batch_submit_grouped(queue_copy)

    # ì „ë¶€ ì‹¤íŒ¨í•œ ê²½ìš° ëŒ€ê¸°ì—´ì— ë³µêµ¬
    all_failed = all("error" in br for br in batch_results)
    if all_failed:
        first_error = batch_results[0].get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜") if batch_results else "ê²°ê³¼ ì—†ìŒ"
        _log(f"[BATCH] ì œì¶œ ì‹¤íŒ¨ (ì „ì²´): {first_error}")
        _batch_api_queue.extend(queue_copy)
        return {"error": first_error}

    # ì„±ê³µí•œ ë°°ì¹˜ë“¤ì„ DBì— PENDING ìƒíƒœë¡œ ì €ì¥
    pending_batches = load_setting("pending_batches") or []
    submitted_ids = []
    for result in batch_results:
        if "error" in result:
            _log(f"[BATCH] í”„ë¡œë°”ì´ë” {result.get('provider','?')} ì œì¶œ ì‹¤íŒ¨: {result['error']}")
            continue

        batch_id = result["batch_id"]
        provider = result["provider"]
        custom_ids_in_batch = result.get("custom_ids", [])

        # ì´ ë°°ì¹˜ì— í¬í•¨ëœ ìš”ì²­ë§Œ í•„í„°ë§
        reqs_in_batch = [r for r in queue_copy if r.get("custom_id", r.get("task_id", "")) in custom_ids_in_batch]

        pending_data = {
            "batch_id": batch_id,
            "provider": provider,
            "status": "pending",
            "auto_delegate": True,
            "submitted_at": datetime.now(KST).isoformat(),
            "requests": [
                {
                    "custom_id": r.get("custom_id", r.get("task_id", "")),
                    "message": r.get("message", "")[:200],
                    "agent_id": r.get("agent_id", ""),
                    "task_id": r.get("task_id", ""),
                }
                for r in reqs_in_batch
            ],
            "results": [],
        }
        pending_batches.append(pending_data)
        submitted_ids.append(batch_id)

        # ê° taskë¥¼ PENDING ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
        for req in reqs_in_batch:
            task_id = req.get("task_id")
            if task_id:
                update_task(task_id, status="pending",
                            result_summary=f"[PENDING] Batch API ì œì¶œë¨ ({batch_id[:20]}...)")

        # WebSocket ì•Œë¦¼
        await wm.broadcast("batch_submitted", {"batch_id": batch_id, "provider": provider, "count": len(reqs_in_batch)})

        _log(f"[BATCH] Batch API ì œì¶œ ì™„ë£Œ: {batch_id} ({len(reqs_in_batch)}ê±´, {provider})")

    save_setting("pending_batches", pending_batches)
    _ensure_batch_poller()

    # ì²« ë²ˆì§¸ ì„±ê³µ ê²°ê³¼ë¥¼ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
    first_success = next((r for r in batch_results if "error" not in r), batch_results[0] if batch_results else {})
    return first_success


@batch_router.post("/api/batch/flush")
async def flush_batch_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ì— ìŒ“ì¸ ìš”ì²­ì„ ì¦‰ì‹œ Batch APIì— ì œì¶œí•©ë‹ˆë‹¤."""
    if not _batch_api_queue:
        return {"success": False, "message": "ëŒ€ê¸°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}
    result = await _flush_batch_api_queue()
    return {"success": "error" not in result, **result}


def _ensure_batch_poller():
    """ë°°ì¹˜ í´ëŸ¬ê°€ ëŒê³  ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì•ˆ ëŒë©´ ì‹œì‘í•©ë‹ˆë‹¤."""

    if app_state.batch_poller_task is None or app_state.batch_poller_task.done():
        app_state.batch_poller_task = asyncio.create_task(_batch_poller_loop())
        _log("[BATCH] ë°°ì¹˜ í´ëŸ¬ ì‹œì‘ë¨ (60ì´ˆ ê°„ê²©)")


async def _batch_poller_loop():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ 60ì´ˆë§ˆë‹¤ PENDING ë°°ì¹˜ + ë°°ì¹˜ ì²´ì¸ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    while True:
        try:
            await asyncio.sleep(60)

            has_work = False

            # â”€â”€ (A) ê¸°ì¡´ ë‹¨ë… ë°°ì¹˜ í™•ì¸ â”€â”€
            pending_batches = load_setting("pending_batches") or []
            active = [b for b in pending_batches if b.get("status") in ("pending", "processing")]

            if active:
                has_work = True
                for batch_info in active:
                    batch_id = batch_info["batch_id"]
                    provider = batch_info["provider"]

                    try:
                        status_result = await batch_check(batch_id, provider)
                        if "error" not in status_result:
                            batch_info["status"] = status_result["status"]
                            batch_info["progress"] = status_result.get("progress", {})

                            if status_result["status"] == "completed":
                                await _collect_batch_results(batch_info, pending_batches)
                            elif status_result["status"] in ("failed", "expired"):
                                batch_info["status"] = status_result["status"]
                                _log(f"[BATCH] ë°°ì¹˜ ì‹¤íŒ¨/ë§Œë£Œ: {batch_id}")
                    except Exception as e:
                        _log(f"[BATCH] ë°°ì¹˜ í™•ì¸ ì‹¤íŒ¨ ({batch_id}): {e}")

                save_setting("pending_batches", pending_batches)

            # â”€â”€ (B) ë°°ì¹˜ ì²´ì¸ í™•ì¸ + ìë™ ì§„í–‰ â”€â”€
            chains = load_setting("batch_chains") or []
            active_chains = [c for c in chains if c.get("status") in ("running", "pending")]

            if active_chains:
                has_work = True
                for chain in active_chains:
                    try:
                        await _advance_batch_chain(chain["chain_id"])
                    except Exception as e:
                        _log(f"[CHAIN] ì²´ì¸ ì§„í–‰ ì˜¤ë¥˜ ({chain['chain_id']}): {e}")

            if not has_work:
                _log("[BATCH] ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜/ì²´ì¸ ì—†ìŒ â€” í´ëŸ¬ ì¢…ë£Œ")
                break

        except asyncio.CancelledError:
            break
        except Exception as e:
            _log(f"[BATCH] í´ëŸ¬ ì˜¤ë¥˜: {e}")
            await asyncio.sleep(30)  # ì—ëŸ¬ ì‹œ 30ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€ ë°°ì¹˜ ì²´ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# CEOê°€ ğŸ“¦ ë°°ì¹˜ ëª¨ë“œë¡œ ëª…ë ¹ì„ ë³´ë‚´ë©´ ìœ„ì„ ì²´ì¸ ì „ì²´ê°€ Batch APIë¡œ ëŒì•„ê°:
#
#   [1ë‹¨ê³„] ë¹„ì„œì‹¤ì¥ ë¶„ë¥˜ â†’ Batch ì œì¶œ â†’ PENDING â†’ ê²°ê³¼: "CIOì—ê²Œ ìœ„ì„"
#   [2ë‹¨ê³„] ì „ë¬¸ê°€ Nëª… â†’ í”„ë¡œë°”ì´ë”ë³„ ë¬¶ì–´ì„œ Batch ì œì¶œ â†’ PENDING â†’ ì „ë¶€ ëŒ€ê¸°
#   [3ë‹¨ê³„] íŒ€ì¥ ì¢…í•©ë³´ê³ ì„œ â†’ Batch ì œì¶œ â†’ PENDING â†’ ê²°ê³¼: ì¢…í•© ë³´ê³ ì„œ
#   [4ë‹¨ê³„] CEOì—ê²Œ ì „ë‹¬ + ì•„ì¹´ì´ë¸Œ ì €ì¥
#
# ë§¤ ë‹¨ê³„ë§ˆë‹¤ Batch API ì‚¬ìš© â†’ ë¹„ìš© ~50% ì ˆê°
# í”„ë¡œë°”ì´ë”ë³„ ìë™ ê·¸ë£¹í™” (Claude + GPT + Gemini ì—ì´ì „íŠ¸ í˜¼í•© ê°€ëŠ¥)

# ë¶„ë¥˜ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë°°ì¹˜ ì²´ì¸ì—ì„œ ì‚¬ìš©)
_BATCH_CLASSIFY_PROMPT = """ë‹¹ì‹ ì€ ì—…ë¬´ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
CEOì˜ ëª…ë ¹ì„ ì½ê³  ì–´ëŠ ë¶€ì„œê°€ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

## ë¶€ì„œ ëª©ë¡
- cto_manager: ê¸°ìˆ ê°œë°œ (ì½”ë“œ, ì›¹ì‚¬ì´íŠ¸, API, ì„œë²„, ë°°í¬, í”„ë¡ íŠ¸ì—”ë“œ, ë°±ì—”ë“œ, ë²„ê·¸, UI, ë””ìì¸, ë°ì´í„°ë² ì´ìŠ¤)
- cso_manager: ì‚¬ì—…ê¸°íš (ì‹œì¥ì¡°ì‚¬, ì‚¬ì—…ê³„íš, ë§¤ì¶œ ì˜ˆì¸¡, ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸, ìˆ˜ìµ, ê²½ìŸì‚¬)
- clo_manager: ë²•ë¬´IP (ì €ì‘ê¶Œ, íŠ¹í—ˆ, ìƒí‘œ, ì•½ê´€, ê³„ì•½, ë²•ë¥ , ì†Œì†¡)
- cmo_manager: ë§ˆì¼€íŒ…ê³ ê° (ë§ˆì¼€íŒ…, ê´‘ê³ , SNS, ì¸ìŠ¤íƒ€ê·¸ë¨, ìœ íŠœë¸Œ, ì½˜í…ì¸ , ë¸Œëœë”©, ì„¤ë¬¸)
- cio_manager: íˆ¬ìë¶„ì„ (ì£¼ì‹, íˆ¬ì, ì¢…ëª©, ì‹œí™©, í¬íŠ¸í´ë¦¬ì˜¤, ì½”ìŠ¤í”¼, ë‚˜ìŠ¤ë‹¥, ì°¨íŠ¸, ê¸ˆë¦¬)
- cpo_manager: ì¶œíŒê¸°ë¡ (íšŒì‚¬ê¸°ë¡, ì—°ëŒ€ê¸°, ë¸”ë¡œê·¸, ì¶œíŒ, í¸ì§‘, íšŒê³ , ë¹Œë”©ë¡œê·¸)
- chief_of_staff: ì¼ë°˜ ì§ˆë¬¸, ìš”ì•½, ì¼ì • ê´€ë¦¬, ê¸°íƒ€ (ìœ„ ë¶€ì„œì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°)

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
{"agent_id": "ë¶€ì„œID", "reason": "í•œì¤„ ì´ìœ "}"""


def _save_chain(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ ìƒíƒœë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    chains = load_setting("batch_chains") or []
    # ê°™ì€ chain_idê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
    found = False
    for i, c in enumerate(chains):
        if c["chain_id"] == chain["chain_id"]:
            chains[i] = chain
            found = True
            break
    if not found:
        chains.append(chain)
    # ìµœê·¼ 50ê°œë§Œ ìœ ì§€ (ì˜¤ë˜ëœ ì™„ë£Œ/ì‹¤íŒ¨ ì²´ì¸ ì •ë¦¬)
    if len(chains) > 50:
        active = [c for c in chains if c.get("status") in ("running", "pending")]
        done = [c for c in chains if c.get("status") not in ("running", "pending")]
        chains = active + done[-20:]
    save_setting("batch_chains", chains)


def _load_chain(chain_id: str) -> dict | None:
    """DBì—ì„œ ë°°ì¹˜ ì²´ì¸ ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    chains = load_setting("batch_chains") or []
    for c in chains:
        if c["chain_id"] == chain_id:
            return c
    return None


async def _broadcast_chain_status(chain: dict, message: str):
    """ë°°ì¹˜ ì²´ì¸ ì§„í–‰ ìƒí™©ì„ WebSocketìœ¼ë¡œ CEOì—ê²Œ ì•Œë¦½ë‹ˆë‹¤."""
    step_labels = {
        "classify": "1ë‹¨ê³„: ë¶„ë¥˜",
        "delegation": "2ë‹¨ê³„: íŒ€ì¥ ì§€ì‹œì„œ",
        "specialists": "3ë‹¨ê³„: ì „ë¬¸ê°€ ë¶„ì„",
        "synthesis": "4ë‹¨ê³„: ì¢…í•© ë³´ê³ ì„œ",
        "completed": "ì™„ë£Œ",
        "failed": "ì‹¤íŒ¨",
        "direct": "ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬",
    }
    step_label = step_labels.get(chain.get("step", ""), chain.get("step", ""))
    await wm.broadcast("batch_chain_progress", {
        "chain_id": chain["chain_id"],
        "step": chain.get("step", ""),
        "step_label": step_label,
        "status": chain.get("status", ""),
        "message": message,
        "mode": chain.get("mode", "single"),
        "target_id": chain.get("target_id"),
    })

    # í…”ë ˆê·¸ë¨ìœ¼ë¡œë„ ì§„í–‰ ìƒíƒœ ì „ë‹¬
    if app_state.telegram_app:
        ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
        if ceo_id:
            try:
                await app_state.telegram_app.bot.send_message(
                    chat_id=int(ceo_id),
                    text=f"ğŸ“¦ {message}",
                )
            except Exception as e:
                logger.debug("TG ë°°ì¹˜ ì§„í–‰ ì „ì†¡ ì‹¤íŒ¨: %s", e)


async def _start_batch_chain(text: str, task_id: str) -> dict:
    """ë°°ì¹˜ ì²´ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.

    CEO ëª…ë ¹ì„ ë°›ì•„ì„œ ìœ„ì„ ì²´ì¸ ì „ì²´ë¥¼ Batch APIë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë˜ë©´ ë¶„ë¥˜ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ë°”ë¡œ ì „ë¬¸ê°€ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    """
    chain_id = f"chain_{datetime.now(KST).strftime('%Y%m%d_%H%M%S')}_{task_id[:8]}"

    correlation_id = f"batch_{chain_id}"

    chain = {
        "chain_id": chain_id,
        "task_id": task_id,
        "correlation_id": correlation_id,
        "text": text,
        "mode": "broadcast" if _ms()._is_broadcast_command(text) else "single",
        "step": "classify",
        "status": "running",
        "target_id": None,
        "batches": {"classify": None, "specialists": [], "synthesis": []},
        "results": {"classify": None, "specialists": {}, "synthesis": {}},
        "custom_id_map": {},  # custom_id â†’ {"agent_id", "step"} ì—­ë§¤í•‘
        "delegation_instructions": {},  # íŒ€ì¥ ì§€ì‹œì„œ (ë‹¨ì¼ ë¶€ì„œ)
        "broadcast_delegations": {},  # íŒ€ì¥ ì§€ì‹œì„œ (ë¸Œë¡œë“œìºìŠ¤íŠ¸)
        "total_cost_usd": 0.0,
        "created_at": datetime.now(KST).isoformat(),
        "completed_at": None,
    }

    # taskì— correlation_id ì—°ê²°
    update_task(task_id, correlation_id=correlation_id)

    # ì˜ˆì‚° í™•ì¸
    limit = float(load_setting("daily_budget_usd") or 7.0)
    today = get_today_cost()
    if today >= limit:
        update_task(task_id, status="failed",
                    result_summary=f"ì¼ì¼ ì˜ˆì‚° ì´ˆê³¼ (${today:.2f}/${limit:.0f})",
                    success=0)
        return {"error": f"ì¼ì¼ ì˜ˆì‚°ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ (${today:.2f}/${limit:.0f})"}

    # â”€â”€ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ëª¨ë“œ â†’ ë¶„ë¥˜ ê±´ë„ˆë›°ê³  íŒ€ì¥ ì§€ì‹œì„œ â†’ ì „ ë¶€ì„œ ì „ë¬¸ê°€ â”€â”€
    if chain["mode"] == "broadcast":
        chain["step"] = "delegation"
        chain["target_id"] = "broadcast"
        _save_chain(chain)

        await _broadcast_chain_status(chain, "ğŸ“¦ ë°°ì¹˜ ì²´ì¸ ì‹œì‘ (ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ ë¶€ì„œ â†’ íŒ€ì¥ ì§€ì‹œì„œ ìƒì„± ì¤‘)")
        await _chain_create_delegation_broadcast(chain)
        return {"chain_id": chain_id, "status": "started", "mode": "broadcast", "step": chain["step"]}

    # â”€â”€ í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œë„ (ë¬´ë£Œ, ì¦‰ì‹œ) â”€â”€
    keyword_match = _ms()._classify_by_keywords(text)
    if keyword_match:
        chain["target_id"] = keyword_match
        chain["results"]["classify"] = {
            "agent_id": keyword_match,
            "method": "í‚¤ì›Œë“œ",
            "cost_usd": 0,
        }

        if keyword_match == "chief_of_staff":
            # ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬ â†’ ë°”ë¡œ ì¢…í•©(=ì§ì ‘ ë‹µë³€) ë‹¨ê³„
            chain["step"] = "synthesis"
            _save_chain(chain)
            await _broadcast_chain_status(chain, "ğŸ“¦ í‚¤ì›Œë“œ ë¶„ë¥˜ â†’ ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬")
            await _chain_submit_synthesis(chain)
        else:
            # íŒ€ì¥ ë¶€ì„œë¡œ ìœ„ì„ â†’ íŒ€ì¥ ì§€ì‹œì„œ â†’ ì „ë¬¸ê°€ í˜¸ì¶œ ë‹¨ê³„
            chain["step"] = "delegation"
            _save_chain(chain)
            target_name = _ms()._AGENT_NAMES.get(keyword_match, keyword_match)
            await _broadcast_chain_status(chain, f"ğŸ“¦ í‚¤ì›Œë“œ ë¶„ë¥˜ â†’ {target_name} ì§€ì‹œì„œ ìƒì„± ì¤‘")
            await _chain_create_delegation(chain)

        return {"chain_id": chain_id, "status": "started", "step": chain["step"]}

    # â”€â”€ AI ë¶„ë¥˜ê°€ í•„ìš” â†’ Batch APIë¡œ ë¶„ë¥˜ ìš”ì²­ ì œì¶œ â”€â”€
    # ê°€ì¥ ì €ë ´í•œ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ ì„ íƒ (Gemini Flash â†’ GPT Mini â†’ Claude)
    providers = get_available_providers()
    if providers.get("google"):
        classify_model = "gemini-2.5-flash"
    elif providers.get("openai"):
        classify_model = "gpt-5-mini"
    elif providers.get("anthropic"):
        classify_model = "claude-sonnet-4-6"
    else:
        # AI ì—†ìŒ â†’ ë¹„ì„œì‹¤ì¥ ì§ì ‘
        chain["target_id"] = "chief_of_staff"
        chain["step"] = "synthesis"
        chain["results"]["classify"] = {"agent_id": "chief_of_staff", "method": "í´ë°±"}
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return {"chain_id": chain_id, "status": "started", "step": "synthesis"}

    classify_custom_id = f"{chain_id}_classify"
    classify_req = {
        "custom_id": classify_custom_id,
        "message": text,
        "system_prompt": _BATCH_CLASSIFY_PROMPT,
        "model": classify_model,
        "max_tokens": 1024,
    }

    result = await batch_submit([classify_req], model=classify_model)

    if "error" in result:
        # ë°°ì¹˜ ì‹¤íŒ¨ â†’ í´ë°±ìœ¼ë¡œ ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬
        _log(f"[CHAIN] ë¶„ë¥˜ ë°°ì¹˜ ì‹¤íŒ¨: {result['error']} â†’ ë¹„ì„œì‹¤ì¥ í´ë°±")
        chain["target_id"] = "chief_of_staff"
        chain["step"] = "synthesis"
        chain["results"]["classify"] = {"agent_id": "chief_of_staff", "method": "í´ë°±", "error": result["error"]}
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return {"chain_id": chain_id, "status": "started", "step": "synthesis"}

    chain["batches"]["classify"] = {
        "batch_id": result["batch_id"],
        "provider": result["provider"],
        "status": "pending",
    }
    chain["status"] = "pending"
    chain["custom_id_map"][classify_custom_id] = {"agent_id": "classify", "step": "classify"}
    _save_chain(chain)

    _ensure_batch_poller()
    update_task(task_id, status="pending",
                result_summary="ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 1ë‹¨ê³„: ë¶„ë¥˜ ìš”ì²­ ì œì¶œë¨")
    await _broadcast_chain_status(chain, "ğŸ“¦ ë°°ì¹˜ ì²´ì¸ ì‹œì‘ â€” 1ë‹¨ê³„: ë¶„ë¥˜ ìš”ì²­ ì œì¶œë¨")

    _log(f"[CHAIN] ì‹œì‘: {chain_id} â€” ë¶„ë¥˜ ë°°ì¹˜ ì œì¶œ (batch_id: {result['batch_id']})")
    return {"chain_id": chain_id, "status": "pending", "step": "classify"}


# â”€â”€ íŒ€ì¥ ì§€ì‹œì„œ ìƒì„± í”„ë¡¬í”„íŠ¸ â”€â”€
_DELEGATION_PROMPT = """ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. CEOë¡œë¶€í„° ì•„ë˜ ì—…ë¬´ ì§€ì‹œë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.

ì†Œì† ì „ë¬¸ê°€ë“¤ì—ê²Œ ê°ê° êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œë¥¼ ë‚´ë ¤ì•¼ í•©ë‹ˆë‹¤.
ê° ì „ë¬¸ê°€ì˜ ì „ë¬¸ ë¶„ì•¼ì— ë§ê²Œ CEO ëª…ë ¹ì„ ì„¸ë¶€ ì—…ë¬´ë¡œ ë¶„í•´í•˜ì„¸ìš”.

## ì†Œì† ì „ë¬¸ê°€
{spec_list}

## CEO ëª…ë ¹
{text}

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
ê° ì „ë¬¸ê°€ IDë¥¼ í‚¤ë¡œ, êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ(2~4ë¬¸ì¥)ë¥¼ ê°’ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

{json_example}"""


async def _chain_create_delegation(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” íŒ€ì¥ì´ ì „ë¬¸ê°€ë³„ ì§€ì‹œì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤ (ì‹¤ì‹œê°„ API 1íšŒ í˜¸ì¶œ).

    ë¶„ë¥˜ ì™„ë£Œ í›„, ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œ ì „ì— í˜¸ì¶œë©ë‹ˆë‹¤.
    íŒ€ì¥ì—ê²Œ CEO ëª…ë ¹ì„ ì „ë‹¬í•˜ê³ , ê° ì „ë¬¸ê°€ì—ê²Œ ë‚´ë¦´ êµ¬ì²´ì  ì§€ì‹œì„œë¥¼ ë°›ìŠµë‹ˆë‹¤.
    """
    target_id = chain["target_id"]
    text = chain["text"]
    specialists = _ms()._MANAGER_SPECIALISTS.get(target_id, [])

    if not specialists:
        # ì „ë¬¸ê°€ ì—†ìŒ â†’ ì§€ì‹œì„œ ìƒì„± ë¶ˆí•„ìš”
        await _chain_submit_specialists(chain)
        return

    mgr_name = _ms()._AGENT_NAMES.get(target_id, target_id)

    # ì „ë¬¸ê°€ ëª©ë¡ í…ìŠ¤íŠ¸ ìƒì„±
    spec_list_parts = []
    json_example_parts = []
    for s_id in specialists:
        s_name = _ms()._SPECIALIST_NAMES.get(s_id, s_id)
        spec_list_parts.append(f"- {s_id}: {s_name}")
        json_example_parts.append(f'  "{s_id}": "êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ ë‚´ìš©"')

    spec_list = "\n".join(spec_list_parts)
    json_example = "{\n" + ",\n".join(json_example_parts) + "\n}"

    delegation_prompt = _DELEGATION_PROMPT.format(
        mgr_name=mgr_name,
        spec_list=spec_list,
        text=text,
        json_example=json_example,
    )

    # ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ë¡œ ì‹¤ì‹œê°„ API í˜¸ì¶œ (Gemini Flash â†’ GPT Mini â†’ Claude)
    providers = get_available_providers()
    if providers.get("google"):
        deleg_model = "gemini-2.5-flash"
    elif providers.get("openai"):
        deleg_model = "gpt-5-mini"
    elif providers.get("anthropic"):
        deleg_model = "claude-sonnet-4-6"
    else:
        deleg_model = None

    if deleg_model:
        # íŒ€ì¥ ì´ˆë¡ë¶ˆ ì¼œê¸°
        await _ms()._broadcast_status(target_id, "working", 0.2, f"{mgr_name} ì§€ì‹œì„œ ì‘ì„± ì¤‘...")
        try:
            result = await ask_ai(
                user_message=delegation_prompt,
                model=deleg_model,
                max_tokens=2048,
            )
            response_text = result.get("content", "") or result.get("text", "")

            # JSON íŒŒì‹± ì‹œë„
            _json = json
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` ë˜ëŠ” { ... })
            json_text = response_text.strip()
            if "```" in json_text:
                # ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ
                start = json_text.find("{")
                end = json_text.rfind("}") + 1
                if start >= 0 and end > start:
                    json_text = json_text[start:end]
            elif json_text.startswith("{"):
                pass  # ì´ë¯¸ JSON
            else:
                # { ì°¾ê¸°
                start = json_text.find("{")
                end = json_text.rfind("}") + 1
                if start >= 0 and end > start:
                    json_text = json_text[start:end]

            instructions = _json.loads(json_text)
            if isinstance(instructions, dict):
                chain["delegation_instructions"] = instructions
                chain["results"]["delegation"] = {
                    "agent_id": target_id,
                    "instructions": instructions,
                    "model": deleg_model,
                    "cost_usd": result.get("cost_usd", 0),
                }
                chain["total_cost_usd"] += result.get("cost_usd", 0)
                _log(f"[CHAIN] {chain['chain_id']} â€” {mgr_name} ì§€ì‹œì„œ ìƒì„± ì™„ë£Œ ({len(instructions)}ëª…)")
            else:
                _log(f"[CHAIN] {chain['chain_id']} â€” ì§€ì‹œì„œ íŒŒì‹± ì‹¤íŒ¨ (dict ì•„ë‹˜)")
        except Exception as e:
            _log(f"[CHAIN] {chain['chain_id']} â€” ì§€ì‹œì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ì§„í–‰ (ì§€ì‹œì„œ ì—†ì´ ì›ë³¸ ëª…ë ¹ìœ¼ë¡œ)

    # ì§€ì‹œì„œ ìƒíƒœ ì—…ë°ì´íŠ¸ + íŒ€ì¥ ì´ˆë¡ë¶ˆ ë„ê¸°
    has_instructions = bool(chain.get("delegation_instructions"))
    deleg_status = f"âœ… {mgr_name} ì§€ì‹œì„œ ìƒì„± ì™„ë£Œ" if has_instructions else f"âš ï¸ ì§€ì‹œì„œ ì—†ì´ ì§„í–‰"
    await _ms()._broadcast_status(target_id, "done", 0.5, deleg_status)
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 2ë‹¨ê³„: {deleg_status}")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 2ë‹¨ê³„: {deleg_status}")

    _save_chain(chain)

    # ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œë¡œ ì§„í–‰
    await _chain_submit_specialists(chain)


async def _chain_create_delegation_broadcast(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ íŒ€ì¥ì´ ê°ê° ì§€ì‹œì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."""
    text = chain["text"]
    all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]

    # ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ ì„ íƒ (Gemini Flash â†’ GPT Mini â†’ Claude)
    providers = get_available_providers()
    if providers.get("google"):
        deleg_model = "gemini-2.5-flash"
    elif providers.get("openai"):
        deleg_model = "gpt-5-mini"
    elif providers.get("anthropic"):
        deleg_model = "claude-sonnet-4-6"
    else:
        deleg_model = None

    broadcast_delegations = {}

    if deleg_model:
        _asyncio = asyncio

        async def _get_delegation(mgr_id: str) -> tuple[str, dict]:
            specialists = _ms()._MANAGER_SPECIALISTS.get(mgr_id, [])
            if not specialists:
                return mgr_id, {}
            mgr_name = _ms()._AGENT_NAMES.get(mgr_id, mgr_id)
            spec_list_parts = []
            json_example_parts = []
            for s_id in specialists:
                s_name = _ms()._SPECIALIST_NAMES.get(s_id, s_id)
                spec_list_parts.append(f"- {s_id}: {s_name}")
                json_example_parts.append(f'  "{s_id}": "êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ ë‚´ìš©"')

            prompt = _DELEGATION_PROMPT.format(
                mgr_name=mgr_name,
                spec_list="\n".join(spec_list_parts),
                text=text,
                json_example="{\n" + ",\n".join(json_example_parts) + "\n}",
            )
            try:
                result = await ask_ai(user_message=prompt, model=deleg_model)
                response_text = result.get("content", "") or result.get("text", "")
                _json = json
                json_text = response_text.strip()
                start = json_text.find("{")
                end = json_text.rfind("}") + 1
                if start >= 0 and end > start:
                    json_text = json_text[start:end]
                instructions = _json.loads(json_text)
                chain["total_cost_usd"] += result.get("cost_usd", 0)
                return mgr_id, instructions if isinstance(instructions, dict) else {}
            except Exception as e:
                _log(f"[CHAIN] ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì§€ì‹œì„œ ì‹¤íŒ¨ ({mgr_id}): {e}")
                return mgr_id, {}

        # 6ê°œ íŒ€ì¥ì—ê²Œ ë™ì‹œì— ì§€ì‹œì„œ ìš”ì²­
        tasks = [_get_delegation(m) for m in all_managers]
        results = await _asyncio.gather(*tasks, return_exceptions=True)
        for r in results:
            if isinstance(r, tuple):
                mgr_id, instructions = r
                if instructions:
                    broadcast_delegations[mgr_id] = instructions

    chain["broadcast_delegations"] = broadcast_delegations
    chain["results"]["delegation"] = {
        "mode": "broadcast",
        "delegations": broadcast_delegations,
    }

    deleg_count = sum(1 for d in broadcast_delegations.values() if d)
    _log(f"[CHAIN] {chain['chain_id']} â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì§€ì‹œì„œ {deleg_count}/6 ì™„ë£Œ")
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 2ë‹¨ê³„: íŒ€ì¥ ì§€ì‹œì„œ {deleg_count}/6 ì™„ë£Œ")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 2ë‹¨ê³„: íŒ€ì¥ ì§€ì‹œì„œ {deleg_count}/6 ì™„ë£Œ")
    _save_chain(chain)

    # ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œë¡œ ì§„í–‰
    await _chain_submit_specialists_broadcast(chain)


async def _chain_submit_specialists(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” ë‹¨ì¼ ë¶€ì„œì˜ ì „ë¬¸ê°€ë“¤ì—ê²Œ ë°°ì¹˜ ì œì¶œí•©ë‹ˆë‹¤."""
    target_id = chain["target_id"]
    text = chain["text"]
    specialists = _ms()._MANAGER_SPECIALISTS.get(target_id, [])

    if not specialists:
        # ì „ë¬¸ê°€ ì—†ìŒ â†’ ë°”ë¡œ ì¢…í•©(íŒ€ì¥ ì§ì ‘ ì²˜ë¦¬) ë‹¨ê³„
        chain["step"] = "synthesis"
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return

    # íŒ€ì¥ ì§€ì‹œì„œê°€ ìˆìœ¼ë©´ ì „ë¬¸ê°€ì—ê²Œ í•¨ê»˜ ì „ë‹¬
    delegation = chain.get("delegation_instructions", {})

    requests = []
    for spec_id in specialists:
        # ì „ë¬¸ê°€ ì´ˆë¡ë¶ˆ ì¼œê¸°
        spec_name = _ms()._SPECIALIST_NAMES.get(spec_id, spec_id)
        await _ms()._broadcast_status(spec_id, "working", 0.3, f"{spec_name} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")

        soul = _ms()._load_agent_prompt(spec_id, include_tools=False) + _BATCH_MODE_SUFFIX
        override = _ms()._get_model_override(spec_id)
        model = select_model(text, override=override)
        custom_id = f"{chain['chain_id']}_spec_{spec_id}"

        # íŒ€ì¥ ì§€ì‹œì„œê°€ ìˆìœ¼ë©´ ì „ë¬¸ê°€ì—ê²Œ í•¨ê»˜ ì „ë‹¬
        spec_instruction = delegation.get(spec_id, "")
        if spec_instruction:
            message = (
                f"## íŒ€ì¥ ì§€ì‹œ\n{spec_instruction}\n\n"
                f"## CEO ì›ë³¸ ëª…ë ¹\n{text}"
            )
        else:
            message = text

        requests.append({
            "custom_id": custom_id,
            "message": message,
            "system_prompt": soul,
            "model": model,
            "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
            "reasoning_effort": _ms()._get_agent_reasoning_effort(spec_id),
        })
        chain["custom_id_map"][custom_id] = {"agent_id": spec_id, "step": "specialists"}

    # í”„ë¡œë°”ì´ë”ë³„ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ ì œì¶œ
    batch_results = await batch_submit_grouped(requests)

    chain["batches"]["specialists"] = []
    for br in batch_results:
        chain["batches"]["specialists"].append({
            "batch_id": br.get("batch_id", ""),
            "provider": br.get("provider", ""),
            "status": "pending" if "error" not in br else "failed",
            "custom_ids": br.get("custom_ids", []),
            "error": br.get("error"),
        })

    chain["step"] = "specialists"
    chain["status"] = "pending"
    _save_chain(chain)

    _ensure_batch_poller()
    spec_count = len(specialists)
    provider_count = len(batch_results)
    target_name = _ms()._AGENT_NAMES.get(target_id, target_id)
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 3ë‹¨ê³„: {target_name} ì „ë¬¸ê°€ {spec_count}ëª… ë°°ì¹˜ ì œì¶œ ({provider_count}ê°œ í”„ë¡œë°”ì´ë”)")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 3ë‹¨ê³„: {target_name} ì „ë¬¸ê°€ {spec_count}ëª… â†’ {provider_count}ê°œ í”„ë¡œë°”ì´ë”ë³„ ë°°ì¹˜ ì œì¶œ")

    _log(f"[CHAIN] {chain['chain_id']} â€” ì „ë¬¸ê°€ {spec_count}ëª… ë°°ì¹˜ ì œì¶œ ({provider_count}ê°œ í”„ë¡œë°”ì´ë”)")


async def _chain_submit_specialists_broadcast(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ ë¶€ì„œ ì „ì²´ ì „ë¬¸ê°€ì—ê²Œ ë°°ì¹˜ ì œì¶œí•©ë‹ˆë‹¤."""
    text = chain["text"]
    all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]

    # ë¸Œë¡œë“œìºìŠ¤íŠ¸ ëª¨ë“œì˜ íŒ€ì¥ë³„ ì§€ì‹œì„œ
    broadcast_delegations = chain.get("broadcast_delegations", {})

    requests = []
    for mgr_id in all_managers:
        specialists = _ms()._MANAGER_SPECIALISTS.get(mgr_id, [])
        mgr_delegation = broadcast_delegations.get(mgr_id, {})
        for spec_id in specialists:
            soul = _ms()._load_agent_prompt(spec_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _ms()._get_model_override(spec_id)
            model = select_model(text, override=override)
            custom_id = f"{chain['chain_id']}_spec_{spec_id}"

            # íŒ€ì¥ ì§€ì‹œì„œê°€ ìˆìœ¼ë©´ ì „ë¬¸ê°€ì—ê²Œ í•¨ê»˜ ì „ë‹¬
            spec_instruction = mgr_delegation.get(spec_id, "")
            if spec_instruction:
                message = (
                    f"## íŒ€ì¥ ì§€ì‹œ\n{spec_instruction}\n\n"
                    f"## CEO ì›ë³¸ ëª…ë ¹\n{text}"
                )
            else:
                message = text

            requests.append({
                "custom_id": custom_id,
                "message": message,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _ms()._get_agent_reasoning_effort(spec_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": spec_id, "step": "specialists"}

    if not requests:
        chain["step"] = "synthesis"
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return

    # í”„ë¡œë°”ì´ë”ë³„ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ ì œì¶œ
    batch_results = await batch_submit_grouped(requests)

    chain["batches"]["specialists"] = []
    for br in batch_results:
        chain["batches"]["specialists"].append({
            "batch_id": br.get("batch_id", ""),
            "provider": br.get("provider", ""),
            "status": "pending" if "error" not in br else "failed",
            "custom_ids": br.get("custom_ids", []),
            "error": br.get("error"),
        })

    chain["step"] = "specialists"
    chain["status"] = "pending"
    _save_chain(chain)

    _ensure_batch_poller()
    spec_count = len(requests)
    provider_count = len(batch_results)
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 2ë‹¨ê³„: ì „ì²´ {spec_count}ëª… ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œ ({provider_count}ê°œ í”„ë¡œë°”ì´ë”)")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 2ë‹¨ê³„: 6ê°œ ë¶€ì„œ ì „ë¬¸ê°€ {spec_count}ëª… â†’ {provider_count}ê°œ í”„ë¡œë°”ì´ë”ë³„ ë°°ì¹˜ ì œì¶œ")

    _log(f"[CHAIN] {chain['chain_id']} â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì „ë¬¸ê°€ {spec_count}ëª… ë°°ì¹˜ ì œì¶œ")


async def _chain_submit_synthesis(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” íŒ€ì¥(ë“¤)ì´ ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ëŠ” ë°°ì¹˜ë¥¼ ì œì¶œí•©ë‹ˆë‹¤."""
    text = chain["text"]

    requests = []

    if chain["mode"] == "broadcast":
        # ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ íŒ€ì¥ì´ ê°ê° ìê¸° íŒ€ ê²°ê³¼ë¥¼ ì¢…í•©
        all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]
        for mgr_id in all_managers:
            specialists = _ms()._MANAGER_SPECIALISTS.get(mgr_id, [])
            spec_parts = []
            for s_id in specialists:
                s_res = chain["results"]["specialists"].get(s_id, {})
                name = _ms()._SPECIALIST_NAMES.get(s_id, s_id)
                content = s_res.get("content", "ì‘ë‹µ ì—†ìŒ")
                if s_res.get("error"):
                    content = f"ì˜¤ë¥˜: {s_res['error'][:100]}"
                spec_parts.append(f"[{name}]\n{content}")

            mgr_name = _ms()._AGENT_NAMES.get(mgr_id, mgr_id)
            synthesis_prompt = (
                f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì†Œì† ì „ë¬¸ê°€ë“¤ì´ ì•„ë˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì œì¶œí–ˆìŠµë‹ˆë‹¤.\n"
                f"ì´ë¥¼ ê²€ìˆ˜í•˜ê³  ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•  ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
                f"ì „ë¬¸ê°€ ì˜ê²¬ ì¤‘ ë¶€ì¡±í•˜ê±°ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì§€ì í•˜ê³  ë³´ì™„í•˜ì„¸ìš”.\n\n"
                f"## CEO ì›ë³¸ ëª…ë ¹\n{text}\n\n"
                f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts)
            )

            soul = _ms()._load_agent_prompt(mgr_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _ms()._get_model_override(mgr_id)
            model = select_model(synthesis_prompt, override=override)
            custom_id = f"{chain['chain_id']}_synth_{mgr_id}"

            requests.append({
                "custom_id": custom_id,
                "message": synthesis_prompt,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _ms()._get_agent_reasoning_effort(mgr_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": mgr_id, "step": "synthesis"}

    elif chain["target_id"] == "chief_of_staff":
        # ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬ (ë¶„ë¥˜ ê²°ê³¼ê°€ chief_of_staffì¸ ê²½ìš°)
        soul = _ms()._load_agent_prompt("chief_of_staff", include_tools=False) + _BATCH_MODE_SUFFIX
        override = _ms()._get_model_override("chief_of_staff")
        model = select_model(text, override=override)
        custom_id = f"{chain['chain_id']}_synth_chief_of_staff"

        requests.append({
            "custom_id": custom_id,
            "message": text,
            "system_prompt": soul,
            "model": model,
            "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
            "reasoning_effort": _ms()._get_agent_reasoning_effort("chief_of_staff"),
        })
        chain["custom_id_map"][custom_id] = {"agent_id": "chief_of_staff", "step": "synthesis"}

    else:
        # ë‹¨ì¼ ë¶€ì„œ: íŒ€ì¥ì´ ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ì¢…í•©
        target_id = chain["target_id"]
        specialists = _ms()._MANAGER_SPECIALISTS.get(target_id, [])

        if not specialists or not chain["results"]["specialists"]:
            # ì „ë¬¸ê°€ ê²°ê³¼ ì—†ìŒ â†’ íŒ€ì¥ì´ ì§ì ‘ ë‹µë³€
            soul = _ms()._load_agent_prompt(target_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _ms()._get_model_override(target_id)
            model = select_model(text, override=override)
            custom_id = f"{chain['chain_id']}_synth_{target_id}"

            requests.append({
                "custom_id": custom_id,
                "message": text,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _ms()._get_agent_reasoning_effort(target_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": target_id, "step": "synthesis"}
        else:
            # ì „ë¬¸ê°€ ê²°ê³¼ ì·¨í•© â†’ íŒ€ì¥ì—ê²Œ ì¢…í•© ìš”ì²­
            spec_parts = []
            for s_id in specialists:
                s_res = chain["results"]["specialists"].get(s_id, {})
                name = _ms()._SPECIALIST_NAMES.get(s_id, s_id)
                content = s_res.get("content", "ì‘ë‹µ ì—†ìŒ")
                if s_res.get("error"):
                    content = f"ì˜¤ë¥˜: {s_res['error'][:100]}"
                spec_parts.append(f"[{name}]\n{content}")

            mgr_name = _ms()._AGENT_NAMES.get(target_id, target_id)
            synthesis_prompt = (
                f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì†Œì† ì „ë¬¸ê°€ë“¤ì´ ì•„ë˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì œì¶œí–ˆìŠµë‹ˆë‹¤.\n"
                f"ì´ë¥¼ ê²€ìˆ˜í•˜ê³  ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•  ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
                f"ì „ë¬¸ê°€ ì˜ê²¬ ì¤‘ ë¶€ì¡±í•˜ê±°ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì§€ì í•˜ê³  ë³´ì™„í•˜ì„¸ìš”.\n\n"
                f"## CEO ì›ë³¸ ëª…ë ¹\n{text}\n\n"
                f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts)
            )

            soul = _ms()._load_agent_prompt(target_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _ms()._get_model_override(target_id)
            model = select_model(synthesis_prompt, override=override)
            custom_id = f"{chain['chain_id']}_synth_{target_id}"

            requests.append({
                "custom_id": custom_id,
                "message": synthesis_prompt,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _ms()._get_agent_reasoning_effort(target_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": target_id, "step": "synthesis"}

    if not requests:
        # ìš”ì²­ ì—†ìŒ â†’ ë°”ë¡œ ì™„ë£Œ
        await _deliver_chain_result(chain)
        return

    # í”„ë¡œë°”ì´ë”ë³„ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ ì œì¶œ
    batch_results = await batch_submit_grouped(requests)

    chain["batches"]["synthesis"] = []
    for br in batch_results:
        chain["batches"]["synthesis"].append({
            "batch_id": br.get("batch_id", ""),
            "provider": br.get("provider", ""),
            "status": "pending" if "error" not in br else "failed",
            "custom_ids": br.get("custom_ids", []),
            "error": br.get("error"),
        })

    chain["step"] = "synthesis"
    chain["status"] = "pending"
    _save_chain(chain)

    _ensure_batch_poller()

    if chain["mode"] == "broadcast":
        update_task(chain["task_id"], status="pending",
                    result_summary="ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 4ë‹¨ê³„: 6ê°œ íŒ€ì¥ ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ì œì¶œ")
        await _broadcast_chain_status(chain, "ğŸ“¦ 4ë‹¨ê³„: 6ê°œ íŒ€ì¥ì´ ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘ (ë°°ì¹˜)")
    else:
        target_name = _ms()._AGENT_NAMES.get(chain["target_id"], chain["target_id"])
        update_task(chain["task_id"], status="pending",
                    result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 4ë‹¨ê³„: {target_name} ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ì œì¶œ")
        await _broadcast_chain_status(chain, f"ğŸ“¦ 4ë‹¨ê³„: {target_name} ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘ (ë°°ì¹˜)")

    _log(f"[CHAIN] {chain['chain_id']} â€” ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ì œì¶œ ({len(requests)}ê±´)")


async def _send_batch_result_to_telegram(content: str, cost: float):
    """ë°°ì¹˜ ì²´ì¸ ê²°ê³¼ë¥¼ í…”ë ˆê·¸ë¨ CEOì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."""
    if not app_state.telegram_app:
        return
    ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
    if not ceo_id:
        return
    try:
        # í…”ë ˆê·¸ë¨ ì½”ë“œëª… ë³€í™˜
        content = _ms()._tg_convert_names(content)
        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (4096ì)
        if len(content) > 3800:
            content = content[:3800] + "\n\n... (ì „ì²´ ê²°ê³¼ëŠ” ì›¹ì—ì„œ í™•ì¸)"
        await app_state.telegram_app.bot.send_message(
            chat_id=int(ceo_id),
            text=f"ğŸ“¦ ë°°ì¹˜ ì²´ì¸ ì™„ë£Œ\n\n{content}\n\nâ”€â”€â”€â”€â”€\nğŸ’° ${cost:.4f}",
        )
    except Exception as e:
        _log(f"[TG] ë°°ì¹˜ ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {e}")



async def _synthesis_realtime_fallback(chain: dict):
    """ì¢…í•© ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ì‹¤ì‹œê°„ ask_ai()ë¡œ ì¢…í•©ë³´ê³ ì„œë¥¼ ëŒ€ì‹  ìƒì„±í•©ë‹ˆë‹¤."""
    text = chain["text"]
    _log(f"[CHAIN] {chain['chain_id']} â€” ì‹¤ì‹œê°„ í´ë°± ì‹œì‘")

    if chain["mode"] == "broadcast":
        all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]
        for mgr_id in all_managers:
            if mgr_id in chain["results"]["synthesis"]:
                continue  # ì´ë¯¸ ìˆìœ¼ë©´ skip
            specialists = _ms()._MANAGER_SPECIALISTS.get(mgr_id, [])
            spec_parts = []
            for s_id in specialists:
                s_res = chain["results"]["specialists"].get(s_id, {})
                name = _ms()._SPECIALIST_NAMES.get(s_id, s_id)
                content = s_res.get("content", "ì‘ë‹µ ì—†ìŒ")
                spec_parts.append(f"[{name}]\n{content}")

            mgr_name = _ms()._AGENT_NAMES.get(mgr_id, mgr_id)
            if spec_parts:
                synthesis_prompt = (
                    f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì†Œì† ì „ë¬¸ê°€ë“¤ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•˜ì„¸ìš”.\n\n"
                    f"## CEO ì›ë³¸ ëª…ë ¹\n{text}\n\n"
                    f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts)
                )
            else:
                synthesis_prompt = text
            soul = _ms()._load_agent_prompt(mgr_id, include_tools=False)
            try:
                result = await ask_ai(user_message=synthesis_prompt, system_prompt=soul)
                chain["results"]["synthesis"][mgr_id] = {
                    "content": result.get("content", ""),
                    "model": result.get("model", ""),
                    "cost_usd": result.get("cost_usd", 0),
                    "error": result.get("error"),
                }
                chain["total_cost_usd"] += result.get("cost_usd", 0)
            except Exception as e:
                _log(f"[CHAIN] ì‹¤ì‹œê°„ í´ë°± ì‹¤íŒ¨ ({mgr_id}): {e}")
                chain["results"]["synthesis"][mgr_id] = {"content": "", "error": str(e)[:100]}
    else:
        target_id = chain.get("target_id", "chief_of_staff")
        if target_id not in chain["results"]["synthesis"]:
            soul = _ms()._load_agent_prompt(target_id, include_tools=False)
            try:
                result = await ask_ai(user_message=text, system_prompt=soul)
                chain["results"]["synthesis"][target_id] = {
                    "content": result.get("content", ""),
                    "model": result.get("model", ""),
                    "cost_usd": result.get("cost_usd", 0),
                    "error": result.get("error"),
                }
                chain["total_cost_usd"] += result.get("cost_usd", 0)
            except Exception as e:
                _log(f"[CHAIN] ì‹¤ì‹œê°„ í´ë°± ì‹¤íŒ¨ ({target_id}): {e}")
                chain["results"]["synthesis"][target_id] = {"content": "", "error": str(e)[:100]}

    target_id = chain.get("target_id", "chief_of_staff")
    await _ms()._broadcast_status(target_id, "done", 1.0, "ë³´ê³  ì™„ë£Œ")
    _save_chain(chain)
    await _deliver_chain_result(chain)


async def _deliver_chain_result(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ ìµœì¢… ê²°ê³¼ë¥¼ CEOì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."""
    # â”€â”€ ì¤‘ë³µ ì „ë‹¬ ë°©ì§€ â”€â”€
    if chain.get("delivered"):
        _log(f"[CHAIN] {chain.get('chain_id', '?')} â€” ì´ë¯¸ ì „ë‹¬ë¨, ì¤‘ë³µ ë°©ì§€")
        return
    chain["delivered"] = True
    # ì¦‰ì‹œ completed ìƒíƒœë¡œ ë³€ê²½ â†’ í´ëŸ¬ê°€ ì¬ì§„ì… ëª»í•˜ê²Œ ë°©ì§€
    chain["step"] = "completed"
    chain["status"] = "completed"
    chain["completed_at"] = datetime.now(KST).isoformat()
    _save_chain(chain)

    task_id = chain["task_id"]
    text = chain["text"]
    total_cost = chain.get("total_cost_usd", 0)

    if chain["mode"] == "broadcast":
        # ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ íŒ€ì¥ ì¢…í•© ê²°ê³¼ë¥¼ ëª¨ì•„ì„œ ì „ë‹¬
        all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]
        parts = []
        total_specialists = 0
        for mgr_id in all_managers:
            synth = chain["results"]["synthesis"].get(mgr_id, {})
            mgr_name = _ms()._AGENT_NAMES.get(mgr_id, mgr_id)
            content = synth.get("content", "")
            # ì¢…í•©ë³´ê³ ì„œê°€ ë¹„ì—ˆìœ¼ë©´ ì „ë¬¸ê°€ ì›ë³¸ ê²°ê³¼ë¥¼ í´ë°±ìœ¼ë¡œ ì‚¬ìš©
            if not content or content == "ì‘ë‹µ ì—†ìŒ":
                specialists = _ms()._MANAGER_SPECIALISTS.get(mgr_id, [])
                fallback_parts = []
                for s_id in specialists:
                    s_res = chain["results"].get("specialists", {}).get(s_id, {})
                    s_content = s_res.get("content", "")
                    if s_content:
                        s_name = _ms()._SPECIALIST_NAMES.get(s_id, s_id)
                        fallback_parts.append(f"**{s_name}**: {s_content[:300]}")
                if fallback_parts:
                    content = "(ì¢…í•© ë°°ì¹˜ ì‹¤íŒ¨ â€” ì „ë¬¸ê°€ ì›ë³¸ ê²°ê³¼)\n" + "\n".join(fallback_parts)
                else:
                    content = "ì‘ë‹µ ì—†ìŒ (ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ)"
            specs = len(_ms()._MANAGER_SPECIALISTS.get(mgr_id, []))
            total_specialists += specs
            spec_label = f" (ì „ë¬¸ê°€ {specs}ëª… ë™ì›)" if specs else ""
            parts.append(f"### ğŸ“‹ {mgr_name}{spec_label}\n{content}")

        compiled = (
            f"ğŸ“¢ **ë°°ì¹˜ ì²´ì¸ ê²°ê³¼** (6ê°œ ë¶€ì„œ + ì „ë¬¸ê°€ {total_specialists}ëª… ë™ì›)\n"
            f"ğŸ’° ì´ ë¹„ìš©: ${total_cost:.4f} (ë°°ì¹˜ í• ì¸ ~50% ì ìš©)\n\n---\n\n"
            + "\n\n---\n\n".join(parts)
        )

        update_task(task_id, status="completed",
                    result_summary=compiled[:500],
                    result_data=compiled,
                    success=1, cost_usd=total_cost,
                    agent_id="chief_of_staff")

        # WebSocketìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ì „ë‹¬
        await wm.broadcast("result", {
            "content": compiled,
            "sender_id": "chief_of_staff",
            "handled_by": "ë¹„ì„œì‹¤ì¥ â†’ 6ê°œ íŒ€ì¥",
            "delegation": "ë¹„ì„œì‹¤ì¥ â†’ íŒ€ì¥ â†’ ì „ë¬¸ê°€ (ë°°ì¹˜)",
            "time_seconds": 0,
            "cost": total_cost,
            "model": "multi-agent-batch",
            "routing_method": "ë°°ì¹˜ ì²´ì¸ (ë¸Œë¡œë“œìºìŠ¤íŠ¸)",
        })

    else:
        # ë‹¨ì¼ ë¶€ì„œ ê²°ê³¼
        target_id = chain.get("target_id", "chief_of_staff")
        synth = chain["results"]["synthesis"].get(
            target_id,
            chain["results"]["synthesis"].get("chief_of_staff", {})
        )
        content = synth.get("content", "")
        target_name = _ms()._AGENT_NAMES.get(target_id, target_id)

        # ìœ„ì„ ì •ë³´ êµ¬ì„±
        specs_count = len(_ms()._MANAGER_SPECIALISTS.get(target_id, []))
        if target_id == "chief_of_staff":
            delegation = ""
            handled_by = "ë¹„ì„œì‹¤ì¥"
            header = "ğŸ“‹ **ë¹„ì„œì‹¤ì¥** (ë°°ì¹˜ ì²˜ë¦¬)"
        else:
            delegation = f"ë¹„ì„œì‹¤ì¥ â†’ {target_name}"
            if specs_count:
                delegation += f" â†’ ì „ë¬¸ê°€ {specs_count}ëª…"
            handled_by = target_name
            header = f"ğŸ“‹ **{target_name}** ë³´ê³  (ë°°ì¹˜ ì²´ì¸)"
            if specs_count:
                header += f" (ì†Œì† ì „ë¬¸ê°€ {specs_count}ëª… ë™ì›)"

        final_content = f"{header}\nğŸ’° ë¹„ìš©: ${total_cost:.4f} (ë°°ì¹˜ í• ì¸ ~50% ì ìš©)\n\n---\n\n{content}"

        update_task(task_id, status="completed",
                    result_summary=final_content[:500],
                    result_data=final_content,
                    success=1, cost_usd=total_cost,
                    agent_id=target_id)

        # WebSocketìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ì „ë‹¬
        await wm.broadcast("result", {
            "content": final_content,
            "sender_id": target_id,
            "handled_by": handled_by,
            "delegation": delegation,
            "time_seconds": 0,
            "cost": total_cost,
            "model": synth.get("model", "batch"),
            "routing_method": "ë°°ì¹˜ ì²´ì¸",
        })

    # í…”ë ˆê·¸ë¨ìœ¼ë¡œë„ ê²°ê³¼ ì „ë‹¬
    tg_content = compiled if chain["mode"] == "broadcast" else final_content
    await _send_batch_result_to_telegram(tg_content, total_cost)

    # ì•„ì¹´ì´ë¸Œì— ì €ì¥
    synth_content = ""
    if chain["mode"] == "broadcast":
        synth_content = compiled
    else:
        synth_content = content

    if synth_content and len(synth_content) > 20:
        division = _ms()._AGENT_DIVISION.get(chain.get("target_id", "chief_of_staff"), "secretary")
        now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
        save_archive(
            division=division,
            filename=f"batch_chain_{now_str}.md",
            content=f"# [ë°°ì¹˜ ì²´ì¸] {text[:60]}\n\n{synth_content}",
            agent_id=chain.get("target_id", "chief_of_staff"),
        )

    _save_chain(chain)

    await _broadcast_chain_status(chain, "âœ… ë°°ì¹˜ ì²´ì¸ ì™„ë£Œ â€” ìµœì¢… ë³´ê³ ì„œ ì „ë‹¬ë¨")
    _log(f"[CHAIN] {chain['chain_id']} â€” ì™„ë£Œ! ë¹„ìš©: ${total_cost:.4f}")


async def _advance_batch_chain(chain_id: str):
    """ë°°ì¹˜ ì²´ì¸ì˜ í˜„ì¬ ë‹¨ê³„ë¥¼ í™•ì¸í•˜ê³ , ì™„ë£Œë˜ì—ˆìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.

    ë°°ì¹˜ í´ëŸ¬(_batch_poller_loop)ì—ì„œ 60ì´ˆë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    chain = _load_chain(chain_id)
    if not chain or chain.get("status") not in ("running", "pending"):
        return

    step = chain.get("step", "")

    # â”€â”€ 1ë‹¨ê³„: ë¶„ë¥˜ â”€â”€
    if step == "classify":
        batch_info = chain["batches"].get("classify")
        if not batch_info:
            return

        status_result = await batch_check(batch_info["batch_id"], batch_info["provider"])
        if "error" in status_result:
            return

        batch_info["status"] = status_result["status"]

        if status_result["status"] == "completed":
            # ë¶„ë¥˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            result = await batch_retrieve(batch_info["batch_id"], batch_info["provider"])
            if "error" in result:
                chain["status"] = "failed"
                _save_chain(chain)
                return

            # JSON íŒŒì‹± â€” {"agent_id": "cio_manager", "reason": "..."}
            results_list = result.get("results", [])
            if results_list:
                raw_content = results_list[0].get("content", "").strip()
                cost = results_list[0].get("cost_usd", 0)
                chain["total_cost_usd"] += cost

                try:
                    if "```" in raw_content:
                        raw_content = raw_content.split("```")[1]
                        if raw_content.startswith("json"):
                            raw_content = raw_content[4:]
                    parsed = json.loads(raw_content)
                    target_id = parsed.get("agent_id", "chief_of_staff")
                    reason = parsed.get("reason", "")
                except (json.JSONDecodeError, IndexError):
                    _log(f"[CHAIN] ë¶„ë¥˜ JSON íŒŒì‹± ì‹¤íŒ¨: {raw_content[:100]}")
                    target_id = "chief_of_staff"
                    reason = "ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨"
            else:
                target_id = "chief_of_staff"
                reason = "ë¶„ë¥˜ ê²°ê³¼ ì—†ìŒ"

            chain["target_id"] = target_id
            chain["results"]["classify"] = {
                "agent_id": target_id,
                "reason": reason,
                "method": "AIë¶„ë¥˜ (ë°°ì¹˜)",
                "cost_usd": cost if results_list else 0,
            }

            target_name = _ms()._AGENT_NAMES.get(target_id, target_id)
            _log(f"[CHAIN] {chain['chain_id']} â€” ë¶„ë¥˜ ì™„ë£Œ: {target_name} ({reason})")

            if target_id == "chief_of_staff":
                # ë¹„ì„œì‹¤ì¥ ì§ì ‘ â†’ ì¢…í•© ë‹¨ê³„
                chain["step"] = "synthesis"
                _save_chain(chain)
                await _broadcast_chain_status(chain, f"ğŸ“¦ ë¶„ë¥˜ ì™„ë£Œ: ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬")
                await _chain_submit_synthesis(chain)
            else:
                # íŒ€ì¥ ì§€ì‹œì„œ â†’ ì „ë¬¸ê°€ ë‹¨ê³„ë¡œ ì§„í–‰
                chain["step"] = "delegation"
                _save_chain(chain)
                await _broadcast_chain_status(chain, f"ğŸ“¦ ë¶„ë¥˜ ì™„ë£Œ: {target_name} ì§€ì‹œì„œ ìƒì„± ì¤‘")
                await _chain_create_delegation(chain)

        elif status_result["status"] in ("failed", "expired"):
            # ë¶„ë¥˜ ë°°ì¹˜ ì‹¤íŒ¨ â†’ ë¹„ì„œì‹¤ì¥ í´ë°±
            chain["target_id"] = "chief_of_staff"
            chain["step"] = "synthesis"
            chain["results"]["classify"] = {"agent_id": "chief_of_staff", "method": "í´ë°±"}
            _save_chain(chain)
            await _chain_submit_synthesis(chain)

    # â”€â”€ delegation ì•ˆì „ë§ â”€â”€
    # delegationì€ ì‹¤ì‹œê°„ APIë¡œ ì¦‰ì‹œ ì²˜ë¦¬ë˜ë¯€ë¡œ í´ëŸ¬ê°€ ê´€ì—¬í•  ì¼ì´ ì—†ìŒ.
    # í•˜ì§€ë§Œ _chain_create_delegation() ì¤‘ ì—ëŸ¬ë¡œ stepì´ "delegation"ì— ë©ˆì¶°ìˆìœ¼ë©´
    # ì—¬ê¸°ì„œ ë³µêµ¬í•˜ì—¬ ì „ë¬¸ê°€ ë‹¨ê³„ë¥¼ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    elif step == "delegation":
        # ì´ë¯¸ ì „ë¬¸ê°€ ë°°ì¹˜ê°€ ì œì¶œëœ ìƒíƒœë©´ â†’ specialistsë¡œ ì „í™˜
        if chain["batches"].get("specialists"):
            chain["step"] = "specialists"
            _save_chain(chain)
            _log(f"[CHAIN] {chain_id} â€” delegation ì•ˆì „ë§: specialistsë¡œ ì „í™˜")
        else:
            # ì „ë¬¸ê°€ ë°°ì¹˜ê°€ ì•„ì§ ì œì¶œ ì•ˆ ë¨ â†’ ì§€ì‹œì„œ ìƒì„±ë¶€í„° ì¬ì‹œë„
            _log(f"[CHAIN] {chain_id} â€” delegation ì•ˆì „ë§: ì§€ì‹œì„œ ìƒì„± ì¬ì‹œë„")
            try:
                await _chain_create_delegation(chain)
            except Exception as e:
                _log(f"[CHAIN] {chain_id} â€” delegation ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ì§€ì‹œì„œ ì—†ì´ ì „ë¬¸ê°€ì—ê²Œ ì§ì ‘ ì „ë‹¬
                chain["step"] = "specialists"
                _save_chain(chain)
                await _chain_submit_specialists(chain)
        return

    # â”€â”€ 3ë‹¨ê³„: ì „ë¬¸ê°€ â”€â”€
    elif step == "specialists":
        all_done = True
        batch_errors = []  # ì˜¤ë¥˜ ì¶”ì 
        for batch_info in chain["batches"].get("specialists", []):
            if batch_info.get("status") in ("pending", "processing"):
                try:
                    status_result = await batch_check(batch_info["batch_id"], batch_info["provider"])
                    if "error" not in status_result:
                        batch_info["status"] = status_result["status"]
                    else:
                        err = status_result.get("error", "ë°°ì¹˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
                        _log(f"[CHAIN] ì „ë¬¸ê°€ ë°°ì¹˜ í™•ì¸ ì˜¤ë¥˜ ({batch_info['provider']}): {err}")
                        batch_errors.append(f"{batch_info['provider']}: {err[:80]}")
                except Exception as e:
                    _log(f"[CHAIN] ì „ë¬¸ê°€ ë°°ì¹˜ í™•ì¸ ì˜ˆì™¸ ({batch_info.get('provider','?')}): {e}")
                    batch_errors.append(f"{batch_info.get('provider','?')}: {str(e)[:80]}")

            if batch_info.get("status") not in ("completed", "failed"):
                all_done = False

        _save_chain(chain)

        if not all_done:
            # â”€â”€ ë°°ì¹˜ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ â†’ ì´ˆë¡ë¶ˆ ìœ ì§€ (ë§¥ë°• íš¨ê³¼) â”€â”€
            # Anthropic/OpenAI/Google í”„ë¡œë°”ì´ë” ë¬´ê´€, ê²°ê³¼ ì—†ëŠ” ì „ë¬¸ê°€ì—ê²Œ ì´ˆë¡ë¶ˆ
            target_id = chain.get("target_id", "")
            specialists = _ms()._MANAGER_SPECIALISTS.get(target_id, [])
            for spec_id in specialists:
                spec_res = chain["results"].get("specialists", {}).get(spec_id)
                if spec_res is None:
                    spec_name = _ms()._SPECIALIST_NAMES.get(spec_id, spec_id)
                    await _ms()._broadcast_status(spec_id, "working", 0.5, f"{spec_name} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
            return

        if all_done:
            # ëª¨ë“  ì „ë¬¸ê°€ ë°°ì¹˜ ì™„ë£Œ â†’ ê²°ê³¼ ìˆ˜ì§‘
            retrieve_errors = []
            for batch_info in chain["batches"]["specialists"]:
                if batch_info.get("status") != "completed":
                    if batch_info.get("status") == "failed":
                        err_detail = batch_info.get("error", "ë°°ì¹˜ ì‹¤íŒ¨")
                        retrieve_errors.append(f"{batch_info['provider']}: {err_detail[:80]}")
                    continue

                result = await batch_retrieve(batch_info["batch_id"], batch_info["provider"])
                if "error" in result:
                    retrieve_errors.append(f"{batch_info['provider']}: {result['error'][:80]}")
                    _log(f"[CHAIN] ì „ë¬¸ê°€ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨ ({batch_info['provider']}): {result['error']}")
                    continue

                for r in result.get("results", []):
                    custom_id = r.get("custom_id", "")
                    mapping = chain["custom_id_map"].get(custom_id, {})
                    agent_id = mapping.get("agent_id", custom_id)

                    chain["results"]["specialists"][agent_id] = {
                        "content": r.get("content", ""),
                        "model": r.get("model", ""),
                        "cost_usd": r.get("cost_usd", 0),
                        "error": r.get("error"),
                    }
                    chain["total_cost_usd"] += r.get("cost_usd", 0)
                    # ì „ë¬¸ê°€ ì´ˆë¡ë¶ˆ ë„ê¸°
                    await _ms()._broadcast_status(agent_id, "done", 1.0, "ì™„ë£Œ")

            spec_count = len(chain["results"]["specialists"])
            _log(f"[CHAIN] {chain['chain_id']} â€” ì „ë¬¸ê°€ {spec_count}ëª… ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")

            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ì›ì¸ì„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ë‹¬
            if spec_count == 0:
                all_errors = batch_errors + retrieve_errors
                if all_errors:
                    error_summary = " | ".join(all_errors[:3])
                    await _broadcast_chain_status(chain, f"âš ï¸ ì „ë¬¸ê°€ ë°°ì¹˜ ì‹¤íŒ¨ â€” ì›ì¸: {error_summary}")
                else:
                    await _broadcast_chain_status(chain, "âš ï¸ ì „ë¬¸ê°€ ë°°ì¹˜ ê²°ê³¼ ì—†ìŒ â€” íŒ€ì¥ ì§ì ‘ ì²˜ë¦¬ë¡œ ì „í™˜")

            # â”€â”€ í’ˆì§ˆê²€ìˆ˜ ì œê±°ë¨ (2026-02-27) â”€â”€

            # ì¢…í•© ë‹¨ê³„ë¡œ ì§„í–‰ â€” íŒ€ì¥ ì´ˆë¡ë¶ˆ ì¼œê¸°
            target_id = chain.get("target_id", "chief_of_staff")
            target_name = _ms()._AGENT_NAMES.get(target_id, target_id)
            await _ms()._broadcast_status(target_id, "working", 0.7, f"{target_name} ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘...")

            chain["step"] = "synthesis"
            _save_chain(chain)
            await _broadcast_chain_status(chain, f"ğŸ“¦ ì „ë¬¸ê°€ {spec_count}ëª… ì™„ë£Œ â†’ ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì‹œì‘")
            await _chain_submit_synthesis(chain)

    # â”€â”€ 4ë‹¨ê³„: ì¢…í•©ë³´ê³ ì„œ â”€â”€
    elif step == "synthesis":
        all_done = True
        synth_errors = []  # ì˜¤ë¥˜ ì¶”ì 
        for batch_info in chain["batches"].get("synthesis", []):
            if batch_info.get("status") in ("pending", "processing"):
                try:
                    status_result = await batch_check(batch_info["batch_id"], batch_info["provider"])
                    if "error" not in status_result:
                        batch_info["status"] = status_result["status"]
                    else:
                        err = status_result.get("error", "ë°°ì¹˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
                        _log(f"[CHAIN] ì¢…í•© ë°°ì¹˜ í™•ì¸ ì˜¤ë¥˜ ({batch_info['provider']}): {err}")
                        synth_errors.append(f"{batch_info['provider']}: {err[:80]}")
                except Exception as e:
                    _log(f"[CHAIN] ì¢…í•© ë°°ì¹˜ í™•ì¸ ì˜ˆì™¸ ({batch_info.get('provider','?')}): {e}")
                    synth_errors.append(f"{batch_info.get('provider','?')}: {str(e)[:80]}")

            if batch_info.get("status") not in ("completed", "failed"):
                all_done = False

        _save_chain(chain)

        if not all_done:
            # â”€â”€ ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ëŒ€ê¸° ì¤‘ â†’ íŒ€ì¥ ì´ˆë¡ë¶ˆ ìœ ì§€ â”€â”€
            target_id = chain.get("target_id", "chief_of_staff")
            target_name = _ms()._AGENT_NAMES.get(target_id, target_id)
            await _ms()._broadcast_status(target_id, "working", 0.8, f"{target_name} ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
            return

        if all_done:
            # ì¢…í•©ë³´ê³ ì„œ ê²°ê³¼ ìˆ˜ì§‘
            retrieve_errors = []
            for batch_info in chain["batches"]["synthesis"]:
                if batch_info.get("status") != "completed":
                    if batch_info.get("status") == "failed":
                        err_detail = batch_info.get("error", "ë°°ì¹˜ ì‹¤íŒ¨")
                        retrieve_errors.append(f"{batch_info['provider']}: {err_detail[:80]}")
                    continue

                result = await batch_retrieve(batch_info["batch_id"], batch_info["provider"])
                if "error" in result:
                    retrieve_errors.append(f"{batch_info['provider']}: {result['error'][:80]}")
                    _log(f"[CHAIN] ì¢…í•© ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨ ({batch_info['provider']}): {result['error']}")
                    continue

                for r in result.get("results", []):
                    custom_id = r.get("custom_id", "")
                    mapping = chain["custom_id_map"].get(custom_id, {})
                    agent_id = mapping.get("agent_id", custom_id)

                    chain["results"]["synthesis"][agent_id] = {
                        "content": r.get("content", ""),
                        "model": r.get("model", ""),
                        "cost_usd": r.get("cost_usd", 0),
                        "error": r.get("error"),
                    }
                    chain["total_cost_usd"] += r.get("cost_usd", 0)

            synth_count = len(chain["results"]["synthesis"])
            _log(f"[CHAIN] {chain['chain_id']} â€” ì¢…í•©ë³´ê³ ì„œ {synth_count}ê°œ ì™„ë£Œ")

            # ì¢…í•© ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ì›ì¸ ì•Œë¦¼ + ì‹¤ì‹œê°„ í´ë°±
            if synth_count == 0:
                all_errors = synth_errors + retrieve_errors
                if all_errors:
                    error_summary = " | ".join(all_errors[:3])
                    await _broadcast_chain_status(chain, f"âš ï¸ ì¢…í•© ë°°ì¹˜ ì‹¤íŒ¨ â€” ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬ì²˜ë¦¬: {error_summary}")
                else:
                    await _broadcast_chain_status(chain, "âš ï¸ ì¢…í•© ë°°ì¹˜ ê²°ê³¼ ì—†ìŒ â€” ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬ì²˜ë¦¬")

                # ì‹¤ì‹œê°„ í´ë°±: ask_ai()ë¡œ ì§ì ‘ ì¢…í•©ë³´ê³ ì„œ ìƒì„±
                await _synthesis_realtime_fallback(chain)
                return

            # â”€â”€ í’ˆì§ˆê²€ìˆ˜ ì œê±°ë¨ (2026-02-27) â”€â”€

            # íŒ€ì¥ ì´ˆë¡ë¶ˆ ë„ê¸°
            target_id = chain.get("target_id", "chief_of_staff")
            await _ms()._broadcast_status(target_id, "done", 1.0, "ë³´ê³  ì™„ë£Œ")

            # ìµœì¢… ì „ë‹¬
            await _deliver_chain_result(chain)


@batch_router.get("/api/batch/chains")
async def get_batch_chains():
    """ì§„í–‰ ì¤‘ì¸ ë°°ì¹˜ ì²´ì¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    chains = load_setting("batch_chains") or []
    active = [c for c in chains if c.get("status") in ("running", "pending")]
    recent_done = [c for c in chains if c.get("status") in ("completed", "failed")][-10:]
    return {"active": active, "recent": recent_done}

