"""
CORTHEX HQ - ARM Server

Oracle Cloud ARM ì„œë²„ (4ì½”ì–´ 24GB)ì—ì„œ ëŒ€ì‹œë³´ë“œë¥¼ ì„œë¹„ìŠ¤í•©ë‹ˆë‹¤.
ì „ì²´ ë°±ì—”ë“œì˜ í•µì‹¬ APIë§Œ ì œê³µí•˜ì—¬ ëŒ€ì‹œë³´ë“œ UIê°€ ì •ìƒ ì‘ë™í•˜ë„ë¡ í•¨.
í…”ë ˆê·¸ë¨ ë´‡ë„ ì—¬ê¸°ì„œ 24ì‹œê°„ êµ¬ë™ë©ë‹ˆë‹¤.
"""
import asyncio
import json
import logging
import os
import re
import sys
import time
import uuid as _uuid
import urllib.request
import urllib.error
from datetime import datetime, timezone, timedelta
from pathlib import Path

# DB + WS ëª¨ë“ˆì„ ê°™ì€ í´ë”ì—ì„œ ì„í¬íŠ¸
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from ws_manager import wm  # WebSocket/SSE ë¸Œë¡œë“œìºìŠ¤íŠ¸ ë§¤ë‹ˆì €
from state import app_state  # ì „ì—­ ìƒíƒœ ê´€ë¦¬ (ê´€ë¦¬ì‚¬ë¬´ì†Œ)
from db import (
    init_db, get_connection, save_message, create_task, get_task as db_get_task,
    update_task, list_tasks, toggle_bookmark as db_toggle_bookmark,
    get_dashboard_stats, save_activity_log, list_activity_logs,
    save_archive, list_archives, get_archive as db_get_archive, delete_archive as db_delete_archive,
    save_setting, load_setting, get_today_cost,
    save_conversation_message, load_conversation_messages, clear_conversation_messages,
    load_conversation_messages_by_id,
    delete_task as db_delete_task, bulk_delete_tasks, bulk_archive_tasks,
    set_task_tags, mark_task_read, bulk_mark_read,
    save_quality_review, get_quality_stats,
    save_collaboration_log,
)
try:
    from ai_handler import (
        init_ai_client, is_ai_ready, ask_ai, select_model,
        classify_task, get_available_providers,
        _load_tool_schemas,  # ë„êµ¬ ìŠ¤í‚¤ë§ˆ ë¡œë”© (function callingìš©)
        batch_submit, batch_check, batch_retrieve,  # Batch API
        batch_submit_grouped,  # í”„ë¡œë°”ì´ë”ë³„ ê·¸ë£¹ ë°°ì¹˜ ì œì¶œ (ë°°ì¹˜ ì²´ì¸ìš©)
    )
except ImportError:
    def init_ai_client(): return False
    def is_ai_ready(): return False
    async def ask_ai(*a, **kw): return {"error": "ai_handler ë¯¸ì„¤ì¹˜"}
    def select_model(t, override=None): return override or "claude-sonnet-4-6"
    async def classify_task(t): return {"agent_id": "chief_of_staff", "reason": "ai_handler ë¯¸ì„¤ì¹˜", "cost_usd": 0}
    def get_available_providers(): return {"anthropic": False, "google": False, "openai": False}
    def _load_tool_schemas(allowed_tools=None): return {}
    async def batch_submit(*a, **kw): return {"error": "ai_handler ë¯¸ì„¤ì¹˜"}
    async def batch_check(*a, **kw): return {"error": "ai_handler ë¯¸ì„¤ì¹˜"}
    async def batch_retrieve(*a, **kw): return {"error": "ai_handler ë¯¸ì„¤ì¹˜"}
    async def batch_submit_grouped(*a, **kw): return [{"error": "ai_handler ë¯¸ì„¤ì¹˜"}]

# í’ˆì§ˆê²€ìˆ˜ ì—”ì§„
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), ".."))
try:
    from src.core.quality_gate import QualityGate, HybridReviewResult
    from src.llm.base import LLMResponse
    _QUALITY_GATE_AVAILABLE = True
except ImportError:
    _QUALITY_GATE_AVAILABLE = False

try:
    from kis_client import (
        get_current_price as _kis_price,
        place_order as _kis_order,
        get_balance as _kis_balance,
        is_configured as _kis_configured,
        get_overseas_price as _kis_us_price,
        place_overseas_order as _kis_us_order,
        place_mock_order as _kis_mock_order,
        place_mock_overseas_order as _kis_mock_us_order,
        get_mock_balance as _kis_mock_balance,
        is_mock_configured as _kis_mock_configured,
        KIS_IS_MOCK,
    )
    _KIS_AVAILABLE = True
except ImportError:
    _KIS_AVAILABLE = False
    KIS_IS_MOCK = True
    async def _kis_price(ticker): return 0
    async def _kis_order(ticker, action, qty, price=0): return {"success": False, "message": "kis_client ë¯¸ì„¤ì¹˜", "order_no": ""}
    async def _kis_balance(): return {"success": False, "cash": 0, "holdings": [], "total_eval": 0}
    def _kis_configured(): return False
    async def _kis_us_price(symbol, exchange=""): return {"success": False, "price": 0}
    async def _kis_us_order(symbol, action, qty, price=0, exchange=""): return {"success": False, "message": "kis_client ë¯¸ì„¤ì¹˜", "order_no": ""}
    async def _kis_mock_order(ticker, action, qty, price=0): return {"success": False, "message": "kis_client ë¯¸ì„¤ì¹˜", "order_no": ""}
    async def _kis_mock_us_order(symbol, action, qty, price=0, exchange=""): return {"success": False, "message": "kis_client ë¯¸ì„¤ì¹˜", "order_no": ""}
    async def _kis_mock_balance(): return {"success": False, "cash": 0, "holdings": [], "total_eval": 0}
    def _kis_mock_configured(): return False
    _log("[KIS] kis_client ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨ â€” ëª¨ì˜íˆ¬ì ëª¨ë“œ")

# Python ì¶œë ¥ ë²„í¼ë§ ë¹„í™œì„±í™” (systemdì—ì„œ ë¡œê·¸ê°€ ë°”ë¡œ ë³´ì´ë„ë¡)
os.environ["PYTHONUNBUFFERED"] = "1"

# ì§„ë‹¨ ì •ë³´ ìˆ˜ì§‘ìš© â†’ app_state.diag ì‚¬ìš©
_diag = app_state.diag
_diag.update({"env_file": "", "env_count": 0,
              "tg_import": False, "tg_import_error": "",
              "tg_token_found": False, "tg_started": False, "tg_error": ""})


def _log(msg: str) -> None:
    """ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ (stdout + stderr ì–‘ìª½ì— flush)."""
    print(msg, flush=True)
    sys.stderr.write(msg + "\n")
    sys.stderr.flush()


_RE_MD_HEADER = re.compile(r'^#{1,3}\s+(.+)', re.MULTILINE)
_RE_SENTENCE_END = re.compile(r'[.!?ã€‚]\s')

def _extract_title_summary(content: str) -> str:
    """AI ì‘ë‹µ contentì—ì„œ ì‘ì „ì¼ì§€ ì œëª©ìœ¼ë¡œ ì“¸ 1ì¤„ ìš”ì•½ì„ ì¶”ì¶œí•œë‹¤.
    ìš°ì„ ìˆœìœ„: â‘  ë§ˆí¬ë‹¤ìš´ í—¤ë”(#~###) â‘¡ ì²« ë¬¸ì¥(50ì) â‘¢ ì• 80ì ì˜ë¼ë‚´ê¸°
    """
    if not content:
        return ""
    text = content.strip()
    # â‘  ë§ˆí¬ë‹¤ìš´ í—¤ë” ì¶”ì¶œ
    m = _RE_MD_HEADER.search(text)
    if m:
        title = m.group(1).strip().rstrip('#').strip()
        if len(title) > 80:
            title = title[:77] + "..."
        return title
    # â‘¡ ì²« ë¬¸ì¥ ì¶”ì¶œ (ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ/ë¬¼ìŒí‘œ ê¸°ì¤€)
    first_line = text.split('\n')[0].strip()
    # ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ìë¡œ ì‹œì‘í•˜ë©´ ìŠ¤í‚µí•˜ê³  ë³¸ë¬¸ ì°¾ê¸°
    if first_line:
        m2 = _RE_SENTENCE_END.search(first_line)
        if m2 and m2.end() <= 80:
            return first_line[:m2.end()].strip()
        if len(first_line) <= 80:
            return first_line
    # â‘¢ ì• 80ì ì˜ë¼ë‚´ê¸°
    return text[:77].rstrip() + "..." if len(text) > 80 else text


def _load_env_file() -> None:
    """í™˜ê²½ë³€ìˆ˜ íŒŒì¼ì„ ì§ì ‘ ì½ì–´ì„œ os.environì— ì„¤ì •."""
    env_paths = [
        Path("/home/ubuntu/corthex.env"),        # ì„œë²„ ë°°í¬ í™˜ê²½
        Path(__file__).parent.parent / ".env.local",  # ë¡œì»¬ ê°œë°œ í™˜ê²½
        Path(__file__).parent.parent / ".env",        # ë¡œì»¬ í´ë°±
    ]
    for env_path in env_paths:
        _log(f"[ENV] í™•ì¸: {env_path} (ì¡´ì¬: {env_path.exists()})")
        if env_path.exists():
            try:
                loaded = 0
                for line in env_path.read_text(encoding="utf-8").splitlines():
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue
                    if "=" in line:
                        key, _, value = line.partition("=")
                        key = key.strip()
                        value = value.strip()
                        if key:
                            os.environ[key] = value
                            loaded += 1
                _diag["env_loaded"] = True
                _diag["env_file"] = str(env_path)
                _diag["env_count"] = loaded
                tg = os.getenv("TELEGRAM_BOT_TOKEN", "")
                _diag["tg_token_found"] = bool(tg)
                _log(f"[ENV] âœ… {loaded}ê°œ ë¡œë“œ: {env_path}")
                _log(f"[ENV] TG_TOKEN: {bool(tg)} (ê¸¸ì´:{len(tg)})")
            except Exception as e:
                _log(f"[ENV] âŒ ì‹¤íŒ¨: {e}")
            break


_load_env_file()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€ (src/ ëª¨ë“ˆ ì„í¬íŠ¸ìš©)
_PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT)

try:
    import yaml
except ImportError:
    yaml = None  # PyYAML ë¯¸ì„¤ì¹˜ ì‹œ graceful fallback

# â”€â”€ ToolPool â†’ app_state.tool_pool ì§ì ‘ ì‚¬ìš© â”€â”€

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Body, Request
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import Optional

logger = logging.getLogger("corthex.arm_server")

# â”€â”€ í…”ë ˆê·¸ë¨ ë´‡ (ì„ íƒì  ë¡œë“œ) â”€â”€
_telegram_available = False
try:
    from telegram import Update, BotCommand, InlineKeyboardButton, InlineKeyboardMarkup
    from telegram.ext import (
        Application,
        CommandHandler,
        MessageHandler,
        CallbackQueryHandler,
        ContextTypes,
        filters,
    )
    _telegram_available = True
    _diag["tg_import"] = True
    _log("[TG] python-telegram-bot ì„í¬íŠ¸ ì„±ê³µ âœ…")
except ImportError as e:
    _diag["tg_import_error"] = str(e)
    _log(f"[TG] python-telegram-bot ì„í¬íŠ¸ ì‹¤íŒ¨ âŒ: {e}")

KST = timezone(timedelta(hours=9))

app = FastAPI(title="CORTHEX HQ")

# â”€â”€ ì „ì²´ í™œë™ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´ (CEO ìš”ì²­: ì›¹ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼ ì „ë¶€ ë¡œê·¸) â”€â”€
# ì •ì  íŒŒì¼, í—¬ìŠ¤ì²´í¬ ë“± ë…¸ì´ì¦ˆë¥¼ ì œì™¸í•œ ëª¨ë“  API ìš”ì²­ì„ activity_logì— ê¸°ë¡
_LOG_SKIP_PREFIXES = ("/static", "/favicon", "/deploy-status", "/ws", "/api/comms")
_LOG_SKIP_EXACT = {"/", "/api/health", "/api/agents/status", "/api/dashboard/stats",
                   "/api/activity-logs", "/api/batch/chain/status",
                   "/api/budget", "/api/trading/summary", "/api/trading/history",
                   "/api/trading/strategies", "/api/trading/signals",
                   "/api/trading/watchlist/prices"}
_LOG_DESCRIPTION: dict[str, str] = {
    # ì±„íŒ…/AI
    "POST /api/chat": "ğŸ’¬ ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡",
    "POST /api/chat/send": "ğŸ’¬ ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡",
    # ì—ì´ì „íŠ¸
    "GET /api/agents": "ğŸ“‹ ì—ì´ì „íŠ¸ ëª©ë¡ ì¡°íšŒ",
    "GET /api/agents/status": "ğŸ”µ ì—ì´ì „íŠ¸ ìƒíƒœ ì¡°íšŒ",
    # ìë™ë§¤ë§¤
    "POST /api/trading/bot/run-now": "ğŸš€ ì¦‰ì‹œ ë§¤ë§¤ ì‹¤í–‰",
    "POST /api/trading/bot/toggle": "âš¡ ìë™ë§¤ë§¤ ë´‡ ON/OFF",
    "GET /api/trading/portfolio": "ğŸ’° í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ",
    "GET /api/trading/signals": "ğŸ“Š ë§¤ë§¤ ì‹œê·¸ë„ ì¡°íšŒ",
    "GET /api/trading/watchlist": "ğŸ‘ï¸ ê´€ì‹¬ì¢…ëª© ì¡°íšŒ",
    "POST /api/trading/watchlist": "ğŸ‘ï¸ ê´€ì‹¬ì¢…ëª© ì¶”ê°€",
    # KIS
    "GET /api/kis/balance": "ğŸ’³ KIS ì”ê³  ì¡°íšŒ",
    "GET /api/kis/status": "ğŸ”Œ KIS ì—°ê²° ìƒíƒœ",
    # ë°°ì¹˜
    "POST /api/batch/chain/start": "â›“ï¸ ë°°ì¹˜ ì²´ì¸ ì‹œì‘",
    "GET /api/batch/chain/status": "â›“ï¸ ë°°ì¹˜ ì²´ì¸ ìƒíƒœ",
    # ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸
    "GET /api/content-pipeline": "ğŸ“° ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸ í˜„í™©",
    "POST /api/content-pipeline/run": "ğŸš€ ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰",
    "POST /api/content-pipeline/approve": "âœ… ì½˜í…ì¸  ìŠ¹ì¸",
    "POST /api/content-pipeline/reject": "âŒ ì½˜í…ì¸  ê±°ì ˆ",
    # ì•„ì¹´ì´ë¸Œ
    "GET /api/archives": "ğŸ“ ì•„ì¹´ì´ë¸Œ ì¡°íšŒ",
    # ì‘ì—…
    "POST /api/tasks": "ğŸ“ ì‘ì—… ìƒì„±",
    "GET /api/tasks": "ğŸ“ ì‘ì—… ëª©ë¡ ì¡°íšŒ",
    # ì„¤ì •
    "GET /api/settings": "âš™ï¸ ì„¤ì • ì¡°íšŒ",
    "POST /api/settings": "âš™ï¸ ì„¤ì • ì €ì¥",
    # ì›Œí¬í”Œë¡œìš°
    "POST /api/workflows/run": "ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰",
    # ë””ë²„ê·¸
    "GET /api/debug/kis-token": "ğŸ” KIS í† í° ë””ë²„ê·¸",
    "GET /api/debug/auto-trading-pipeline": "ğŸ” ìë™ë§¤ë§¤ íŒŒì´í”„ë¼ì¸ ë””ë²„ê·¸",
}

from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request as StarletteRequest

class ActivityLogMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: StarletteRequest, call_next):
        path = request.url.path
        method = request.method

        # ë…¸ì´ì¦ˆ ì œì™¸
        if path in _LOG_SKIP_EXACT or any(path.startswith(p) for p in _LOG_SKIP_PREFIXES):
            return await call_next(request)

        start = time.time()
        response = await call_next(request)
        elapsed = time.time() - start

        # ë¡œê·¸ ê¸°ë¡ (ë¹„ë™ê¸° WebSocket broadcastëŠ” startup ì´í›„ì—ë§Œ ê°€ëŠ¥)
        key = f"{method} {path}"
        desc = _LOG_DESCRIPTION.get(key, "")
        status = response.status_code
        level = "info" if status < 400 else ("warning" if status < 500 else "error")

        # ì§§ì€ ìš”ì•½ ìƒì„±
        if desc:
            action = f"{desc} ({elapsed:.1f}s)"
        else:
            action = f"ğŸŒ {method} {path} â†’ {status} ({elapsed:.1f}s)"

        try:
            log_entry = save_activity_log("system", action, level)
            # ì‹œìŠ¤í…œ HTTP ë¡œê·¸ëŠ” ë¸Œë¡œë“œìºìŠ¤íŠ¸í•˜ì§€ ì•ŠìŒ (ë…¸ì´ì¦ˆ ê°ì†Œ)
            # ì—ì´ì „íŠ¸ í™œë™ë¡œê·¸ë§Œ ì‹¤ì‹œê°„ ì „ì†¡
        except Exception as e:
            logger.debug("í™œë™ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: %s", e)

        return response

app.add_middleware(ActivityLogMiddleware)

# â”€â”€ HTML ì„œë¹™ â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMPLATE_DIR = os.path.join(BASE_DIR, "templates")

def get_build_number() -> str:
    """ë¹Œë“œ ë²ˆí˜¸ ë°˜í™˜.
    ì‹¤ì œ ë¹Œë“œ ë²ˆí˜¸ëŠ” GitHub Actions ë°°í¬ ì‹œ deploy.ymlì´ HTMLì— ì§ì ‘ ì£¼ì…í•¨.
    ì´ í•¨ìˆ˜ëŠ” ë¡œì»¬ ê°œë°œ í™˜ê²½(ë°°í¬ ì „)ì—ì„œë§Œ ì‚¬ìš©ë˜ëŠ” í´ë°± ê°’ì„ ë°˜í™˜."""
    return "dev"

# â”€â”€ ì„¤ì • íŒŒì¼ì—ì„œ ì—ì´ì „íŠ¸/ë„êµ¬ ì •ë³´ ë¡œë“œ â”€â”€
CONFIG_DIR = Path(BASE_DIR).parent / "config"

def _load_config(name: str) -> dict:
    """ì„¤ì • íŒŒì¼ ë¡œë“œ. JSONì„ ë¨¼ì € ì‹œë„í•˜ê³ , ì—†ìœ¼ë©´ YAMLë¡œ ì‹œë„."""
    # 1ìˆœìœ„: JSON íŒŒì¼ (deploy.ymlì´ ë°°í¬ ì‹œ YAML â†’ JSONìœ¼ë¡œ ë³€í™˜í•´ë‘ )
    json_path = CONFIG_DIR / f"{name}.json"
    if json_path.exists():
        try:
            raw = json.loads(json_path.read_text(encoding="utf-8"))
            logger.info("%s.json ë¡œë“œ ì„±ê³µ", name)
            return raw
        except Exception as e:
            logger.warning("%s.json ë¡œë“œ ì‹¤íŒ¨: %s", name, e)

    # 2ìˆœìœ„: YAML íŒŒì¼ (PyYAML í•„ìš”)
    yaml_path = CONFIG_DIR / f"{name}.yaml"
    if yaml is not None and yaml_path.exists():
        try:
            raw = yaml.safe_load(yaml_path.read_text(encoding="utf-8"))
            logger.info("%s.yaml ë¡œë“œ ì„±ê³µ", name)
            # ë³´í—˜: YAML ì½ì€ í›„ JSONë„ ìë™ ìƒì„± (ë‹¤ìŒ ê¸°ë™ ì‹œ 1ìˆœìœ„ë¡œ ë°”ë¡œ ë¡œë“œ)
            try:
                json_path.write_text(
                    json.dumps(raw, ensure_ascii=False, indent=2), encoding="utf-8"
                )
                logger.info("%s.yaml â†’ %s.json ìë™ ë³€í™˜ ì™„ë£Œ", name, name)
            except Exception as e:
                logger.debug("YAMLâ†’JSON ë³€í™˜ ì €ì¥ ì‹¤íŒ¨: %s", e)
            return raw
        except Exception as e:
            logger.warning("%s.yaml ë¡œë“œ ì‹¤íŒ¨: %s", name, e)

    logger.warning("%s ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ (ë¹ˆ ì„¤ì • ì‚¬ìš©)", name)
    return {}


def _load_agents() -> dict:
    """ì—ì´ì „íŠ¸ë³„ ìƒì„¸ ì •ë³´(allowed_tools, capabilities ë“±)ë¥¼ ë¡œë“œ."""
    raw = _load_config("agents")
    lookup: dict[str, dict] = {}
    for a in raw.get("agents", []):
        lookup[a["agent_id"]] = a
    return lookup


def _load_tools() -> list[dict]:
    """ë„êµ¬ ëª©ë¡ì„ ë¡œë“œ."""
    raw = _load_config("tools")
    return raw.get("tools", [])

# ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½: í•„ìš”í•œ ì •ë³´ë§Œ ìºì‹œ)
_AGENTS_DETAIL: dict[str, dict] = _load_agents()
_TOOLS_LIST: list[dict] = _load_tools()

# â”€â”€ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬ (ëŸ°íƒ€ì„ ë°ì´í„°) â”€â”€
DATA_DIR = Path(BASE_DIR).parent / "data"
DATA_DIR.mkdir(exist_ok=True)
KNOWLEDGE_DIR = Path(BASE_DIR).parent / "knowledge"
ARCHIVE_DIR = Path(BASE_DIR).parent / "archive"


def _load_data(name: str, default=None):
    """DBì—ì„œ ì„¤ì • ë°ì´í„° ë¡œë“œ. DBì— ì—†ìœ¼ë©´ ê¸°ì¡´ JSON íŒŒì¼ í™•ì¸ í›„ ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜."""
    # 1ìˆœìœ„: SQLite DB
    db_val = load_setting(name)
    if db_val is not None:
        return db_val
    # 2ìˆœìœ„: ê¸°ì¡´ JSON íŒŒì¼ (ìë™ ë§ˆì´ê·¸ë ˆì´ì…˜)
    path = DATA_DIR / f"{name}.json"
    if path.exists():
        try:
            val = json.loads(path.read_text(encoding="utf-8"))
            save_setting(name, val)  # DBë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜
            return val
        except Exception as e:
            logger.debug("JSONâ†’DB ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ (%s): %s", name, e)
    return default if default is not None else {}


def _save_data(name: str, data) -> None:
    """DBì— ì„¤ì • ë°ì´í„° ì €ì¥."""
    save_setting(name, data)


def _save_config_file(name: str, data: dict) -> None:
    """ì„¤ì • ë³€ê²½ì„ DBì— ì €ì¥. (ì¬ë°°í¬í•´ë„ ìœ ì§€ë¨)"""
    save_setting(f"config_{name}", data)


def _sync_agent_defaults_to_db():
    """agents.yamlì˜ ì‹ ê·œ ì—ì´ì „íŠ¸ë§Œ agent_overrides DBì— ì¶”ê°€.
    ì´ë¯¸ DBì— ì¡´ì¬í•˜ëŠ” ì—ì´ì „íŠ¸ëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ (ì‚¬ìš©ìê°€ ìˆ˜ë™ ë³€ê²½í•œ ëª¨ë¸ ìœ ì§€)."""
    try:
        agents_config = _load_config("agents")
        if not agents_config:
            return
        agents_list = agents_config.get("agents", [])

        overrides = _load_data("agent_overrides", {})
        changed = False

        for agent_data in agents_list:
            agent_id = agent_data.get("agent_id")
            if not agent_id:
                continue
            model_name = agent_data.get("model_name") or agent_data.get("model")
            reasoning = agent_data.get("reasoning_effort") or agent_data.get("reasoning")
            if not model_name:
                continue
            # DBì— ì—†ëŠ” ì‹ ê·œ ì—ì´ì „íŠ¸ë§Œ yaml ê¸°ë³¸ê°’ ì ìš© (ê¸°ì¡´ ê°’ì€ ë³´ì¡´)
            if agent_id not in overrides:
                overrides[agent_id] = {"model_name": model_name}
                if reasoning:
                    overrides[agent_id]["reasoning_effort"] = reasoning
                changed = True

        if changed:
            _save_data("agent_overrides", overrides)
            logger.info("agent_overrides DB ë™ê¸°í™”: ì‹ ê·œ ì—ì´ì „íŠ¸ %dê±´ ì¶”ê°€", changed)
    except Exception as e:
        logger.warning("agent_overrides ë™ê¸°í™” ì‹¤íŒ¨: %s", e)


@app.get("/", response_class=HTMLResponse)
async def index():
    html_path = os.path.join(TEMPLATE_DIR, "index.html")
    with open(html_path, "r", encoding="utf-8") as f:
        html_content = f.read()

    # BUILD_NUMBER_PLACEHOLDERë¥¼ ì‹¤ì œ ë¹Œë“œ ë²ˆí˜¸ë¡œ ì¹˜í™˜
    build_number = get_build_number()
    html_content = html_content.replace("BUILD_NUMBER_PLACEHOLDER", build_number)

    return HTMLResponse(content=html_content)


@app.get("/sw.js")
async def service_worker():
    """PWA Service Worker â€” root scope í•„ìš”."""
    sw_path = os.path.join(os.path.dirname(__file__), "static", "sw.js")
    from starlette.responses import FileResponse
    return FileResponse(sw_path, media_type="application/javascript")


@app.get("/deploy-status.json")
async def deploy_status():
    """ë°°í¬ ìƒíƒœ JSON (deploy.ymlì´ /var/www/html/ì— ìƒì„±í•œ íŒŒì¼ ì½ê¸°)."""
    import json as _json
    for path in ["/var/www/html/deploy-status.json", os.path.join(BASE_DIR, "deploy-status.json")]:
        if os.path.exists(path):
            try:
                with open(path, "r") as f:
                    return _json.load(f)
            except Exception as e:
                logger.debug("ë°°í¬ ìƒíƒœ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (%s): %s", path, e)
    return {"build": get_build_number(), "time": datetime.now(KST).isoformat(), "status": "success", "commit": ""}


# ëª¨ë¸ë³„ ê¸°ë³¸ ì¶”ë¡  ë ˆë²¨ ìë™ ë§¤í•‘ (ìµœì‹  2026ë…„ ê¸°ì¤€)
MODEL_REASONING_MAP: dict[str, str] = {
    "claude-haiku-4-5-20251001": "low",
    "claude-sonnet-4-6":       "medium",
    "claude-opus-4-6":         "high",
    "gemini-3.1-pro-preview":    "high",
    "gemini-2.5-pro":          "high",
    "gpt-5.2":                 "high",
    "gpt-5.2-pro":             "xhigh",
    "gpt-5":                   "high",
    "gpt-5-mini":              "medium",
    "o3":                      "high",
    "o4-mini":                 "medium",
}

# ëª¨ë¸ë³„ ìµœëŒ€ ì¶œë ¥ í† í° í•œë„ (ê³µì‹ API ê¸°ì¤€, 2026ë…„ 2ì›”)
MODEL_MAX_TOKENS_MAP: dict[str, int] = {
    "claude-haiku-4-5-20251001": 64000,
    "claude-sonnet-4-6":         64000,
    "claude-opus-4-6":           64000,
    "gemini-3.1-pro-preview":      64000,
    "gemini-2.5-pro":            65536,
    "gpt-5.2":                   128000,
    "gpt-5.2-pro":               128000,
    "gpt-5":                     128000,
    "gpt-5-mini":                32768,
    "o3":                        100000,
    "o4-mini":                   65536,
}


# â”€â”€ ì—ì´ì „íŠ¸ ëª©ë¡ (agents.yamlì—ì„œ ë™ì  ë¡œë“œ) â”€â”€
_AGENTS_FALLBACK = [
    {"agent_id": "chief_of_staff", "name_ko": "ë¹„ì„œì‹¤ì¥", "role": "manager", "division": "secretary", "status": "idle", "model_name": "claude-sonnet-4-6"},
    {"agent_id": "cso_manager", "name_ko": "ì „ëµíŒ€ì¥", "role": "manager", "division": "leet_master.strategy", "status": "idle", "model_name": "claude-sonnet-4-6"},
    {"agent_id": "clo_manager", "name_ko": "ë²•ë¬´íŒ€ì¥", "role": "manager", "division": "leet_master.legal", "status": "idle", "model_name": "claude-sonnet-4-6"},
    {"agent_id": "cmo_manager", "name_ko": "ë§ˆì¼€íŒ…íŒ€ì¥", "role": "manager", "division": "leet_master.marketing", "status": "idle", "model_name": "claude-sonnet-4-6"},
    {"agent_id": "cio_manager", "name_ko": "ê¸ˆìœµë¶„ì„íŒ€ì¥", "role": "manager", "division": "finance.investment", "status": "idle", "model_name": "claude-opus-4-6"},
    {"agent_id": "cpo_manager", "name_ko": "ì½˜í…ì¸ íŒ€ì¥", "role": "manager", "division": "publishing", "status": "idle", "model_name": "claude-sonnet-4-6"},
]


def _build_agents_from_yaml() -> list[dict]:
    """agents.yaml(ë˜ëŠ” agents.json)ì—ì„œ AGENTS ë¦¬ìŠ¤íŠ¸ë¥¼ ë™ì  ìƒì„±.
    ë¡œë“œ ì‹¤íŒ¨ ì‹œ _AGENTS_FALLBACK ì‚¬ìš©."""
    try:
        agents_detail = _load_agents()  # _AGENTS_DETAILê³¼ ë™ì¼ ì†ŒìŠ¤
        if not agents_detail:
            _log("[AGENTS] agents.yaml ë¡œë“œ ê²°ê³¼ ë¹„ì–´ìˆìŒ â€” í´ë°± ì‚¬ìš©")
            return list(_AGENTS_FALLBACK)
        result = []
        for aid, detail in agents_detail.items():
            entry = {
                "agent_id": aid,
                "name_ko": detail.get("name_ko", aid),
                "role": detail.get("role", "specialist"),
                "division": detail.get("division", ""),
                "superior_id": detail.get("superior_id", ""),
                "dormant": detail.get("dormant", False),
                "status": "idle",
                "model_name": detail.get("model_name", "claude-sonnet-4-6"),
            }
            if detail.get("telegram_code"):
                entry["telegram_code"] = detail["telegram_code"]
            result.append(entry)
        _log(f"[AGENTS] agents.yamlì—ì„œ {len(result)}ëª… ë¡œë“œ ì™„ë£Œ")
        return result
    except Exception as e:
        _log(f"[AGENTS] agents.yaml ë¡œë“œ ì‹¤íŒ¨ ({e}) â€” í´ë°± ì‚¬ìš©")
        return list(_AGENTS_FALLBACK)


AGENTS = _build_agents_from_yaml()

# â”€â”€ WebSocket ê´€ë¦¬ (wm ì‹±ê¸€í„´ ì‚¬ìš©) â”€â”€
# í•˜ìœ„ í˜¸í™˜: connected_clientsëŠ” wm ë‚´ë¶€ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¸ì¡°
connected_clients = wm._connections

# â”€â”€ ë°±ê·¸ë¼ìš´ë“œ ì—ì´ì „íŠ¸ íƒœìŠ¤í¬ (ìƒˆë¡œê³ ì¹¨í•´ë„ ì•ˆ ëŠê¹€) â”€â”€
# â†’ app_stateë¡œ ì´ë™. í•˜ìœ„ í˜¸í™˜ alias (dict/listëŠ” ê³µìœ  ì°¸ì¡°ë¡œ ë™ì‘)
_bg_tasks = app_state.bg_tasks
_bg_results = app_state.bg_results
# app_state.bg_current_task_idëŠ” primitive(ì¬í• ë‹¹)ì´ë¯€ë¡œ app_state.bg_current_task_id ì§ì ‘ ì‚¬ìš©


@app.websocket("/ws")
async def websocket_endpoint(ws: WebSocket):
    await ws.accept()
    wm._connections.append(ws)
    try:
        # ì—°ê²° ì‹œ ì´ˆê¸° ìƒíƒœ ì „ì†¡ (activity_logê°€ ì•„ë‹Œ system_info ì´ë²¤íŠ¸ ì‚¬ìš© â€” í†µì‹ ë¡œê·¸ì— ì•ˆ ëœ¨ê²Œ)
        now = datetime.now(KST).strftime("%H:%M:%S")
        await ws.send_json({
            "event": "system_info",
            "data": {
                "message": "ì‹œìŠ¤í…œ ì—°ê²° ì™„ë£Œ. ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.",
                "time": now,
            }
        })
        # ì—°ê²° ì§í›„ ì˜¤ëŠ˜ ë¹„ìš©ì„ ì „ì†¡ â†’ ìš°ì¸¡ ìƒë‹¨ $0.0000 ë¬¸ì œ í•´ê²°
        try:
            today_cost = get_today_cost()
            await ws.send_json({
                "event": "cost_update",
                "data": {"total_cost": today_cost, "total_tokens": 0},
            })
        except Exception as e:
            logger.debug("WS ë¹„ìš© ì „ì†¡ ì‹¤íŒ¨: %s", e)
        # ìƒˆë¡œê³ ì¹¨ ë³µêµ¬: ì§„í–‰ ì¤‘ì¸ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ê°€ ìˆìœ¼ë©´ ìƒíƒœ ì „ì†¡
        if app_state.bg_current_task_id and app_state.bg_current_task_id in _bg_tasks:
            try:
                await ws.send_json({
                    "event": "agent_status",
                    "data": {
                        "agent_id": "chief_of_staff",
                        "status": "working",
                        "progress": 0.5,
                        "detail": "ì—ì´ì „íŠ¸ ì‘ì—… ì§„í–‰ì¤‘ (ìƒˆë¡œê³ ì¹¨ ë³µêµ¬)",
                        "task_id": app_state.bg_current_task_id,
                    },
                })
            except Exception as e:
                logger.debug("WS ìƒíƒœ ì „ì†¡ ì‹¤íŒ¨: %s", e)
        while True:
            data = await ws.receive_text()
            # ë©”ì‹œì§€ í¬ê¸° ì œí•œ (64KB) â€” ë¹„ì •ìƒì ìœ¼ë¡œ í° í˜ì´ë¡œë“œ ì°¨ë‹¨
            if len(data) > 65536:
                await ws.send_json({"event": "error", "data": {"message": "ë©”ì‹œì§€ í¬ê¸° ì´ˆê³¼ (64KB ì œí•œ)"}})
                continue
            msg = json.loads(data)
            # ë©”ì‹œì§€ë¥¼ ë°›ìœ¼ë©´ DBì— ì €ì¥ + ì‘ë‹µ
            if msg.get("type") == "cancel":
                # ì·¨ì†Œ ìš”ì²­: DBì—ì„œ running íƒœìŠ¤í¬ë¥¼ cancelledë¡œ ë³€ê²½
                cancel_tid = msg.get("task_id")
                if cancel_tid:
                    update_task(cancel_tid, status="failed",
                                result_summary="CEO ì·¨ì†Œ", success=0)
                else:
                    # task_id ì—†ìœ¼ë©´ running íƒœìŠ¤í¬ ì „ë¶€ ì·¨ì†Œ
                    try:
                        running = list_tasks(status="running", limit=10)
                        for rt in running:
                            update_task(rt["task_id"], status="failed",
                                        result_summary="CEO ì·¨ì†Œ", success=0)
                    except Exception as e:
                        logger.debug("íƒœìŠ¤í¬ ì¼ê´„ ì·¨ì†Œ ì‹¤íŒ¨: %s", e)
                continue
            if msg.get("type") == "command":
                cmd_text = (msg.get("content") or msg.get("text", "")).strip()
                use_batch = msg.get("batch", False)
                ws_target_agent_id = msg.get("target_agent_id", None)
                ws_conversation_id = msg.get("conversation_id", None)
                if cmd_text:
                    # DBì— ë©”ì‹œì§€ + ì‘ì—… ì €ì¥
                    task = create_task(cmd_text, source="websocket_batch" if use_batch else "websocket")
                    save_message(cmd_text, source="websocket",
                                 task_id=task["task_id"])
                    # ì‘ì—… ì ‘ìˆ˜ ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸
                    mode_label = "ğŸ“¦ ë°°ì¹˜" if use_batch else "âš¡ ì‹¤ì‹œê°„"
                    log_entry = save_activity_log(
                        "chief_of_staff",
                        f"[ì›¹] {mode_label} ëª…ë ¹ ì ‘ìˆ˜: {cmd_text[:50]}{'...' if len(cmd_text) > 50 else ''} (#{task['task_id']})",
                    )
                    await wm.broadcast_multi([
                        ("task_accepted", task),
                        ("activity_log", log_entry),
                    ])

                    # ë°°ì¹˜ ëª¨ë“œ: ìœ„ì„ ì²´ì¸ ì „ì²´ë¥¼ Batch APIë¡œ ì‹¤í–‰
                    if use_batch and is_ai_ready():
                        update_task(task["task_id"], status="pending",
                                    result_summary="ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] ì‹œì‘ ì¤‘...")
                        # ì¦‰ì‹œ ì ‘ìˆ˜ ì‘ë‹µ â†’ ëŒ€í™”ì°½ ë°”ë¡œ í’€ë¦¼ (ë°°ì¹˜ëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
                        await ws.send_json({
                            "event": "result",
                            "data": {
                                "content": (
                                    f"ğŸ“¦ **ë°°ì¹˜ ì ‘ìˆ˜ ì™„ë£Œ** (#{task['task_id']})\n\n"
                                    f"ë°°ì¹˜ ì²´ì¸ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n"
                                    f"ê° ë‹¨ê³„ ì™„ë£Œ ì‹œ ìë™ìœ¼ë¡œ ì§„í–‰ë˜ë©°, "
                                    f"ìµœì¢… ë³´ê³ ì„œê°€ ì™„ì„±ë˜ë©´ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\n"
                                    f"ğŸ’¡ ëŒ€í™”ë¥¼ ê³„ì†í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                                ),
                                "sender_id": "chief_of_staff",
                                "handled_by": "ë¹„ì„œì‹¤ì¥",
                                "time_seconds": 0,
                                "cost": 0,
                            }
                        })

                        # ë°°ì¹˜ ì²´ì¸ì„ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰ (ëŒ€í™” ì°¨ë‹¨ ì—†ìŒ)
                        async def _run_batch_chain(text, task_id, ws_ref):
                            try:
                                chain_result = await _start_batch_chain(text, task_id)
                                if "error" in chain_result:
                                    await wm.broadcast("batch_chain_progress", {"message": f"âŒ ë°°ì¹˜ ì‹œì‘ ì‹¤íŒ¨: {chain_result['error']}"})
                            except Exception as e:
                                _log(f"[CHAIN] ë°±ê·¸ë¼ìš´ë“œ ë°°ì¹˜ ì²´ì¸ ì˜¤ë¥˜: {e}")

                        asyncio.create_task(_run_batch_chain(cmd_text, task["task_id"], ws))
                        continue

                    # í† ë¡  ëª…ë ¹: ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ì±„íŒ… ì°¨ë‹¨ ì—†ìŒ)
                    _stripped = cmd_text.strip()
                    is_debate_cmd = _stripped.startswith("/í† ë¡ ") or _stripped.startswith("/ì‹¬ì¸µí† ë¡ ")
                    if is_ai_ready() and is_debate_cmd:
                        debate_rounds = 3 if _stripped.startswith("/ì‹¬ì¸µí† ë¡ ") else 2
                        await ws.send_json({
                            "event": "result",
                            "data": {
                                "content": (
                                    f"ğŸ—£ï¸ **ì„ì› í† ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤** ({debate_rounds}ë¼ìš´ë“œ)\n\n"
                                    f"íŒ€ì¥ 6ëª…ì´ í† ë¡  ì¤‘ì…ë‹ˆë‹¤. 2~5ë¶„ ì†Œìš”ë©ë‹ˆë‹¤.\n"
                                    f"**í† ë¡ ì´ ì™„ë£Œë˜ë©´ ìë™ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì „ë‹¬í•´ë“œë¦½ë‹ˆë‹¤.**\n"
                                    f"ğŸ’¡ í† ë¡ ì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆ ì±„íŒ…ì„ ê³„ì† ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                                ),
                                "sender_id": "chief_of_staff",
                                "handled_by": f"ì„ì› í† ë¡  ì‹œì‘ ({debate_rounds}ë¼ìš´ë“œ)",
                                "time_seconds": 0,
                                "cost": 0,
                            }
                        })

                        async def _run_debate_bg(text, task_id):
                            try:
                                update_task(task_id, status="running")
                                debate_result = await _process_ai_command(text, task_id)
                                if "error" in debate_result:
                                    update_task(task_id, status="failed",
                                                result_summary=str(debate_result.get("error", ""))[:200],
                                                success=0)
                                else:
                                    update_task(task_id, status="completed",
                                                result_summary=_extract_title_summary(debate_result.get("content", "") or ""),
                                                success=1,
                                                cost_usd=debate_result.get("total_cost_usd", debate_result.get("cost_usd", 0)))
                                if "error" in debate_result:
                                    await wm.broadcast("result", {
                                        "content": f"âŒ í† ë¡  ì‹¤íŒ¨: {debate_result['error']}",
                                        "sender_id": "chief_of_staff",
                                        "time_seconds": 0,
                                        "cost": 0,
                                    })
                                else:
                                    await wm.broadcast("result", {
                                        "content": debate_result.get("content", ""),
                                        "sender_id": debate_result.get("agent_id", "chief_of_staff"),
                                        "handled_by": debate_result.get("handled_by", "ì„ì› í† ë¡ "),
                                        "time_seconds": debate_result.get("time_seconds", 0),
                                        "cost": debate_result.get("total_cost_usd", debate_result.get("cost_usd", 0)),
                                    })
                                # í† ë¡  ê²°ê³¼ë„ í…”ë ˆê·¸ë¨ CEO ì „ë‹¬
                                if "error" not in debate_result:
                                    await _forward_web_response_to_telegram(
                                        text,
                                        {
                                            "content": debate_result.get("content", ""),
                                            "handled_by": debate_result.get("handled_by", "ì„ì› í† ë¡ "),
                                            "cost": debate_result.get("total_cost_usd", debate_result.get("cost_usd", 0)),
                                        },
                                    )
                            except Exception as e:
                                _log(f"[DEBATE] ë°±ê·¸ë¼ìš´ë“œ í† ë¡  ì˜¤ë¥˜: {e}")

                        asyncio.create_task(_run_debate_bg(cmd_text, task["task_id"]))
                        continue

                    # ì‹¤ì‹œê°„ ëª¨ë“œ: ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰ (ìƒˆë¡œê³ ì¹¨í•´ë„ ì•ˆ ëŠê¹€)
                    if is_ai_ready():
                        update_task(task["task_id"], status="running")
                        app_state.bg_current_task_id = task["task_id"]
                        asyncio.create_task(
                            _run_agent_bg(cmd_text, task["task_id"], ws_target_agent_id, ws_conversation_id)
                        )
                    else:
                        update_task(task["task_id"], status="completed",
                                    result_summary="AI ë¯¸ì—°ê²° â€” ì ‘ìˆ˜ë§Œ ì™„ë£Œ",
                                    success=1, time_seconds=0.1)
                        await ws.send_json({
                            "event": "result",
                            "data": {
                                "content": "AIê°€ ì•„ì§ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ANTHROPIC_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                                "sender_id": "chief_of_staff",
                                "time_seconds": 0.1,
                                "cost": 0,
                            }
                        })
    except WebSocketDisconnect:
        wm.disconnect(ws)
    except Exception:
        wm.disconnect(ws)


# â”€â”€ ë°±ê·¸ë¼ìš´ë“œ ì—ì´ì „íŠ¸ ì‹¤í–‰ (ìƒˆë¡œê³ ì¹¨í•´ë„ ì•ˆ ëŠê¹€) â”€â”€

async def _run_agent_bg(cmd_text: str, task_id: str, target_agent_id: str | None = None,
                        conversation_id: str | None = None):
    """ì—ì´ì „íŠ¸ ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰. WebSocket ì—°ê²°ê³¼ ë¬´ê´€í•˜ê²Œ ë™ì‘."""

    _bg_tasks[task_id] = asyncio.current_task()
    try:
        result = await _process_ai_command(cmd_text, task_id, target_agent_id=target_agent_id,
                                           conversation_id=conversation_id)
        if "error" in result:
            update_task(task_id, status="failed",
                        result_summary=result.get("error", "")[:200],
                        success=0, time_seconds=0)
            _result_payload = {
                "content": f"âŒ {result['error']}",
                "sender_id": result.get("agent_id", "chief_of_staff"),
                "handled_by": result.get("handled_by", "ë¹„ì„œì‹¤ì¥"),
                "time_seconds": 0, "cost": 0, "task_id": task_id,
            }
            try:
                save_conversation_message(
                    "result", content=_result_payload["content"],
                    sender_id=_result_payload["sender_id"],
                    handled_by=_result_payload["handled_by"],
                    time_seconds=0, cost=0, task_id=task_id, source="web",
                    conversation_id=conversation_id,
                )
            except Exception as e:
                logger.debug("ì—ëŸ¬ ê²°ê³¼ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: %s", e)
            _result_payload["_completed_at"] = time.time()
            _bg_results[task_id] = _result_payload
            await wm.broadcast("result", _result_payload)
        else:
            _result_data = {
                "content": result.get("content", ""),
                "sender_id": result.get("agent_id", "chief_of_staff"),
                "handled_by": result.get("handled_by", "ë¹„ì„œì‹¤ì¥"),
                "delegation": result.get("delegation", ""),
                "time_seconds": result.get("time_seconds", 0),
                "cost": result.get("total_cost_usd", result.get("cost_usd", 0)),
                "model": result.get("model", ""),
                "routing_method": result.get("routing_method", ""),
                "task_id": task_id,
            }
            try:
                save_conversation_message(
                    "result", content=_result_data["content"],
                    sender_id=_result_data["sender_id"],
                    handled_by=_result_data["handled_by"],
                    delegation=_result_data.get("delegation", ""),
                    model=_result_data.get("model", ""),
                    time_seconds=_result_data.get("time_seconds", 0),
                    cost=_result_data.get("cost", 0),
                    task_id=task_id, source="web",
                    conversation_id=conversation_id,
                )
            except Exception as e:
                logger.debug("ê²°ê³¼ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: %s", e)
            # ëŒ€í™” ì„¸ì…˜ ë¹„ìš© ëˆ„ì 
            if conversation_id and _result_data.get("cost"):
                try:
                    from db import get_conversation, update_conversation
                    _conv = get_conversation(conversation_id)
                    if _conv:
                        update_conversation(conversation_id,
                                            total_cost=_conv["total_cost"] + _result_data["cost"])
                except Exception:
                    pass
            _result_data["_completed_at"] = time.time()
            _bg_results[task_id] = _result_data
            await wm.broadcast("result", _result_data)
            update_task(task_id, status="completed",
                        result_summary=_extract_title_summary(result.get("content", "") or ""),
                        success=1,
                        time_seconds=result.get("time_seconds", 0),
                        cost_usd=result.get("total_cost_usd", result.get("cost_usd", 0)),
                        agent_id=result.get("agent_id", "chief_of_staff"))
            await _forward_web_response_to_telegram(cmd_text, _result_data)
    except Exception as e:
        _log(f"[BG-AGENT] ë°±ê·¸ë¼ìš´ë“œ ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
        update_task(task_id, status="failed", result_summary=str(e)[:200], success=0, agent_id="chief_of_staff")
        _bg_results[task_id] = {"content": f"âŒ ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}", "sender_id": "chief_of_staff", "task_id": task_id, "_completed_at": time.time()}
        await wm.broadcast("result", _bg_results[task_id])
    finally:
        _bg_tasks.pop(task_id, None)
        app_state.bg_current_task_id = None


# â”€â”€ ë¯¸ë””ì–´ API â†’ handlers/media_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.media_handler import router as media_router
app.include_router(media_router)

# â”€â”€ API ì—”ë“œí¬ì¸íŠ¸ â”€â”€

# â”€â”€ ì¸ì¦(Auth) API â†’ handlers/auth_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.auth_handler import router as auth_router, check_auth as _check_auth
app.include_router(auth_router)


# â”€â”€ ì—ì´ì „íŠ¸ ê´€ë¦¬ API â†’ handlers/agent_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.agent_handler import router as agent_router
app.include_router(agent_router)

# â”€â”€ ë„êµ¬(Tool) API â†’ handlers/tools_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.tools_handler import router as tools_router
app.include_router(tools_router)


@app.get("/api/dashboard")
async def get_dashboard():
    now = datetime.now(KST).isoformat()
    stats = get_dashboard_stats()
    today_cost = get_today_cost()
    daily_limit = float(load_setting("daily_budget_usd") or 7.0)

    # â”€â”€ í”„ë¡œë°”ì´ë”ë³„ ì˜¤ëŠ˜ AI í˜¸ì¶œ íšŸìˆ˜ â”€â”€
    provider_calls = {"anthropic": 0, "openai": 0, "google": 0}
    try:
        conn = __import__("db").get_connection()
        # KST ìì •ì„ UTCë¡œ ë³€í™˜ (DBëŠ” UTC ISO í˜•ì‹ìœ¼ë¡œ ì €ì¥ë¨)
        _kst_midnight = datetime.now(KST).replace(hour=0, minute=0, second=0, microsecond=0)
        today_start = _kst_midnight.astimezone(timezone.utc).strftime("%Y-%m-%dT%H:%M:%S")
        rows = conn.execute(
            "SELECT provider, COUNT(*) FROM agent_calls "
            "WHERE created_at >= ? GROUP BY provider", (today_start,)
        ).fetchall()
        for row in rows:
            p = (row[0] or "").lower()
            if p in provider_calls:
                provider_calls[p] = row[1]
        conn.close()
    except Exception as e:
        logger.debug("í”„ë¡œë°”ì´ë” í˜¸ì¶œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: %s", e)
    total_ai_calls = sum(provider_calls.values())

    # â”€â”€ ë°°ì¹˜ í˜„í™© â”€â”€
    chains = load_setting("batch_chains") or []
    batch_active = len([c for c in chains if c.get("status") in ("running", "pending")])
    batch_done = len([c for c in chains if c.get("status") == "completed"])

    # â”€â”€ ë„êµ¬ ìˆ˜ â”€â”€
    tool_count = 0
    try:
        pool = _init_tool_pool()
        if pool:
            tool_count = len(pool.registry)
    except Exception as e:
        logger.debug("ë„êµ¬ í’€ ì¹´ìš´íŠ¸ ì‹¤íŒ¨: %s", e)
    if tool_count == 0:
        tool_count = len(_load_tool_schemas().get("anthropic", []))

    # â”€â”€ API í‚¤ ìƒíƒœ â”€â”€
    api_keys = {
        "anthropic": get_available_providers().get("anthropic", False),
        "google": get_available_providers().get("google", False),
        "openai": get_available_providers().get("openai", False),
        "notion": bool(os.getenv("NOTION_API_KEY", "")),
        "telegram": bool(os.getenv("TELEGRAM_BOT_TOKEN", "")),
    }
    api_connected = sum(1 for v in api_keys.values() if v)
    api_total = len(api_keys)

    # â”€â”€ ì‹œìŠ¤í…œ ìƒíƒœ íŒë‹¨ (ìµœê·¼ 1ì‹œê°„ ì‹¤íŒ¨ 3ê±´ ì´ìƒ â†’ ì´ìƒ) â”€â”€
    recent_failed = 0
    try:
        one_hour_ago = (datetime.now(KST) - timedelta(hours=1)).isoformat()
        _conn_tmp = __import__("db").get_connection()
        recent_failed = _conn_tmp.execute(
            "SELECT COUNT(*) FROM tasks WHERE created_at >= ? AND status = 'failed'",
            (one_hour_ago,),
        ).fetchone()[0]
        _conn_tmp.close()
    except Exception as e:
        logger.debug("ìµœê·¼ ì‹¤íŒ¨ ê±´ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: %s", e)
    if recent_failed >= 3:
        sys_status = "error"
    elif stats["running_count"] > 0:
        sys_status = "busy"
    else:
        sys_status = "ok"

    return {
        "total_agents": len(AGENTS),
        "active_agents": stats["running_count"],
        "idle_agents": len(AGENTS) - stats["running_count"],
        "total_tasks_today": stats["today_task_count"],
        "today_completed": stats["today_completed"],
        "today_failed": stats["today_failed"],
        "total_cost": stats["total_cost"],
        "today_cost": today_cost,
        "total_tokens": stats["total_tokens"],
        "notion_connected": bool(_NOTION_API_KEY),
        "system_status": sys_status,
        "uptime": now,
        "agents": AGENTS,
        "recent_completed": stats["recent_completed"],
        "api_keys": api_keys,
        # â”€â”€ Cì•ˆ: ëŒ€ì‹œë³´ë“œ í™•ì¥ ë°ì´í„° â”€â”€
        "provider_calls": provider_calls,
        "total_ai_calls": total_ai_calls,
        "daily_limit": daily_limit,
        "batch_active": batch_active,
        "batch_done": batch_done,
        "tool_count": tool_count,
        "api_connected": api_connected,
        "api_total": api_total,
    }


# â”€â”€ ì˜ˆì‚°(Budget) Â· ëª¨ë¸ëª¨ë“œ â†’ handlers/agent_handler.pyë¡œ ë¶„ë¦¬ â”€â”€

# â”€â”€ í’ˆì§ˆê²€ìˆ˜ í†µê³„ + í”„ë¦¬ì…‹ â†’ handlers/quality_handler.py, handlers/preset_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.preset_handler import router as preset_router
app.include_router(preset_router)


# â”€â”€ ì„±ëŠ¥/ì‘ì—… (ì½ê¸° ì „ìš© â€” ì‹¤ì œ ë°ì´í„°ëŠ” í’€ ì„œë²„ì—ì„œ ìƒì„±) â”€â”€

@app.get("/api/performance")
async def get_performance():
    """ì—ì´ì „íŠ¸ë³„ ì‹¤ì œ ì„±ëŠ¥ í†µê³„ë¥¼ DBì—ì„œ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    from db import get_connection
    conn = get_connection()
    try:
        # DBì—ì„œ ì—ì´ì „íŠ¸ë³„ ì‘ì—… í†µê³„ ì§‘ê³„
        rows = conn.execute("""
            SELECT agent_id,
                   COUNT(*) as total_tasks,
                   SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as completed,
                   SUM(CASE WHEN success = 0 THEN 1 ELSE 0 END) as failed,
                   COALESCE(SUM(cost_usd), 0) as total_cost,
                   COALESCE(AVG(time_seconds), 0) as avg_time,
                   COALESCE(SUM(tokens_used), 0) as total_tokens
            FROM tasks
            WHERE agent_id IS NOT NULL AND agent_id != ''
            GROUP BY agent_id
            ORDER BY total_tasks DESC
        """).fetchall()

        # ì—ì´ì „íŠ¸ ì´ë¦„/ì—­í•  ë§µ êµ¬ì¶•
        agent_map = {a["agent_id"]: a for a in AGENTS}

        agents_perf = []
        total_llm_calls = 0
        total_cost = 0.0

        for row in rows:
            aid = row["agent_id"]
            info = agent_map.get(aid, {})
            total = row["total_tasks"]
            completed = row["completed"] or 0
            rate = round(completed / total * 100, 1) if total > 0 else 0

            agents_perf.append({
                "agent_id": aid,
                "name_ko": info.get("name_ko", aid),
                "role": info.get("role", "unknown"),
                "division": info.get("division", ""),
                "llm_calls": total,
                "tasks_completed": completed,
                "tasks_failed": row["failed"] or 0,
                "success_rate": rate,
                "cost_usd": round(row["total_cost"], 6),
                "avg_execution_seconds": round(row["avg_time"], 2),
                "total_tokens": row["total_tokens"] or 0,
            })
            total_llm_calls += total
            total_cost += row["total_cost"]

        # DBì— ì‘ì—…ì´ ì•„ì§ ì—†ìœ¼ë©´ ì—ì´ì „íŠ¸ ëª©ë¡ë§Œ ë¹ˆ ê°’ìœ¼ë¡œ ë°˜í™˜
        if not agents_perf:
            for a in AGENTS:
                agents_perf.append({
                    "agent_id": a["agent_id"],
                    "name_ko": a["name_ko"],
                    "role": a["role"],
                    "division": a.get("division", ""),
                    "llm_calls": 0,
                    "tasks_completed": 0,
                    "tasks_failed": 0,
                    "success_rate": 0,
                    "cost_usd": 0,
                    "avg_execution_seconds": 0,
                    "total_tokens": 0,
                })

        # agent_calls í…Œì´ë¸” ë°ì´í„°ë¥¼ agents_perfì— ë³‘í•©
        # (ìŠ¤í˜ì…œë¦¬ìŠ¤íŠ¸ ë“± tasksì— ì—†ëŠ” ì—ì´ì „íŠ¸ë„ í¬í•¨ì‹œí‚¤ê¸° ìœ„í•¨)
        try:
            from db import get_agent_performance
            agent_perf = get_agent_performance()
        except Exception:
            agent_perf = []

        perf_map = {ap["agent_id"]: ap for ap in agents_perf}
        for ap in agent_perf:
            aid = ap["agent_id"]
            if aid in perf_map:
                # tasksì— ì´ë¯¸ ìˆëŠ” ì—ì´ì „íŠ¸ â†’ agent_calls ìˆ˜ì¹˜ í•©ì‚°
                existing = perf_map[aid]
                existing["llm_calls"] += ap.get("call_count", 0)
                existing["cost_usd"] = round(
                    existing["cost_usd"] + ap.get("total_cost", 0), 6
                )
                existing["total_tokens"] += (
                    ap.get("total_input_tokens", 0) + ap.get("total_output_tokens", 0)
                )
                total_llm_calls += ap.get("call_count", 0)
                total_cost += ap.get("total_cost", 0)
            else:
                # tasksì— ì—†ëŠ” ì—ì´ì „íŠ¸ (ìŠ¤í˜ì…œë¦¬ìŠ¤íŠ¸ ë“±) â†’ ìƒˆë¡œ ì¶”ê°€
                info = agent_map.get(aid, {})
                call_count = ap.get("call_count", 0)
                cost = ap.get("total_cost", 0)
                new_entry = {
                    "agent_id": aid,
                    "name_ko": info.get("name_ko", aid),
                    "role": info.get("role", "unknown"),
                    "division": info.get("division", ""),
                    "llm_calls": call_count,
                    "tasks_completed": 0,
                    "tasks_failed": 0,
                    "success_rate": ap.get("success_rate", 0),
                    "cost_usd": round(cost, 6),
                    "avg_execution_seconds": ap.get("avg_time", 0),
                    "total_tokens": (
                        ap.get("total_input_tokens", 0)
                        + ap.get("total_output_tokens", 0)
                    ),
                }
                agents_perf.append(new_entry)
                perf_map[aid] = new_entry
                total_llm_calls += call_count
                total_cost += cost

        return {
            "agents": agents_perf,
            "total_llm_calls": total_llm_calls,
            "total_cost_usd": round(total_cost, 6),
        }
    except Exception as e:
        logger.error("ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: %s", e)
        # ì—ëŸ¬ ì‹œì—ë„ ì—ì´ì „íŠ¸ ëª©ë¡ì€ ë³´ì—¬ì£¼ê¸°
        return {
            "agents": [{"agent_id": a["agent_id"], "name_ko": a["name_ko"],
                        "role": a["role"], "llm_calls": 0, "tasks_completed": 0,
                        "success_rate": 0, "cost_usd": 0, "avg_execution_seconds": 0}
                       for a in AGENTS],
            "total_llm_calls": 0,
            "total_cost_usd": 0,
            "agent_calls": [],
        }
    finally:
        conn.close()


# â”€â”€ ì‘ì—…(Task) API â†’ handlers/task_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.task_handler import router as task_router
app.include_router(task_router)


# â”€â”€ ë°°ì¹˜ ëª…ë ¹ (ì—¬ëŸ¬ ëª…ë ¹ í•œë²ˆì— ì‹¤í–‰) â”€â”€

# â†’ app_stateë¡œ ì´ë™. alias (listëŠ” ê³µìœ  ì°¸ì¡°)
_batch_queue = app_state.batch_queue
_batch_api_queue = app_state.batch_api_queue
# app_state.batch_runningì€ primitive â†’ app_state.batch_running ì§ì ‘ ì‚¬ìš©


@app.get("/api/batch/queue")
async def get_batch_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ ì¡°íšŒ."""
    return {"queue": _batch_queue, "running": app_state.batch_running}


@app.post("/api/batch")
async def submit_batch(request: Request):
    """ë°°ì¹˜ ëª…ë ¹ ì œì¶œ â€” ì—¬ëŸ¬ ëª…ë ¹ì„ í•œë²ˆì— ì ‘ìˆ˜í•©ë‹ˆë‹¤."""
    body = await request.json()
    commands = body.get("commands", [])
    mode = body.get("mode", "sequential")  # sequential ë˜ëŠ” parallel

    if not commands:
        return {"success": False, "error": "ëª…ë ¹ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

    batch_id = f"batch_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}"
    batch_items = []
    for i, cmd in enumerate(commands):
        item = {
            "batch_id": batch_id,
            "index": i,
            "command": cmd if isinstance(cmd, str) else cmd.get("command", ""),
            "status": "pending",
            "result": None,
            "task_id": None,
        }
        batch_items.append(item)
        _batch_queue.append(item)

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë°°ì¹˜ ì‹¤í–‰
    asyncio.create_task(_run_batch(batch_id, batch_items, mode))

    return {"success": True, "batch_id": batch_id, "count": len(commands), "mode": mode}


async def _run_batch(batch_id: str, items: list, mode: str):
    """ë°°ì¹˜ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""

    app_state.batch_running = True

    try:
        if mode == "parallel":
            # ë³‘ë ¬ ì‹¤í–‰
            tasks = []
            for item in items:
                tasks.append(_run_batch_item(item))
            await asyncio.gather(*tasks)
        else:
            # ìˆœì°¨ ì‹¤í–‰
            for item in items:
                await _run_batch_item(item)
    finally:
        app_state.batch_running = False
        # ì™„ë£Œëœ ë°°ì¹˜ í•­ëª©ì€ 10ë¶„ í›„ ì •ë¦¬
        await asyncio.sleep(600)
        for item in items:
            if item in _batch_queue:
                _batch_queue.remove(item)


async def _run_batch_item(item: dict):
    """ë°°ì¹˜ ë‚´ ê°œë³„ ëª…ë ¹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    item["status"] = "running"
    try:
        task = create_task(item["command"], source="batch")
        item["task_id"] = task["task_id"]

        # AI ì²˜ë¦¬
        result = await _process_ai_command(item["command"], task["task_id"])

        item["status"] = "completed"
        item["result"] = result.get("content", "")[:200] if isinstance(result, dict) else str(result)[:200]
        # R-3: ì „ë ¥ë¶„ì„ ë°ì´í„°ìš© agent_id ê¸°ë¡
        agent_id = result.get("agent_id", "chief_of_staff") if isinstance(result, dict) else "chief_of_staff"
        update_task(task["task_id"], agent_id=agent_id)
    except Exception as e:
        item["status"] = "failed"
        item["result"] = str(e)[:200]


@app.delete("/api/batch/queue")
async def clear_batch_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ì„ ë¹„ì›ë‹ˆë‹¤."""
    _batch_queue[:] = [item for item in _batch_queue if item.get("status") == "running"]
    return {"success": True}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€ AI Batch API ì‹œìŠ¤í…œ (PENDING ì¶”ì  + ìë™ ê²°ê³¼ ìˆ˜ì§‘) â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# CEOê°€ ì—¬ëŸ¬ ëª…ë ¹ì„ AI Batch APIë¡œ ë³´ë‚´ë©´:
#   1) ê° ëª…ë ¹ì´ PENDING ìƒíƒœë¡œ DBì— ì €ì¥ë¨
#   2) í”„ë¡œë°”ì´ë”ì˜ Batch APIì— í•œêº¼ë²ˆì— ì œì¶œ (ì‹¤ì‹œê°„ë³´ë‹¤ ~50% ì €ë ´)
#   3) ë°±ê·¸ë¼ìš´ë“œ í´ëŸ¬ê°€ 60ì´ˆë§ˆë‹¤ ìƒíƒœë¥¼ í™•ì¸
#   4) ì™„ë£Œë˜ë©´ ìë™ìœ¼ë¡œ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„í•˜ì—¬ ë³´ê³ ì„œ ì‘ì„±
#   5) WebSocketìœ¼ë¡œ CEOì—ê²Œ ì‹¤ì‹œê°„ ì•Œë¦¼

# app_state.batch_poller_task â†’ app_state.batch_poller_task ì§ì ‘ ì‚¬ìš©


@app.post("/api/batch/ai")
async def submit_ai_batch(request: Request):
    """AI Batch APIë¡œ ì—¬ëŸ¬ ìš”ì²­ì„ í•œêº¼ë²ˆì— ì œì¶œí•©ë‹ˆë‹¤.

    ìš”ì²­ body:
    {
        "requests": [
            {"message": "ì‚¼ì„±ì „ì ë¶„ì„í•´ì¤˜", "system_prompt": "...", "agent_id": "cio_manager"},
            {"message": "íŠ¹í—ˆ ê²€ìƒ‰í•´ì¤˜", "system_prompt": "...", "agent_id": "clo_manager"},
        ],
        "model": "claude-sonnet-4-6",  // ê¸°ë³¸ ëª¨ë¸ (ì„ íƒ)
        "auto_delegate": true  // ê²°ê³¼ë¥¼ ì—ì´ì „íŠ¸ì—ê²Œ ìë™ ìœ„ì„í• ì§€ (ê¸°ë³¸: true)
    }

    ì‘ë‹µ: {"batch_id": "...", "count": N, "status": "submitted"}
    """
    body = await request.json()
    requests_list = body.get("requests", [])
    model = body.get("model")
    auto_delegate = body.get("auto_delegate", True)

    if not requests_list:
        return {"success": False, "error": "ìš”ì²­ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

    # ê° ìš”ì²­ì— custom_id ìë™ ë¶€ì—¬
    now_str = datetime.now(KST).strftime('%Y%m%d_%H%M%S')
    for i, req in enumerate(requests_list):
        if "custom_id" not in req:
            req["custom_id"] = f"batch_{now_str}_{i}"
        # ì—ì´ì „íŠ¸ ì†Œìš¸(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)ì„ ìë™ìœ¼ë¡œ ë¡œë“œ
        agent_id = req.get("agent_id")
        if agent_id and not req.get("system_prompt"):
            req["system_prompt"] = _load_agent_prompt(agent_id)

    # Batch API ì œì¶œ
    result = await batch_submit(requests_list, model=model)

    if "error" in result:
        return {"success": False, "error": result["error"]}

    batch_id = result["batch_id"]
    provider = result["provider"]

    # DBì— PENDING ìƒíƒœë¡œ ì €ì¥
    pending_data = {
        "batch_id": batch_id,
        "provider": provider,
        "model": model,
        "status": "pending",
        "auto_delegate": auto_delegate,
        "submitted_at": datetime.now(KST).isoformat(),
        "requests": [
            {
                "custom_id": r.get("custom_id"),
                "message": r.get("message", "")[:200],
                "agent_id": r.get("agent_id", ""),
            }
            for r in requests_list
        ],
        "results": [],
    }

    # ê¸°ì¡´ pending_batches ëª©ë¡ì— ì¶”ê°€
    pending_batches = load_setting("pending_batches") or []
    pending_batches.append(pending_data)
    save_setting("pending_batches", pending_batches)

    # ê° ìš”ì²­ì„ taskë¡œë„ ìƒì„± (PENDING ìƒíƒœ)
    for req in requests_list:
        task = create_task(
            req.get("message", "ë°°ì¹˜ ìš”ì²­"),
            source="batch_api",
            agent_id=req.get("agent_id", "chief_of_staff"),
        )
        update_task(task["task_id"], status="pending",
                    result_summary=f"[PENDING] ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ (batch_id: {batch_id[:20]}...)")

    # WebSocket ì•Œë¦¼
    await wm.broadcast("batch_submitted", {
        "batch_id": batch_id,
        "provider": provider,
        "count": len(requests_list),
    })

    _log(f"[BATCH] AI ë°°ì¹˜ ì œì¶œ ì™„ë£Œ: {batch_id} ({len(requests_list)}ê°œ ìš”ì²­, {provider})")

    # í´ëŸ¬ê°€ ì•ˆ ëŒê³  ìˆìœ¼ë©´ ì‹œì‘
    _ensure_batch_poller()

    return {
        "success": True,
        "batch_id": batch_id,
        "provider": provider,
        "count": len(requests_list),
        "status": "submitted",
    }


@app.get("/api/batch/pending")
async def get_pending_batches():
    """PENDING ìƒíƒœì¸ ë°°ì¹˜ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    pending_batches = load_setting("pending_batches") or []
    # pendingê³¼ processingë§Œ ë°˜í™˜
    active = [b for b in pending_batches if b.get("status") in ("pending", "processing")]
    return {"pending": active, "total": len(pending_batches)}


@app.post("/api/batch/check/{batch_id}")
async def check_batch_status(batch_id: str):
    """íŠ¹ì • ë°°ì¹˜ì˜ ìƒíƒœë¥¼ ìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤."""
    pending_batches = load_setting("pending_batches") or []
    batch_info = next((b for b in pending_batches if b["batch_id"] == batch_id), None)

    if not batch_info:
        return {"error": f"ë°°ì¹˜ '{batch_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

    provider = batch_info["provider"]
    status_result = await batch_check(batch_id, provider)

    if "error" in status_result:
        return status_result

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    batch_info["status"] = status_result["status"]
    batch_info["progress"] = status_result.get("progress", {})
    save_setting("pending_batches", pending_batches)

    # ì™„ë£Œë˜ì—ˆìœ¼ë©´ ê²°ê³¼ ìˆ˜ì§‘
    if status_result["status"] == "completed":
        await _collect_batch_results(batch_info, pending_batches)

    return status_result


@app.post("/api/batch/resume")
async def resume_all_pending():
    """ëª¨ë“  PENDING ë°°ì¹˜ì˜ ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , ì™„ë£Œëœ ê²ƒì€ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    pending_batches = load_setting("pending_batches") or []
    active = [b for b in pending_batches if b.get("status") in ("pending", "processing")]

    if not active:
        return {"message": "ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤", "checked": 0}

    checked = 0
    collected = 0
    for batch_info in active:
        batch_id = batch_info["batch_id"]
        provider = batch_info["provider"]

        status_result = await batch_check(batch_id, provider)
        if "error" not in status_result:
            batch_info["status"] = status_result["status"]
            batch_info["progress"] = status_result.get("progress", {})
            checked += 1

            if status_result["status"] == "completed":
                await _collect_batch_results(batch_info, pending_batches)
                collected += 1

    save_setting("pending_batches", pending_batches)
    return {"checked": checked, "collected": collected, "remaining": len(active) - collected}


@app.get("/api/batch/history")
async def get_batch_history():
    """ëª¨ë“  ë°°ì¹˜ì˜ íˆìŠ¤í† ë¦¬ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì™„ë£Œëœ ê²ƒ í¬í•¨)."""
    all_batches = load_setting("pending_batches") or []
    return {"batches": all_batches[-50:], "total": len(all_batches)}  # ìµœê·¼ 50ê°œë§Œ


async def _collect_batch_results(batch_info: dict, all_batches: list):
    """ì™„ë£Œëœ ë°°ì¹˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì§‘í•˜ê³ , í•„ìš”ì‹œ ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„í•©ë‹ˆë‹¤."""
    batch_id = batch_info["batch_id"]
    provider = batch_info["provider"]

    _log(f"[BATCH] ê²°ê³¼ ìˆ˜ì§‘ ì‹œì‘: {batch_id}")

    # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    result = await batch_retrieve(batch_id, provider)
    if "error" in result:
        _log(f"[BATCH] ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {result['error']}")
        return

    results = result.get("results", [])
    batch_info["results"] = results
    batch_info["status"] = "completed"
    batch_info["completed_at"] = datetime.now(KST).isoformat()

    # ì´ ë¹„ìš© ê³„ì‚°
    total_cost = sum(r.get("cost_usd", 0) for r in results if r.get("cost_usd"))
    batch_info["total_cost_usd"] = round(total_cost, 6)

    save_setting("pending_batches", all_batches)

    # ì—ì´ì „íŠ¸ì—ê²Œ ìë™ ìœ„ì„ (auto_delegate=trueì¸ ê²½ìš°)
    if batch_info.get("auto_delegate"):
        req_map = {r["custom_id"]: r for r in batch_info.get("requests", [])}
        for res in results:
            if res.get("error"):
                continue
            custom_id = res.get("custom_id", "")
            req_info = req_map.get(custom_id, {})
            agent_id = req_info.get("agent_id")
            message = req_info.get("message", "")

            if agent_id and res.get("content"):
                # ê²°ê³¼ë¥¼ í™œë™ ë¡œê·¸ì— ê¸°ë¡
                agent_name = _AGENT_NAMES.get(agent_id, agent_id)
                log_entry = save_activity_log(
                    agent_id,
                    f"[ë°°ì¹˜ ì™„ë£Œ] {agent_name}: {message[:40]}... â†’ {res['content'][:60]}..."
                )
                await wm.send_activity_log(log_entry)

                # ì•„ì¹´ì´ë¸Œì— ì €ì¥
                division = _AGENT_DIVISION.get(agent_id, "secretary")
                now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
                archive_content = f"# [ë°°ì¹˜] [{agent_name}] {message[:60]}\n\n{res['content']}"
                save_archive(
                    division=division,
                    filename=f"batch_{agent_id}_{now_str}.md",
                    content=archive_content,
                    agent_id=agent_id,
                )

    # WebSocketìœ¼ë¡œ ì™„ë£Œ ì•Œë¦¼
    await wm.broadcast("batch_completed", {
        "batch_id": batch_id,
        "provider": provider,
        "count": len(results),
        "total_cost_usd": total_cost,
        "succeeded": sum(1 for r in results if not r.get("error")),
        "failed": sum(1 for r in results if r.get("error")),
    })

    _log(f"[BATCH] ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ: {batch_id} ({len(results)}ê°œ, ${total_cost:.4f})")


async def _flush_batch_api_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ì— ìŒ“ì¸ ìš”ì²­ì„ Batch APIì— ì œì¶œí•©ë‹ˆë‹¤."""
    if not _batch_api_queue:
        return {"message": "ëŒ€ê¸°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

    queue_copy = list(_batch_api_queue)
    _batch_api_queue.clear()

    _log(f"[BATCH] ëŒ€ê¸°ì—´ {len(queue_copy)}ê±´ â†’ Batch API ì œì¶œ ì¤‘...")

    # ê° ìš”ì²­ì— ì—ì´ì „íŠ¸ ë¼ìš°íŒ… (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê²°ì •)
    for req in queue_copy:
        if not req.get("system_prompt"):
            routing = await _route_task(req.get("message", ""))
            agent_id = routing.get("agent_id", "chief_of_staff")
            req["agent_id"] = agent_id
            req["system_prompt"] = _load_agent_prompt(agent_id)

    # Batch API ì œì¶œ â€” í”„ë¡œë°”ì´ë”ë³„ë¡œ ìë™ ê·¸ë£¹í™” (Claude/GPT/Gemini ìš”ì²­ì´ ì„ì—¬ë„ ê°ê° ì˜¬ë°”ë¥¸ APIë¡œ ì „ì†¡)
    batch_results = await batch_submit_grouped(queue_copy)

    # ì „ë¶€ ì‹¤íŒ¨í•œ ê²½ìš° ëŒ€ê¸°ì—´ì— ë³µêµ¬
    all_failed = all("error" in br for br in batch_results)
    if all_failed:
        first_error = batch_results[0].get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜") if batch_results else "ê²°ê³¼ ì—†ìŒ"
        _log(f"[BATCH] ì œì¶œ ì‹¤íŒ¨ (ì „ì²´): {first_error}")
        _batch_api_queue.extend(queue_copy)
        return {"error": first_error}

    # ì„±ê³µí•œ ë°°ì¹˜ë“¤ì„ DBì— PENDING ìƒíƒœë¡œ ì €ì¥
    pending_batches = load_setting("pending_batches") or []
    submitted_ids = []
    for result in batch_results:
        if "error" in result:
            _log(f"[BATCH] í”„ë¡œë°”ì´ë” {result.get('provider','?')} ì œì¶œ ì‹¤íŒ¨: {result['error']}")
            continue

        batch_id = result["batch_id"]
        provider = result["provider"]
        custom_ids_in_batch = result.get("custom_ids", [])

        # ì´ ë°°ì¹˜ì— í¬í•¨ëœ ìš”ì²­ë§Œ í•„í„°ë§
        reqs_in_batch = [r for r in queue_copy if r.get("custom_id", r.get("task_id", "")) in custom_ids_in_batch]

        pending_data = {
            "batch_id": batch_id,
            "provider": provider,
            "status": "pending",
            "auto_delegate": True,
            "submitted_at": datetime.now(KST).isoformat(),
            "requests": [
                {
                    "custom_id": r.get("custom_id", r.get("task_id", "")),
                    "message": r.get("message", "")[:200],
                    "agent_id": r.get("agent_id", ""),
                    "task_id": r.get("task_id", ""),
                }
                for r in reqs_in_batch
            ],
            "results": [],
        }
        pending_batches.append(pending_data)
        submitted_ids.append(batch_id)

        # ê° taskë¥¼ PENDING ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
        for req in reqs_in_batch:
            task_id = req.get("task_id")
            if task_id:
                update_task(task_id, status="pending",
                            result_summary=f"[PENDING] Batch API ì œì¶œë¨ ({batch_id[:20]}...)")

        # WebSocket ì•Œë¦¼
        await wm.broadcast("batch_submitted", {"batch_id": batch_id, "provider": provider, "count": len(reqs_in_batch)})

        _log(f"[BATCH] Batch API ì œì¶œ ì™„ë£Œ: {batch_id} ({len(reqs_in_batch)}ê±´, {provider})")

    save_setting("pending_batches", pending_batches)
    _ensure_batch_poller()

    # ì²« ë²ˆì§¸ ì„±ê³µ ê²°ê³¼ë¥¼ ë°˜í™˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
    first_success = next((r for r in batch_results if "error" not in r), batch_results[0] if batch_results else {})
    return first_success


@app.post("/api/batch/flush")
async def flush_batch_queue():
    """ë°°ì¹˜ ëŒ€ê¸°ì—´ì— ìŒ“ì¸ ìš”ì²­ì„ ì¦‰ì‹œ Batch APIì— ì œì¶œí•©ë‹ˆë‹¤."""
    if not _batch_api_queue:
        return {"success": False, "message": "ëŒ€ê¸°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}
    result = await _flush_batch_api_queue()
    return {"success": "error" not in result, **result}


def _ensure_batch_poller():
    """ë°°ì¹˜ í´ëŸ¬ê°€ ëŒê³  ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì•ˆ ëŒë©´ ì‹œì‘í•©ë‹ˆë‹¤."""

    if app_state.batch_poller_task is None or app_state.batch_poller_task.done():
        app_state.batch_poller_task = asyncio.create_task(_batch_poller_loop())
        _log("[BATCH] ë°°ì¹˜ í´ëŸ¬ ì‹œì‘ë¨ (60ì´ˆ ê°„ê²©)")


async def _batch_poller_loop():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ 60ì´ˆë§ˆë‹¤ PENDING ë°°ì¹˜ + ë°°ì¹˜ ì²´ì¸ì„ í™•ì¸í•©ë‹ˆë‹¤."""
    while True:
        try:
            await asyncio.sleep(60)

            has_work = False

            # â”€â”€ (A) ê¸°ì¡´ ë‹¨ë… ë°°ì¹˜ í™•ì¸ â”€â”€
            pending_batches = load_setting("pending_batches") or []
            active = [b for b in pending_batches if b.get("status") in ("pending", "processing")]

            if active:
                has_work = True
                for batch_info in active:
                    batch_id = batch_info["batch_id"]
                    provider = batch_info["provider"]

                    try:
                        status_result = await batch_check(batch_id, provider)
                        if "error" not in status_result:
                            batch_info["status"] = status_result["status"]
                            batch_info["progress"] = status_result.get("progress", {})

                            if status_result["status"] == "completed":
                                await _collect_batch_results(batch_info, pending_batches)
                            elif status_result["status"] in ("failed", "expired"):
                                batch_info["status"] = status_result["status"]
                                _log(f"[BATCH] ë°°ì¹˜ ì‹¤íŒ¨/ë§Œë£Œ: {batch_id}")
                    except Exception as e:
                        _log(f"[BATCH] ë°°ì¹˜ í™•ì¸ ì‹¤íŒ¨ ({batch_id}): {e}")

                save_setting("pending_batches", pending_batches)

            # â”€â”€ (B) ë°°ì¹˜ ì²´ì¸ í™•ì¸ + ìë™ ì§„í–‰ â”€â”€
            chains = load_setting("batch_chains") or []
            active_chains = [c for c in chains if c.get("status") in ("running", "pending")]

            if active_chains:
                has_work = True
                for chain in active_chains:
                    try:
                        await _advance_batch_chain(chain["chain_id"])
                    except Exception as e:
                        _log(f"[CHAIN] ì²´ì¸ ì§„í–‰ ì˜¤ë¥˜ ({chain['chain_id']}): {e}")

            if not has_work:
                _log("[BATCH] ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜/ì²´ì¸ ì—†ìŒ â€” í´ëŸ¬ ì¢…ë£Œ")
                break

        except asyncio.CancelledError:
            break
        except Exception as e:
            _log(f"[BATCH] í´ëŸ¬ ì˜¤ë¥˜: {e}")
            await asyncio.sleep(30)  # ì—ëŸ¬ ì‹œ 30ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# â”€â”€ ë°°ì¹˜ ì²´ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° â”€â”€
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# CEOê°€ ğŸ“¦ ë°°ì¹˜ ëª¨ë“œë¡œ ëª…ë ¹ì„ ë³´ë‚´ë©´ ìœ„ì„ ì²´ì¸ ì „ì²´ê°€ Batch APIë¡œ ëŒì•„ê°:
#
#   [1ë‹¨ê³„] ë¹„ì„œì‹¤ì¥ ë¶„ë¥˜ â†’ Batch ì œì¶œ â†’ PENDING â†’ ê²°ê³¼: "CIOì—ê²Œ ìœ„ì„"
#   [2ë‹¨ê³„] ì „ë¬¸ê°€ Nëª… â†’ í”„ë¡œë°”ì´ë”ë³„ ë¬¶ì–´ì„œ Batch ì œì¶œ â†’ PENDING â†’ ì „ë¶€ ëŒ€ê¸°
#   [3ë‹¨ê³„] íŒ€ì¥ ì¢…í•©ë³´ê³ ì„œ â†’ Batch ì œì¶œ â†’ PENDING â†’ ê²°ê³¼: ì¢…í•© ë³´ê³ ì„œ
#   [4ë‹¨ê³„] CEOì—ê²Œ ì „ë‹¬ + ì•„ì¹´ì´ë¸Œ ì €ì¥
#
# ë§¤ ë‹¨ê³„ë§ˆë‹¤ Batch API ì‚¬ìš© â†’ ë¹„ìš© ~50% ì ˆê°
# í”„ë¡œë°”ì´ë”ë³„ ìë™ ê·¸ë£¹í™” (Claude + GPT + Gemini ì—ì´ì „íŠ¸ í˜¼í•© ê°€ëŠ¥)

# ë¶„ë¥˜ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë°°ì¹˜ ì²´ì¸ì—ì„œ ì‚¬ìš©)
_BATCH_CLASSIFY_PROMPT = """ë‹¹ì‹ ì€ ì—…ë¬´ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
CEOì˜ ëª…ë ¹ì„ ì½ê³  ì–´ëŠ ë¶€ì„œê°€ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

## ë¶€ì„œ ëª©ë¡
- cto_manager: ê¸°ìˆ ê°œë°œ (ì½”ë“œ, ì›¹ì‚¬ì´íŠ¸, API, ì„œë²„, ë°°í¬, í”„ë¡ íŠ¸ì—”ë“œ, ë°±ì—”ë“œ, ë²„ê·¸, UI, ë””ìì¸, ë°ì´í„°ë² ì´ìŠ¤)
- cso_manager: ì‚¬ì—…ê¸°íš (ì‹œì¥ì¡°ì‚¬, ì‚¬ì—…ê³„íš, ë§¤ì¶œ ì˜ˆì¸¡, ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸, ìˆ˜ìµ, ê²½ìŸì‚¬)
- clo_manager: ë²•ë¬´IP (ì €ì‘ê¶Œ, íŠ¹í—ˆ, ìƒí‘œ, ì•½ê´€, ê³„ì•½, ë²•ë¥ , ì†Œì†¡)
- cmo_manager: ë§ˆì¼€íŒ…ê³ ê° (ë§ˆì¼€íŒ…, ê´‘ê³ , SNS, ì¸ìŠ¤íƒ€ê·¸ë¨, ìœ íŠœë¸Œ, ì½˜í…ì¸ , ë¸Œëœë”©, ì„¤ë¬¸)
- cio_manager: íˆ¬ìë¶„ì„ (ì£¼ì‹, íˆ¬ì, ì¢…ëª©, ì‹œí™©, í¬íŠ¸í´ë¦¬ì˜¤, ì½”ìŠ¤í”¼, ë‚˜ìŠ¤ë‹¥, ì°¨íŠ¸, ê¸ˆë¦¬)
- cpo_manager: ì¶œíŒê¸°ë¡ (íšŒì‚¬ê¸°ë¡, ì—°ëŒ€ê¸°, ë¸”ë¡œê·¸, ì¶œíŒ, í¸ì§‘, íšŒê³ , ë¹Œë”©ë¡œê·¸)
- chief_of_staff: ì¼ë°˜ ì§ˆë¬¸, ìš”ì•½, ì¼ì • ê´€ë¦¬, ê¸°íƒ€ (ìœ„ ë¶€ì„œì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°)

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
{"agent_id": "ë¶€ì„œID", "reason": "í•œì¤„ ì´ìœ "}"""


def _save_chain(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ ìƒíƒœë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤."""
    chains = load_setting("batch_chains") or []
    # ê°™ì€ chain_idê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì¶”ê°€
    found = False
    for i, c in enumerate(chains):
        if c["chain_id"] == chain["chain_id"]:
            chains[i] = chain
            found = True
            break
    if not found:
        chains.append(chain)
    # ìµœê·¼ 50ê°œë§Œ ìœ ì§€ (ì˜¤ë˜ëœ ì™„ë£Œ/ì‹¤íŒ¨ ì²´ì¸ ì •ë¦¬)
    if len(chains) > 50:
        active = [c for c in chains if c.get("status") in ("running", "pending")]
        done = [c for c in chains if c.get("status") not in ("running", "pending")]
        chains = active + done[-20:]
    save_setting("batch_chains", chains)


def _load_chain(chain_id: str) -> dict | None:
    """DBì—ì„œ ë°°ì¹˜ ì²´ì¸ ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    chains = load_setting("batch_chains") or []
    for c in chains:
        if c["chain_id"] == chain_id:
            return c
    return None


async def _broadcast_chain_status(chain: dict, message: str):
    """ë°°ì¹˜ ì²´ì¸ ì§„í–‰ ìƒí™©ì„ WebSocketìœ¼ë¡œ CEOì—ê²Œ ì•Œë¦½ë‹ˆë‹¤."""
    step_labels = {
        "classify": "1ë‹¨ê³„: ë¶„ë¥˜",
        "delegation": "2ë‹¨ê³„: íŒ€ì¥ ì§€ì‹œì„œ",
        "specialists": "3ë‹¨ê³„: ì „ë¬¸ê°€ ë¶„ì„",
        "synthesis": "4ë‹¨ê³„: ì¢…í•© ë³´ê³ ì„œ",
        "completed": "ì™„ë£Œ",
        "failed": "ì‹¤íŒ¨",
        "direct": "ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬",
    }
    step_label = step_labels.get(chain.get("step", ""), chain.get("step", ""))
    await wm.broadcast("batch_chain_progress", {
        "chain_id": chain["chain_id"],
        "step": chain.get("step", ""),
        "step_label": step_label,
        "status": chain.get("status", ""),
        "message": message,
        "mode": chain.get("mode", "single"),
        "target_id": chain.get("target_id"),
    })

    # í…”ë ˆê·¸ë¨ìœ¼ë¡œë„ ì§„í–‰ ìƒíƒœ ì „ë‹¬
    if app_state.telegram_app:
        ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
        if ceo_id:
            try:
                await app_state.telegram_app.bot.send_message(
                    chat_id=int(ceo_id),
                    text=f"ğŸ“¦ {message}",
                )
            except Exception as e:
                logger.debug("TG ë°°ì¹˜ ì§„í–‰ ì „ì†¡ ì‹¤íŒ¨: %s", e)


async def _start_batch_chain(text: str, task_id: str) -> dict:
    """ë°°ì¹˜ ì²´ì¸ì„ ì‹œì‘í•©ë‹ˆë‹¤.

    CEO ëª…ë ¹ì„ ë°›ì•„ì„œ ìœ„ì„ ì²´ì¸ ì „ì²´ë¥¼ Batch APIë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë˜ë©´ ë¶„ë¥˜ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ë°”ë¡œ ì „ë¬¸ê°€ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.
    """
    chain_id = f"chain_{datetime.now(KST).strftime('%Y%m%d_%H%M%S')}_{task_id[:8]}"

    correlation_id = f"batch_{chain_id}"

    chain = {
        "chain_id": chain_id,
        "task_id": task_id,
        "correlation_id": correlation_id,
        "text": text,
        "mode": "broadcast" if _is_broadcast_command(text) else "single",
        "step": "classify",
        "status": "running",
        "target_id": None,
        "batches": {"classify": None, "specialists": [], "synthesis": []},
        "results": {"classify": None, "specialists": {}, "synthesis": {}},
        "custom_id_map": {},  # custom_id â†’ {"agent_id", "step"} ì—­ë§¤í•‘
        "delegation_instructions": {},  # íŒ€ì¥ ì§€ì‹œì„œ (ë‹¨ì¼ ë¶€ì„œ)
        "broadcast_delegations": {},  # íŒ€ì¥ ì§€ì‹œì„œ (ë¸Œë¡œë“œìºìŠ¤íŠ¸)
        "total_cost_usd": 0.0,
        "created_at": datetime.now(KST).isoformat(),
        "completed_at": None,
    }

    # taskì— correlation_id ì—°ê²°
    update_task(task_id, correlation_id=correlation_id)

    # ì˜ˆì‚° í™•ì¸
    limit = float(load_setting("daily_budget_usd") or 7.0)
    today = get_today_cost()
    if today >= limit:
        update_task(task_id, status="failed",
                    result_summary=f"ì¼ì¼ ì˜ˆì‚° ì´ˆê³¼ (${today:.2f}/${limit:.0f})",
                    success=0)
        return {"error": f"ì¼ì¼ ì˜ˆì‚°ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ (${today:.2f}/${limit:.0f})"}

    # â”€â”€ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ëª¨ë“œ â†’ ë¶„ë¥˜ ê±´ë„ˆë›°ê³  íŒ€ì¥ ì§€ì‹œì„œ â†’ ì „ ë¶€ì„œ ì „ë¬¸ê°€ â”€â”€
    if chain["mode"] == "broadcast":
        chain["step"] = "delegation"
        chain["target_id"] = "broadcast"
        _save_chain(chain)

        await _broadcast_chain_status(chain, "ğŸ“¦ ë°°ì¹˜ ì²´ì¸ ì‹œì‘ (ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ ë¶€ì„œ â†’ íŒ€ì¥ ì§€ì‹œì„œ ìƒì„± ì¤‘)")
        await _chain_create_delegation_broadcast(chain)
        return {"chain_id": chain_id, "status": "started", "mode": "broadcast", "step": chain["step"]}

    # â”€â”€ í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œë„ (ë¬´ë£Œ, ì¦‰ì‹œ) â”€â”€
    keyword_match = _classify_by_keywords(text)
    if keyword_match:
        chain["target_id"] = keyword_match
        chain["results"]["classify"] = {
            "agent_id": keyword_match,
            "method": "í‚¤ì›Œë“œ",
            "cost_usd": 0,
        }

        if keyword_match == "chief_of_staff":
            # ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬ â†’ ë°”ë¡œ ì¢…í•©(=ì§ì ‘ ë‹µë³€) ë‹¨ê³„
            chain["step"] = "synthesis"
            _save_chain(chain)
            await _broadcast_chain_status(chain, "ğŸ“¦ í‚¤ì›Œë“œ ë¶„ë¥˜ â†’ ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬")
            await _chain_submit_synthesis(chain)
        else:
            # íŒ€ì¥ ë¶€ì„œë¡œ ìœ„ì„ â†’ íŒ€ì¥ ì§€ì‹œì„œ â†’ ì „ë¬¸ê°€ í˜¸ì¶œ ë‹¨ê³„
            chain["step"] = "delegation"
            _save_chain(chain)
            target_name = _AGENT_NAMES.get(keyword_match, keyword_match)
            await _broadcast_chain_status(chain, f"ğŸ“¦ í‚¤ì›Œë“œ ë¶„ë¥˜ â†’ {target_name} ì§€ì‹œì„œ ìƒì„± ì¤‘")
            await _chain_create_delegation(chain)

        return {"chain_id": chain_id, "status": "started", "step": chain["step"]}

    # â”€â”€ AI ë¶„ë¥˜ê°€ í•„ìš” â†’ Batch APIë¡œ ë¶„ë¥˜ ìš”ì²­ ì œì¶œ â”€â”€
    # ê°€ì¥ ì €ë ´í•œ ì‚¬ìš© ê°€ëŠ¥ ëª¨ë¸ ì„ íƒ (Gemini Flash â†’ GPT Mini â†’ Claude)
    providers = get_available_providers()
    if providers.get("google"):
        classify_model = "gemini-2.5-flash"
    elif providers.get("openai"):
        classify_model = "gpt-5-mini"
    elif providers.get("anthropic"):
        classify_model = "claude-sonnet-4-6"
    else:
        # AI ì—†ìŒ â†’ ë¹„ì„œì‹¤ì¥ ì§ì ‘
        chain["target_id"] = "chief_of_staff"
        chain["step"] = "synthesis"
        chain["results"]["classify"] = {"agent_id": "chief_of_staff", "method": "í´ë°±"}
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return {"chain_id": chain_id, "status": "started", "step": "synthesis"}

    classify_custom_id = f"{chain_id}_classify"
    classify_req = {
        "custom_id": classify_custom_id,
        "message": text,
        "system_prompt": _BATCH_CLASSIFY_PROMPT,
        "model": classify_model,
        "max_tokens": 1024,
    }

    result = await batch_submit([classify_req], model=classify_model)

    if "error" in result:
        # ë°°ì¹˜ ì‹¤íŒ¨ â†’ í´ë°±ìœ¼ë¡œ ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬
        _log(f"[CHAIN] ë¶„ë¥˜ ë°°ì¹˜ ì‹¤íŒ¨: {result['error']} â†’ ë¹„ì„œì‹¤ì¥ í´ë°±")
        chain["target_id"] = "chief_of_staff"
        chain["step"] = "synthesis"
        chain["results"]["classify"] = {"agent_id": "chief_of_staff", "method": "í´ë°±", "error": result["error"]}
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return {"chain_id": chain_id, "status": "started", "step": "synthesis"}

    chain["batches"]["classify"] = {
        "batch_id": result["batch_id"],
        "provider": result["provider"],
        "status": "pending",
    }
    chain["status"] = "pending"
    chain["custom_id_map"][classify_custom_id] = {"agent_id": "classify", "step": "classify"}
    _save_chain(chain)

    _ensure_batch_poller()
    update_task(task_id, status="pending",
                result_summary="ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 1ë‹¨ê³„: ë¶„ë¥˜ ìš”ì²­ ì œì¶œë¨")
    await _broadcast_chain_status(chain, "ğŸ“¦ ë°°ì¹˜ ì²´ì¸ ì‹œì‘ â€” 1ë‹¨ê³„: ë¶„ë¥˜ ìš”ì²­ ì œì¶œë¨")

    _log(f"[CHAIN] ì‹œì‘: {chain_id} â€” ë¶„ë¥˜ ë°°ì¹˜ ì œì¶œ (batch_id: {result['batch_id']})")
    return {"chain_id": chain_id, "status": "pending", "step": "classify"}


# â”€â”€ íŒ€ì¥ ì§€ì‹œì„œ ìƒì„± í”„ë¡¬í”„íŠ¸ â”€â”€
_DELEGATION_PROMPT = """ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. CEOë¡œë¶€í„° ì•„ë˜ ì—…ë¬´ ì§€ì‹œë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.

ì†Œì† ì „ë¬¸ê°€ë“¤ì—ê²Œ ê°ê° êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œë¥¼ ë‚´ë ¤ì•¼ í•©ë‹ˆë‹¤.
ê° ì „ë¬¸ê°€ì˜ ì „ë¬¸ ë¶„ì•¼ì— ë§ê²Œ CEO ëª…ë ¹ì„ ì„¸ë¶€ ì—…ë¬´ë¡œ ë¶„í•´í•˜ì„¸ìš”.

## ì†Œì† ì „ë¬¸ê°€
{spec_list}

## CEO ëª…ë ¹
{text}

## ì¶œë ¥ í˜•ì‹
ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.
ê° ì „ë¬¸ê°€ IDë¥¼ í‚¤ë¡œ, êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ(2~4ë¬¸ì¥)ë¥¼ ê°’ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

{json_example}"""


async def _chain_create_delegation(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” íŒ€ì¥ì´ ì „ë¬¸ê°€ë³„ ì§€ì‹œì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤ (ì‹¤ì‹œê°„ API 1íšŒ í˜¸ì¶œ).

    ë¶„ë¥˜ ì™„ë£Œ í›„, ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œ ì „ì— í˜¸ì¶œë©ë‹ˆë‹¤.
    íŒ€ì¥ì—ê²Œ CEO ëª…ë ¹ì„ ì „ë‹¬í•˜ê³ , ê° ì „ë¬¸ê°€ì—ê²Œ ë‚´ë¦´ êµ¬ì²´ì  ì§€ì‹œì„œë¥¼ ë°›ìŠµë‹ˆë‹¤.
    """
    target_id = chain["target_id"]
    text = chain["text"]
    specialists = _MANAGER_SPECIALISTS.get(target_id, [])

    if not specialists:
        # ì „ë¬¸ê°€ ì—†ìŒ â†’ ì§€ì‹œì„œ ìƒì„± ë¶ˆí•„ìš”
        await _chain_submit_specialists(chain)
        return

    mgr_name = _AGENT_NAMES.get(target_id, target_id)

    # ì „ë¬¸ê°€ ëª©ë¡ í…ìŠ¤íŠ¸ ìƒì„±
    spec_list_parts = []
    json_example_parts = []
    for s_id in specialists:
        s_name = _SPECIALIST_NAMES.get(s_id, s_id)
        spec_list_parts.append(f"- {s_id}: {s_name}")
        json_example_parts.append(f'  "{s_id}": "êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ ë‚´ìš©"')

    spec_list = "\n".join(spec_list_parts)
    json_example = "{\n" + ",\n".join(json_example_parts) + "\n}"

    delegation_prompt = _DELEGATION_PROMPT.format(
        mgr_name=mgr_name,
        spec_list=spec_list,
        text=text,
        json_example=json_example,
    )

    # ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ë¡œ ì‹¤ì‹œê°„ API í˜¸ì¶œ (Gemini Flash â†’ GPT Mini â†’ Claude)
    providers = get_available_providers()
    if providers.get("google"):
        deleg_model = "gemini-2.5-flash"
    elif providers.get("openai"):
        deleg_model = "gpt-5-mini"
    elif providers.get("anthropic"):
        deleg_model = "claude-sonnet-4-6"
    else:
        deleg_model = None

    if deleg_model:
        # íŒ€ì¥ ì´ˆë¡ë¶ˆ ì¼œê¸°
        await _broadcast_status(target_id, "working", 0.2, f"{mgr_name} ì§€ì‹œì„œ ì‘ì„± ì¤‘...")
        try:
            result = await ask_ai(
                user_message=delegation_prompt,
                model=deleg_model,
                max_tokens=2048,
            )
            response_text = result.get("content", "") or result.get("text", "")

            # JSON íŒŒì‹± ì‹œë„
            import json as _json
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` ë˜ëŠ” { ... })
            json_text = response_text.strip()
            if "```" in json_text:
                # ì½”ë“œ ë¸”ë¡ì—ì„œ ì¶”ì¶œ
                start = json_text.find("{")
                end = json_text.rfind("}") + 1
                if start >= 0 and end > start:
                    json_text = json_text[start:end]
            elif json_text.startswith("{"):
                pass  # ì´ë¯¸ JSON
            else:
                # { ì°¾ê¸°
                start = json_text.find("{")
                end = json_text.rfind("}") + 1
                if start >= 0 and end > start:
                    json_text = json_text[start:end]

            instructions = _json.loads(json_text)
            if isinstance(instructions, dict):
                chain["delegation_instructions"] = instructions
                chain["results"]["delegation"] = {
                    "agent_id": target_id,
                    "instructions": instructions,
                    "model": deleg_model,
                    "cost_usd": result.get("cost_usd", 0),
                }
                chain["total_cost_usd"] += result.get("cost_usd", 0)
                _log(f"[CHAIN] {chain['chain_id']} â€” {mgr_name} ì§€ì‹œì„œ ìƒì„± ì™„ë£Œ ({len(instructions)}ëª…)")
            else:
                _log(f"[CHAIN] {chain['chain_id']} â€” ì§€ì‹œì„œ íŒŒì‹± ì‹¤íŒ¨ (dict ì•„ë‹˜)")
        except Exception as e:
            _log(f"[CHAIN] {chain['chain_id']} â€” ì§€ì‹œì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨í•´ë„ ì§„í–‰ (ì§€ì‹œì„œ ì—†ì´ ì›ë³¸ ëª…ë ¹ìœ¼ë¡œ)

    # ì§€ì‹œì„œ ìƒíƒœ ì—…ë°ì´íŠ¸ + íŒ€ì¥ ì´ˆë¡ë¶ˆ ë„ê¸°
    has_instructions = bool(chain.get("delegation_instructions"))
    deleg_status = f"âœ… {mgr_name} ì§€ì‹œì„œ ìƒì„± ì™„ë£Œ" if has_instructions else f"âš ï¸ ì§€ì‹œì„œ ì—†ì´ ì§„í–‰"
    await _broadcast_status(target_id, "done", 0.5, deleg_status)
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 2ë‹¨ê³„: {deleg_status}")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 2ë‹¨ê³„: {deleg_status}")

    _save_chain(chain)

    # ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œë¡œ ì§„í–‰
    await _chain_submit_specialists(chain)


async def _chain_create_delegation_broadcast(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ íŒ€ì¥ì´ ê°ê° ì§€ì‹œì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."""
    text = chain["text"]
    all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]

    # ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ ì„ íƒ (Gemini Flash â†’ GPT Mini â†’ Claude)
    providers = get_available_providers()
    if providers.get("google"):
        deleg_model = "gemini-2.5-flash"
    elif providers.get("openai"):
        deleg_model = "gpt-5-mini"
    elif providers.get("anthropic"):
        deleg_model = "claude-sonnet-4-6"
    else:
        deleg_model = None

    broadcast_delegations = {}

    if deleg_model:
        import asyncio as _asyncio

        async def _get_delegation(mgr_id: str) -> tuple[str, dict]:
            specialists = _MANAGER_SPECIALISTS.get(mgr_id, [])
            if not specialists:
                return mgr_id, {}
            mgr_name = _AGENT_NAMES.get(mgr_id, mgr_id)
            spec_list_parts = []
            json_example_parts = []
            for s_id in specialists:
                s_name = _SPECIALIST_NAMES.get(s_id, s_id)
                spec_list_parts.append(f"- {s_id}: {s_name}")
                json_example_parts.append(f'  "{s_id}": "êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ ë‚´ìš©"')

            prompt = _DELEGATION_PROMPT.format(
                mgr_name=mgr_name,
                spec_list="\n".join(spec_list_parts),
                text=text,
                json_example="{\n" + ",\n".join(json_example_parts) + "\n}",
            )
            try:
                result = await ask_ai(user_message=prompt, model=deleg_model)
                response_text = result.get("content", "") or result.get("text", "")
                import json as _json
                json_text = response_text.strip()
                start = json_text.find("{")
                end = json_text.rfind("}") + 1
                if start >= 0 and end > start:
                    json_text = json_text[start:end]
                instructions = _json.loads(json_text)
                chain["total_cost_usd"] += result.get("cost_usd", 0)
                return mgr_id, instructions if isinstance(instructions, dict) else {}
            except Exception as e:
                _log(f"[CHAIN] ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì§€ì‹œì„œ ì‹¤íŒ¨ ({mgr_id}): {e}")
                return mgr_id, {}

        # 6ê°œ íŒ€ì¥ì—ê²Œ ë™ì‹œì— ì§€ì‹œì„œ ìš”ì²­
        tasks = [_get_delegation(m) for m in all_managers]
        results = await _asyncio.gather(*tasks, return_exceptions=True)
        for r in results:
            if isinstance(r, tuple):
                mgr_id, instructions = r
                if instructions:
                    broadcast_delegations[mgr_id] = instructions

    chain["broadcast_delegations"] = broadcast_delegations
    chain["results"]["delegation"] = {
        "mode": "broadcast",
        "delegations": broadcast_delegations,
    }

    deleg_count = sum(1 for d in broadcast_delegations.values() if d)
    _log(f"[CHAIN] {chain['chain_id']} â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì§€ì‹œì„œ {deleg_count}/6 ì™„ë£Œ")
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 2ë‹¨ê³„: íŒ€ì¥ ì§€ì‹œì„œ {deleg_count}/6 ì™„ë£Œ")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 2ë‹¨ê³„: íŒ€ì¥ ì§€ì‹œì„œ {deleg_count}/6 ì™„ë£Œ")
    _save_chain(chain)

    # ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œë¡œ ì§„í–‰
    await _chain_submit_specialists_broadcast(chain)


async def _chain_submit_specialists(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” ë‹¨ì¼ ë¶€ì„œì˜ ì „ë¬¸ê°€ë“¤ì—ê²Œ ë°°ì¹˜ ì œì¶œí•©ë‹ˆë‹¤."""
    target_id = chain["target_id"]
    text = chain["text"]
    specialists = _MANAGER_SPECIALISTS.get(target_id, [])

    if not specialists:
        # ì „ë¬¸ê°€ ì—†ìŒ â†’ ë°”ë¡œ ì¢…í•©(íŒ€ì¥ ì§ì ‘ ì²˜ë¦¬) ë‹¨ê³„
        chain["step"] = "synthesis"
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return

    # íŒ€ì¥ ì§€ì‹œì„œê°€ ìˆìœ¼ë©´ ì „ë¬¸ê°€ì—ê²Œ í•¨ê»˜ ì „ë‹¬
    delegation = chain.get("delegation_instructions", {})

    requests = []
    for spec_id in specialists:
        # ì „ë¬¸ê°€ ì´ˆë¡ë¶ˆ ì¼œê¸°
        spec_name = _SPECIALIST_NAMES.get(spec_id, spec_id)
        await _broadcast_status(spec_id, "working", 0.3, f"{spec_name} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")

        soul = _load_agent_prompt(spec_id, include_tools=False) + _BATCH_MODE_SUFFIX
        override = _get_model_override(spec_id)
        model = select_model(text, override=override)
        custom_id = f"{chain['chain_id']}_spec_{spec_id}"

        # íŒ€ì¥ ì§€ì‹œì„œê°€ ìˆìœ¼ë©´ ì „ë¬¸ê°€ì—ê²Œ í•¨ê»˜ ì „ë‹¬
        spec_instruction = delegation.get(spec_id, "")
        if spec_instruction:
            message = (
                f"## íŒ€ì¥ ì§€ì‹œ\n{spec_instruction}\n\n"
                f"## CEO ì›ë³¸ ëª…ë ¹\n{text}"
            )
        else:
            message = text

        requests.append({
            "custom_id": custom_id,
            "message": message,
            "system_prompt": soul,
            "model": model,
            "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
            "reasoning_effort": _get_agent_reasoning_effort(spec_id),
        })
        chain["custom_id_map"][custom_id] = {"agent_id": spec_id, "step": "specialists"}

    # í”„ë¡œë°”ì´ë”ë³„ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ ì œì¶œ
    batch_results = await batch_submit_grouped(requests)

    chain["batches"]["specialists"] = []
    for br in batch_results:
        chain["batches"]["specialists"].append({
            "batch_id": br.get("batch_id", ""),
            "provider": br.get("provider", ""),
            "status": "pending" if "error" not in br else "failed",
            "custom_ids": br.get("custom_ids", []),
            "error": br.get("error"),
        })

    chain["step"] = "specialists"
    chain["status"] = "pending"
    _save_chain(chain)

    _ensure_batch_poller()
    spec_count = len(specialists)
    provider_count = len(batch_results)
    target_name = _AGENT_NAMES.get(target_id, target_id)
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 3ë‹¨ê³„: {target_name} ì „ë¬¸ê°€ {spec_count}ëª… ë°°ì¹˜ ì œì¶œ ({provider_count}ê°œ í”„ë¡œë°”ì´ë”)")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 3ë‹¨ê³„: {target_name} ì „ë¬¸ê°€ {spec_count}ëª… â†’ {provider_count}ê°œ í”„ë¡œë°”ì´ë”ë³„ ë°°ì¹˜ ì œì¶œ")

    _log(f"[CHAIN] {chain['chain_id']} â€” ì „ë¬¸ê°€ {spec_count}ëª… ë°°ì¹˜ ì œì¶œ ({provider_count}ê°œ í”„ë¡œë°”ì´ë”)")


async def _chain_submit_specialists_broadcast(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ ë¶€ì„œ ì „ì²´ ì „ë¬¸ê°€ì—ê²Œ ë°°ì¹˜ ì œì¶œí•©ë‹ˆë‹¤."""
    text = chain["text"]
    all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]

    # ë¸Œë¡œë“œìºìŠ¤íŠ¸ ëª¨ë“œì˜ íŒ€ì¥ë³„ ì§€ì‹œì„œ
    broadcast_delegations = chain.get("broadcast_delegations", {})

    requests = []
    for mgr_id in all_managers:
        specialists = _MANAGER_SPECIALISTS.get(mgr_id, [])
        mgr_delegation = broadcast_delegations.get(mgr_id, {})
        for spec_id in specialists:
            soul = _load_agent_prompt(spec_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _get_model_override(spec_id)
            model = select_model(text, override=override)
            custom_id = f"{chain['chain_id']}_spec_{spec_id}"

            # íŒ€ì¥ ì§€ì‹œì„œê°€ ìˆìœ¼ë©´ ì „ë¬¸ê°€ì—ê²Œ í•¨ê»˜ ì „ë‹¬
            spec_instruction = mgr_delegation.get(spec_id, "")
            if spec_instruction:
                message = (
                    f"## íŒ€ì¥ ì§€ì‹œ\n{spec_instruction}\n\n"
                    f"## CEO ì›ë³¸ ëª…ë ¹\n{text}"
                )
            else:
                message = text

            requests.append({
                "custom_id": custom_id,
                "message": message,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _get_agent_reasoning_effort(spec_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": spec_id, "step": "specialists"}

    if not requests:
        chain["step"] = "synthesis"
        _save_chain(chain)
        await _chain_submit_synthesis(chain)
        return

    # í”„ë¡œë°”ì´ë”ë³„ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ ì œì¶œ
    batch_results = await batch_submit_grouped(requests)

    chain["batches"]["specialists"] = []
    for br in batch_results:
        chain["batches"]["specialists"].append({
            "batch_id": br.get("batch_id", ""),
            "provider": br.get("provider", ""),
            "status": "pending" if "error" not in br else "failed",
            "custom_ids": br.get("custom_ids", []),
            "error": br.get("error"),
        })

    chain["step"] = "specialists"
    chain["status"] = "pending"
    _save_chain(chain)

    _ensure_batch_poller()
    spec_count = len(requests)
    provider_count = len(batch_results)
    update_task(chain["task_id"], status="pending",
                result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 2ë‹¨ê³„: ì „ì²´ {spec_count}ëª… ì „ë¬¸ê°€ ë°°ì¹˜ ì œì¶œ ({provider_count}ê°œ í”„ë¡œë°”ì´ë”)")
    await _broadcast_chain_status(chain, f"ğŸ“¦ 2ë‹¨ê³„: 6ê°œ ë¶€ì„œ ì „ë¬¸ê°€ {spec_count}ëª… â†’ {provider_count}ê°œ í”„ë¡œë°”ì´ë”ë³„ ë°°ì¹˜ ì œì¶œ")

    _log(f"[CHAIN] {chain['chain_id']} â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì „ë¬¸ê°€ {spec_count}ëª… ë°°ì¹˜ ì œì¶œ")


async def _chain_submit_synthesis(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ â€” íŒ€ì¥(ë“¤)ì´ ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ëŠ” ë°°ì¹˜ë¥¼ ì œì¶œí•©ë‹ˆë‹¤."""
    text = chain["text"]

    requests = []

    if chain["mode"] == "broadcast":
        # ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ íŒ€ì¥ì´ ê°ê° ìê¸° íŒ€ ê²°ê³¼ë¥¼ ì¢…í•©
        all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]
        for mgr_id in all_managers:
            specialists = _MANAGER_SPECIALISTS.get(mgr_id, [])
            spec_parts = []
            for s_id in specialists:
                s_res = chain["results"]["specialists"].get(s_id, {})
                name = _SPECIALIST_NAMES.get(s_id, s_id)
                content = s_res.get("content", "ì‘ë‹µ ì—†ìŒ")
                if s_res.get("error"):
                    content = f"ì˜¤ë¥˜: {s_res['error'][:100]}"
                spec_parts.append(f"[{name}]\n{content}")

            mgr_name = _AGENT_NAMES.get(mgr_id, mgr_id)
            synthesis_prompt = (
                f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì†Œì† ì „ë¬¸ê°€ë“¤ì´ ì•„ë˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì œì¶œí–ˆìŠµë‹ˆë‹¤.\n"
                f"ì´ë¥¼ ê²€ìˆ˜í•˜ê³  ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•  ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
                f"ì „ë¬¸ê°€ ì˜ê²¬ ì¤‘ ë¶€ì¡±í•˜ê±°ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì§€ì í•˜ê³  ë³´ì™„í•˜ì„¸ìš”.\n\n"
                f"## CEO ì›ë³¸ ëª…ë ¹\n{text}\n\n"
                f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts)
            )

            soul = _load_agent_prompt(mgr_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _get_model_override(mgr_id)
            model = select_model(synthesis_prompt, override=override)
            custom_id = f"{chain['chain_id']}_synth_{mgr_id}"

            requests.append({
                "custom_id": custom_id,
                "message": synthesis_prompt,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _get_agent_reasoning_effort(mgr_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": mgr_id, "step": "synthesis"}

    elif chain["target_id"] == "chief_of_staff":
        # ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬ (ë¶„ë¥˜ ê²°ê³¼ê°€ chief_of_staffì¸ ê²½ìš°)
        soul = _load_agent_prompt("chief_of_staff", include_tools=False) + _BATCH_MODE_SUFFIX
        override = _get_model_override("chief_of_staff")
        model = select_model(text, override=override)
        custom_id = f"{chain['chain_id']}_synth_chief_of_staff"

        requests.append({
            "custom_id": custom_id,
            "message": text,
            "system_prompt": soul,
            "model": model,
            "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
            "reasoning_effort": _get_agent_reasoning_effort("chief_of_staff"),
        })
        chain["custom_id_map"][custom_id] = {"agent_id": "chief_of_staff", "step": "synthesis"}

    else:
        # ë‹¨ì¼ ë¶€ì„œ: íŒ€ì¥ì´ ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ì¢…í•©
        target_id = chain["target_id"]
        specialists = _MANAGER_SPECIALISTS.get(target_id, [])

        if not specialists or not chain["results"]["specialists"]:
            # ì „ë¬¸ê°€ ê²°ê³¼ ì—†ìŒ â†’ íŒ€ì¥ì´ ì§ì ‘ ë‹µë³€
            soul = _load_agent_prompt(target_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _get_model_override(target_id)
            model = select_model(text, override=override)
            custom_id = f"{chain['chain_id']}_synth_{target_id}"

            requests.append({
                "custom_id": custom_id,
                "message": text,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _get_agent_reasoning_effort(target_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": target_id, "step": "synthesis"}
        else:
            # ì „ë¬¸ê°€ ê²°ê³¼ ì·¨í•© â†’ íŒ€ì¥ì—ê²Œ ì¢…í•© ìš”ì²­
            spec_parts = []
            for s_id in specialists:
                s_res = chain["results"]["specialists"].get(s_id, {})
                name = _SPECIALIST_NAMES.get(s_id, s_id)
                content = s_res.get("content", "ì‘ë‹µ ì—†ìŒ")
                if s_res.get("error"):
                    content = f"ì˜¤ë¥˜: {s_res['error'][:100]}"
                spec_parts.append(f"[{name}]\n{content}")

            mgr_name = _AGENT_NAMES.get(target_id, target_id)
            synthesis_prompt = (
                f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì†Œì† ì „ë¬¸ê°€ë“¤ì´ ì•„ë˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì œì¶œí–ˆìŠµë‹ˆë‹¤.\n"
                f"ì´ë¥¼ ê²€ìˆ˜í•˜ê³  ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•  ê°„ê²°í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
                f"ì „ë¬¸ê°€ ì˜ê²¬ ì¤‘ ë¶€ì¡±í•˜ê±°ë‚˜ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì§€ì í•˜ê³  ë³´ì™„í•˜ì„¸ìš”.\n\n"
                f"## CEO ì›ë³¸ ëª…ë ¹\n{text}\n\n"
                f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts)
            )

            soul = _load_agent_prompt(target_id, include_tools=False) + _BATCH_MODE_SUFFIX
            override = _get_model_override(target_id)
            model = select_model(synthesis_prompt, override=override)
            custom_id = f"{chain['chain_id']}_synth_{target_id}"

            requests.append({
                "custom_id": custom_id,
                "message": synthesis_prompt,
                "system_prompt": soul,
                "model": model,
                "max_tokens": min(MODEL_MAX_TOKENS_MAP.get(model, 8192), 16384),
                "reasoning_effort": _get_agent_reasoning_effort(target_id),
            })
            chain["custom_id_map"][custom_id] = {"agent_id": target_id, "step": "synthesis"}

    if not requests:
        # ìš”ì²­ ì—†ìŒ â†’ ë°”ë¡œ ì™„ë£Œ
        await _deliver_chain_result(chain)
        return

    # í”„ë¡œë°”ì´ë”ë³„ ê·¸ë£¹í™”í•˜ì—¬ ë°°ì¹˜ ì œì¶œ
    batch_results = await batch_submit_grouped(requests)

    chain["batches"]["synthesis"] = []
    for br in batch_results:
        chain["batches"]["synthesis"].append({
            "batch_id": br.get("batch_id", ""),
            "provider": br.get("provider", ""),
            "status": "pending" if "error" not in br else "failed",
            "custom_ids": br.get("custom_ids", []),
            "error": br.get("error"),
        })

    chain["step"] = "synthesis"
    chain["status"] = "pending"
    _save_chain(chain)

    _ensure_batch_poller()

    if chain["mode"] == "broadcast":
        update_task(chain["task_id"], status="pending",
                    result_summary="ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 4ë‹¨ê³„: 6ê°œ íŒ€ì¥ ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ì œì¶œ")
        await _broadcast_chain_status(chain, "ğŸ“¦ 4ë‹¨ê³„: 6ê°œ íŒ€ì¥ì´ ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘ (ë°°ì¹˜)")
    else:
        target_name = _AGENT_NAMES.get(chain["target_id"], chain["target_id"])
        update_task(chain["task_id"], status="pending",
                    result_summary=f"ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] 4ë‹¨ê³„: {target_name} ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ì œì¶œ")
        await _broadcast_chain_status(chain, f"ğŸ“¦ 4ë‹¨ê³„: {target_name} ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘ (ë°°ì¹˜)")

    _log(f"[CHAIN] {chain['chain_id']} â€” ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ì œì¶œ ({len(requests)}ê±´)")


async def _send_batch_result_to_telegram(content: str, cost: float):
    """ë°°ì¹˜ ì²´ì¸ ê²°ê³¼ë¥¼ í…”ë ˆê·¸ë¨ CEOì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."""
    if not app_state.telegram_app:
        return
    ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
    if not ceo_id:
        return
    try:
        # í…”ë ˆê·¸ë¨ ì½”ë“œëª… ë³€í™˜
        content = _tg_convert_names(content)
        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (4096ì)
        if len(content) > 3800:
            content = content[:3800] + "\n\n... (ì „ì²´ ê²°ê³¼ëŠ” ì›¹ì—ì„œ í™•ì¸)"
        await app_state.telegram_app.bot.send_message(
            chat_id=int(ceo_id),
            text=f"ğŸ“¦ ë°°ì¹˜ ì²´ì¸ ì™„ë£Œ\n\n{content}\n\nâ”€â”€â”€â”€â”€\nğŸ’° ${cost:.4f}",
        )
    except Exception as e:
        _log(f"[TG] ë°°ì¹˜ ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {e}")


async def _forward_web_response_to_telegram(
    user_command: str, result_data: dict
) -> None:
    """ì›¹ ì±„íŒ… ì—ì´ì „íŠ¸ ì‘ë‹µì„ í…”ë ˆê·¸ë¨ CEOì—ê²Œ ìë™ ì „ë‹¬í•©ë‹ˆë‹¤."""
    if not app_state.telegram_app:
        return
    ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
    if not ceo_id:
        return
    content = result_data.get("content", "")
    if not content:
        return
    handled_by = result_data.get("handled_by", "")
    # í…”ë ˆê·¸ë¨ ì½”ë“œëª… ë³€í™˜
    tg_who = _tg_convert_names(handled_by) if handled_by else ""
    cost = result_data.get("cost", 0)
    try:
        # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (4096ì)
        cmd_preview = user_command[:60] + ("..." if len(user_command) > 60 else "")
        header = f"ğŸ’¬ [{tg_who}] ì›¹ ì‘ë‹µ\nğŸ“ \"{cmd_preview}\"\nâ”€â”€â”€â”€â”€\n"
        footer = f"\nâ”€â”€â”€â”€â”€\nğŸ’° ${cost:.4f}" if cost else ""
        max_content = 4096 - len(header) - len(footer) - 50
        if len(content) > max_content:
            content = content[:max_content] + "\n\n... (ì „ì²´ëŠ” ì›¹ì—ì„œ í™•ì¸)"
        msg = f"{header}{content}{footer}"
        await app_state.telegram_app.bot.send_message(
            chat_id=int(ceo_id), text=msg,
        )
    except Exception as e:
        _log(f"[TG] ì›¹ ì‘ë‹µ ì „ì†¡ ì‹¤íŒ¨: {e}")


async def _synthesis_realtime_fallback(chain: dict):
    """ì¢…í•© ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ ì‹¤ì‹œê°„ ask_ai()ë¡œ ì¢…í•©ë³´ê³ ì„œë¥¼ ëŒ€ì‹  ìƒì„±í•©ë‹ˆë‹¤."""
    text = chain["text"]
    _log(f"[CHAIN] {chain['chain_id']} â€” ì‹¤ì‹œê°„ í´ë°± ì‹œì‘")

    if chain["mode"] == "broadcast":
        all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]
        for mgr_id in all_managers:
            if mgr_id in chain["results"]["synthesis"]:
                continue  # ì´ë¯¸ ìˆìœ¼ë©´ skip
            specialists = _MANAGER_SPECIALISTS.get(mgr_id, [])
            spec_parts = []
            for s_id in specialists:
                s_res = chain["results"]["specialists"].get(s_id, {})
                name = _SPECIALIST_NAMES.get(s_id, s_id)
                content = s_res.get("content", "ì‘ë‹µ ì—†ìŒ")
                spec_parts.append(f"[{name}]\n{content}")

            mgr_name = _AGENT_NAMES.get(mgr_id, mgr_id)
            if spec_parts:
                synthesis_prompt = (
                    f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì†Œì† ì „ë¬¸ê°€ë“¤ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ë³´ê³ í•˜ì„¸ìš”.\n\n"
                    f"## CEO ì›ë³¸ ëª…ë ¹\n{text}\n\n"
                    f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts)
                )
            else:
                synthesis_prompt = text
            soul = _load_agent_prompt(mgr_id, include_tools=False)
            try:
                result = await ask_ai(user_message=synthesis_prompt, system_prompt=soul)
                chain["results"]["synthesis"][mgr_id] = {
                    "content": result.get("content", ""),
                    "model": result.get("model", ""),
                    "cost_usd": result.get("cost_usd", 0),
                    "error": result.get("error"),
                }
                chain["total_cost_usd"] += result.get("cost_usd", 0)
            except Exception as e:
                _log(f"[CHAIN] ì‹¤ì‹œê°„ í´ë°± ì‹¤íŒ¨ ({mgr_id}): {e}")
                chain["results"]["synthesis"][mgr_id] = {"content": "", "error": str(e)[:100]}
    else:
        target_id = chain.get("target_id", "chief_of_staff")
        if target_id not in chain["results"]["synthesis"]:
            soul = _load_agent_prompt(target_id, include_tools=False)
            try:
                result = await ask_ai(user_message=text, system_prompt=soul)
                chain["results"]["synthesis"][target_id] = {
                    "content": result.get("content", ""),
                    "model": result.get("model", ""),
                    "cost_usd": result.get("cost_usd", 0),
                    "error": result.get("error"),
                }
                chain["total_cost_usd"] += result.get("cost_usd", 0)
            except Exception as e:
                _log(f"[CHAIN] ì‹¤ì‹œê°„ í´ë°± ì‹¤íŒ¨ ({target_id}): {e}")
                chain["results"]["synthesis"][target_id] = {"content": "", "error": str(e)[:100]}

    target_id = chain.get("target_id", "chief_of_staff")
    await _broadcast_status(target_id, "done", 1.0, "ë³´ê³  ì™„ë£Œ")
    _save_chain(chain)
    await _deliver_chain_result(chain)


async def _deliver_chain_result(chain: dict):
    """ë°°ì¹˜ ì²´ì¸ ìµœì¢… ê²°ê³¼ë¥¼ CEOì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤."""
    # â”€â”€ ì¤‘ë³µ ì „ë‹¬ ë°©ì§€ â”€â”€
    if chain.get("delivered"):
        _log(f"[CHAIN] {chain.get('chain_id', '?')} â€” ì´ë¯¸ ì „ë‹¬ë¨, ì¤‘ë³µ ë°©ì§€")
        return
    chain["delivered"] = True
    # ì¦‰ì‹œ completed ìƒíƒœë¡œ ë³€ê²½ â†’ í´ëŸ¬ê°€ ì¬ì§„ì… ëª»í•˜ê²Œ ë°©ì§€
    chain["step"] = "completed"
    chain["status"] = "completed"
    chain["completed_at"] = datetime.now(KST).isoformat()
    _save_chain(chain)

    task_id = chain["task_id"]
    text = chain["text"]
    total_cost = chain.get("total_cost_usd", 0)

    if chain["mode"] == "broadcast":
        # ë¸Œë¡œë“œìºìŠ¤íŠ¸: 6ê°œ íŒ€ì¥ ì¢…í•© ê²°ê³¼ë¥¼ ëª¨ì•„ì„œ ì „ë‹¬
        all_managers = ["cto_manager", "cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]
        parts = []
        total_specialists = 0
        for mgr_id in all_managers:
            synth = chain["results"]["synthesis"].get(mgr_id, {})
            mgr_name = _AGENT_NAMES.get(mgr_id, mgr_id)
            content = synth.get("content", "")
            # ì¢…í•©ë³´ê³ ì„œê°€ ë¹„ì—ˆìœ¼ë©´ ì „ë¬¸ê°€ ì›ë³¸ ê²°ê³¼ë¥¼ í´ë°±ìœ¼ë¡œ ì‚¬ìš©
            if not content or content == "ì‘ë‹µ ì—†ìŒ":
                specialists = _MANAGER_SPECIALISTS.get(mgr_id, [])
                fallback_parts = []
                for s_id in specialists:
                    s_res = chain["results"].get("specialists", {}).get(s_id, {})
                    s_content = s_res.get("content", "")
                    if s_content:
                        s_name = _SPECIALIST_NAMES.get(s_id, s_id)
                        fallback_parts.append(f"**{s_name}**: {s_content[:300]}")
                if fallback_parts:
                    content = "(ì¢…í•© ë°°ì¹˜ ì‹¤íŒ¨ â€” ì „ë¬¸ê°€ ì›ë³¸ ê²°ê³¼)\n" + "\n".join(fallback_parts)
                else:
                    content = "ì‘ë‹µ ì—†ìŒ (ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ)"
            specs = len(_MANAGER_SPECIALISTS.get(mgr_id, []))
            total_specialists += specs
            spec_label = f" (ì „ë¬¸ê°€ {specs}ëª… ë™ì›)" if specs else ""
            parts.append(f"### ğŸ“‹ {mgr_name}{spec_label}\n{content}")

        compiled = (
            f"ğŸ“¢ **ë°°ì¹˜ ì²´ì¸ ê²°ê³¼** (6ê°œ ë¶€ì„œ + ì „ë¬¸ê°€ {total_specialists}ëª… ë™ì›)\n"
            f"ğŸ’° ì´ ë¹„ìš©: ${total_cost:.4f} (ë°°ì¹˜ í• ì¸ ~50% ì ìš©)\n\n---\n\n"
            + "\n\n---\n\n".join(parts)
        )

        update_task(task_id, status="completed",
                    result_summary=compiled[:500],
                    result_data=compiled,
                    success=1, cost_usd=total_cost,
                    agent_id="chief_of_staff")

        # WebSocketìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ì „ë‹¬
        await wm.broadcast("result", {
            "content": compiled,
            "sender_id": "chief_of_staff",
            "handled_by": "ë¹„ì„œì‹¤ì¥ â†’ 6ê°œ íŒ€ì¥",
            "delegation": "ë¹„ì„œì‹¤ì¥ â†’ íŒ€ì¥ â†’ ì „ë¬¸ê°€ (ë°°ì¹˜)",
            "time_seconds": 0,
            "cost": total_cost,
            "model": "multi-agent-batch",
            "routing_method": "ë°°ì¹˜ ì²´ì¸ (ë¸Œë¡œë“œìºìŠ¤íŠ¸)",
        })

    else:
        # ë‹¨ì¼ ë¶€ì„œ ê²°ê³¼
        target_id = chain.get("target_id", "chief_of_staff")
        synth = chain["results"]["synthesis"].get(
            target_id,
            chain["results"]["synthesis"].get("chief_of_staff", {})
        )
        content = synth.get("content", "")
        target_name = _AGENT_NAMES.get(target_id, target_id)

        # ìœ„ì„ ì •ë³´ êµ¬ì„±
        specs_count = len(_MANAGER_SPECIALISTS.get(target_id, []))
        if target_id == "chief_of_staff":
            delegation = ""
            handled_by = "ë¹„ì„œì‹¤ì¥"
            header = "ğŸ“‹ **ë¹„ì„œì‹¤ì¥** (ë°°ì¹˜ ì²˜ë¦¬)"
        else:
            delegation = f"ë¹„ì„œì‹¤ì¥ â†’ {target_name}"
            if specs_count:
                delegation += f" â†’ ì „ë¬¸ê°€ {specs_count}ëª…"
            handled_by = target_name
            header = f"ğŸ“‹ **{target_name}** ë³´ê³  (ë°°ì¹˜ ì²´ì¸)"
            if specs_count:
                header += f" (ì†Œì† ì „ë¬¸ê°€ {specs_count}ëª… ë™ì›)"

        final_content = f"{header}\nğŸ’° ë¹„ìš©: ${total_cost:.4f} (ë°°ì¹˜ í• ì¸ ~50% ì ìš©)\n\n---\n\n{content}"

        update_task(task_id, status="completed",
                    result_summary=final_content[:500],
                    result_data=final_content,
                    success=1, cost_usd=total_cost,
                    agent_id=target_id)

        # WebSocketìœ¼ë¡œ ìµœì¢… ê²°ê³¼ ì „ë‹¬
        await wm.broadcast("result", {
            "content": final_content,
            "sender_id": target_id,
            "handled_by": handled_by,
            "delegation": delegation,
            "time_seconds": 0,
            "cost": total_cost,
            "model": synth.get("model", "batch"),
            "routing_method": "ë°°ì¹˜ ì²´ì¸",
        })

    # í…”ë ˆê·¸ë¨ìœ¼ë¡œë„ ê²°ê³¼ ì „ë‹¬
    tg_content = compiled if chain["mode"] == "broadcast" else final_content
    await _send_batch_result_to_telegram(tg_content, total_cost)

    # ì•„ì¹´ì´ë¸Œì— ì €ì¥
    synth_content = ""
    if chain["mode"] == "broadcast":
        synth_content = compiled
    else:
        synth_content = content

    if synth_content and len(synth_content) > 20:
        division = _AGENT_DIVISION.get(chain.get("target_id", "chief_of_staff"), "secretary")
        now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
        save_archive(
            division=division,
            filename=f"batch_chain_{now_str}.md",
            content=f"# [ë°°ì¹˜ ì²´ì¸] {text[:60]}\n\n{synth_content}",
            agent_id=chain.get("target_id", "chief_of_staff"),
        )

    _save_chain(chain)

    await _broadcast_chain_status(chain, "âœ… ë°°ì¹˜ ì²´ì¸ ì™„ë£Œ â€” ìµœì¢… ë³´ê³ ì„œ ì „ë‹¬ë¨")
    _log(f"[CHAIN] {chain['chain_id']} â€” ì™„ë£Œ! ë¹„ìš©: ${total_cost:.4f}")


async def _advance_batch_chain(chain_id: str):
    """ë°°ì¹˜ ì²´ì¸ì˜ í˜„ì¬ ë‹¨ê³„ë¥¼ í™•ì¸í•˜ê³ , ì™„ë£Œë˜ì—ˆìœ¼ë©´ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.

    ë°°ì¹˜ í´ëŸ¬(_batch_poller_loop)ì—ì„œ 60ì´ˆë§ˆë‹¤ í˜¸ì¶œë©ë‹ˆë‹¤.
    """
    chain = _load_chain(chain_id)
    if not chain or chain.get("status") not in ("running", "pending"):
        return

    step = chain.get("step", "")

    # â”€â”€ 1ë‹¨ê³„: ë¶„ë¥˜ â”€â”€
    if step == "classify":
        batch_info = chain["batches"].get("classify")
        if not batch_info:
            return

        status_result = await batch_check(batch_info["batch_id"], batch_info["provider"])
        if "error" in status_result:
            return

        batch_info["status"] = status_result["status"]

        if status_result["status"] == "completed":
            # ë¶„ë¥˜ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            result = await batch_retrieve(batch_info["batch_id"], batch_info["provider"])
            if "error" in result:
                chain["status"] = "failed"
                _save_chain(chain)
                return

            # JSON íŒŒì‹± â€” {"agent_id": "cio_manager", "reason": "..."}
            results_list = result.get("results", [])
            if results_list:
                raw_content = results_list[0].get("content", "").strip()
                cost = results_list[0].get("cost_usd", 0)
                chain["total_cost_usd"] += cost

                try:
                    if "```" in raw_content:
                        raw_content = raw_content.split("```")[1]
                        if raw_content.startswith("json"):
                            raw_content = raw_content[4:]
                    parsed = json.loads(raw_content)
                    target_id = parsed.get("agent_id", "chief_of_staff")
                    reason = parsed.get("reason", "")
                except (json.JSONDecodeError, IndexError):
                    _log(f"[CHAIN] ë¶„ë¥˜ JSON íŒŒì‹± ì‹¤íŒ¨: {raw_content[:100]}")
                    target_id = "chief_of_staff"
                    reason = "ë¶„ë¥˜ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨"
            else:
                target_id = "chief_of_staff"
                reason = "ë¶„ë¥˜ ê²°ê³¼ ì—†ìŒ"

            chain["target_id"] = target_id
            chain["results"]["classify"] = {
                "agent_id": target_id,
                "reason": reason,
                "method": "AIë¶„ë¥˜ (ë°°ì¹˜)",
                "cost_usd": cost if results_list else 0,
            }

            target_name = _AGENT_NAMES.get(target_id, target_id)
            _log(f"[CHAIN] {chain['chain_id']} â€” ë¶„ë¥˜ ì™„ë£Œ: {target_name} ({reason})")

            if target_id == "chief_of_staff":
                # ë¹„ì„œì‹¤ì¥ ì§ì ‘ â†’ ì¢…í•© ë‹¨ê³„
                chain["step"] = "synthesis"
                _save_chain(chain)
                await _broadcast_chain_status(chain, f"ğŸ“¦ ë¶„ë¥˜ ì™„ë£Œ: ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬")
                await _chain_submit_synthesis(chain)
            else:
                # íŒ€ì¥ ì§€ì‹œì„œ â†’ ì „ë¬¸ê°€ ë‹¨ê³„ë¡œ ì§„í–‰
                chain["step"] = "delegation"
                _save_chain(chain)
                await _broadcast_chain_status(chain, f"ğŸ“¦ ë¶„ë¥˜ ì™„ë£Œ: {target_name} ì§€ì‹œì„œ ìƒì„± ì¤‘")
                await _chain_create_delegation(chain)

        elif status_result["status"] in ("failed", "expired"):
            # ë¶„ë¥˜ ë°°ì¹˜ ì‹¤íŒ¨ â†’ ë¹„ì„œì‹¤ì¥ í´ë°±
            chain["target_id"] = "chief_of_staff"
            chain["step"] = "synthesis"
            chain["results"]["classify"] = {"agent_id": "chief_of_staff", "method": "í´ë°±"}
            _save_chain(chain)
            await _chain_submit_synthesis(chain)

    # â”€â”€ delegation ì•ˆì „ë§ â”€â”€
    # delegationì€ ì‹¤ì‹œê°„ APIë¡œ ì¦‰ì‹œ ì²˜ë¦¬ë˜ë¯€ë¡œ í´ëŸ¬ê°€ ê´€ì—¬í•  ì¼ì´ ì—†ìŒ.
    # í•˜ì§€ë§Œ _chain_create_delegation() ì¤‘ ì—ëŸ¬ë¡œ stepì´ "delegation"ì— ë©ˆì¶°ìˆìœ¼ë©´
    # ì—¬ê¸°ì„œ ë³µêµ¬í•˜ì—¬ ì „ë¬¸ê°€ ë‹¨ê³„ë¥¼ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    elif step == "delegation":
        # ì´ë¯¸ ì „ë¬¸ê°€ ë°°ì¹˜ê°€ ì œì¶œëœ ìƒíƒœë©´ â†’ specialistsë¡œ ì „í™˜
        if chain["batches"].get("specialists"):
            chain["step"] = "specialists"
            _save_chain(chain)
            _log(f"[CHAIN] {chain_id} â€” delegation ì•ˆì „ë§: specialistsë¡œ ì „í™˜")
        else:
            # ì „ë¬¸ê°€ ë°°ì¹˜ê°€ ì•„ì§ ì œì¶œ ì•ˆ ë¨ â†’ ì§€ì‹œì„œ ìƒì„±ë¶€í„° ì¬ì‹œë„
            _log(f"[CHAIN] {chain_id} â€” delegation ì•ˆì „ë§: ì§€ì‹œì„œ ìƒì„± ì¬ì‹œë„")
            try:
                await _chain_create_delegation(chain)
            except Exception as e:
                _log(f"[CHAIN] {chain_id} â€” delegation ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ ì‹œ ì§€ì‹œì„œ ì—†ì´ ì „ë¬¸ê°€ì—ê²Œ ì§ì ‘ ì „ë‹¬
                chain["step"] = "specialists"
                _save_chain(chain)
                await _chain_submit_specialists(chain)
        return

    # â”€â”€ 3ë‹¨ê³„: ì „ë¬¸ê°€ â”€â”€
    elif step == "specialists":
        all_done = True
        batch_errors = []  # ì˜¤ë¥˜ ì¶”ì 
        for batch_info in chain["batches"].get("specialists", []):
            if batch_info.get("status") in ("pending", "processing"):
                try:
                    status_result = await batch_check(batch_info["batch_id"], batch_info["provider"])
                    if "error" not in status_result:
                        batch_info["status"] = status_result["status"]
                    else:
                        err = status_result.get("error", "ë°°ì¹˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
                        _log(f"[CHAIN] ì „ë¬¸ê°€ ë°°ì¹˜ í™•ì¸ ì˜¤ë¥˜ ({batch_info['provider']}): {err}")
                        batch_errors.append(f"{batch_info['provider']}: {err[:80]}")
                except Exception as e:
                    _log(f"[CHAIN] ì „ë¬¸ê°€ ë°°ì¹˜ í™•ì¸ ì˜ˆì™¸ ({batch_info.get('provider','?')}): {e}")
                    batch_errors.append(f"{batch_info.get('provider','?')}: {str(e)[:80]}")

            if batch_info.get("status") not in ("completed", "failed"):
                all_done = False

        _save_chain(chain)

        if not all_done:
            # â”€â”€ ë°°ì¹˜ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘ â†’ ì´ˆë¡ë¶ˆ ìœ ì§€ (ë§¥ë°• íš¨ê³¼) â”€â”€
            # Anthropic/OpenAI/Google í”„ë¡œë°”ì´ë” ë¬´ê´€, ê²°ê³¼ ì—†ëŠ” ì „ë¬¸ê°€ì—ê²Œ ì´ˆë¡ë¶ˆ
            target_id = chain.get("target_id", "")
            specialists = _MANAGER_SPECIALISTS.get(target_id, [])
            for spec_id in specialists:
                spec_res = chain["results"].get("specialists", {}).get(spec_id)
                if spec_res is None:
                    spec_name = _SPECIALIST_NAMES.get(spec_id, spec_id)
                    await _broadcast_status(spec_id, "working", 0.5, f"{spec_name} ë°°ì¹˜ ì²˜ë¦¬ ì¤‘...")
            return

        if all_done:
            # ëª¨ë“  ì „ë¬¸ê°€ ë°°ì¹˜ ì™„ë£Œ â†’ ê²°ê³¼ ìˆ˜ì§‘
            retrieve_errors = []
            for batch_info in chain["batches"]["specialists"]:
                if batch_info.get("status") != "completed":
                    if batch_info.get("status") == "failed":
                        err_detail = batch_info.get("error", "ë°°ì¹˜ ì‹¤íŒ¨")
                        retrieve_errors.append(f"{batch_info['provider']}: {err_detail[:80]}")
                    continue

                result = await batch_retrieve(batch_info["batch_id"], batch_info["provider"])
                if "error" in result:
                    retrieve_errors.append(f"{batch_info['provider']}: {result['error'][:80]}")
                    _log(f"[CHAIN] ì „ë¬¸ê°€ ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨ ({batch_info['provider']}): {result['error']}")
                    continue

                for r in result.get("results", []):
                    custom_id = r.get("custom_id", "")
                    mapping = chain["custom_id_map"].get(custom_id, {})
                    agent_id = mapping.get("agent_id", custom_id)

                    chain["results"]["specialists"][agent_id] = {
                        "content": r.get("content", ""),
                        "model": r.get("model", ""),
                        "cost_usd": r.get("cost_usd", 0),
                        "error": r.get("error"),
                    }
                    chain["total_cost_usd"] += r.get("cost_usd", 0)
                    # ì „ë¬¸ê°€ ì´ˆë¡ë¶ˆ ë„ê¸°
                    await _broadcast_status(agent_id, "done", 1.0, "ì™„ë£Œ")

            spec_count = len(chain["results"]["specialists"])
            _log(f"[CHAIN] {chain['chain_id']} â€” ì „ë¬¸ê°€ {spec_count}ëª… ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ")

            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ì›ì¸ì„ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì „ë‹¬
            if spec_count == 0:
                all_errors = batch_errors + retrieve_errors
                if all_errors:
                    error_summary = " | ".join(all_errors[:3])
                    await _broadcast_chain_status(chain, f"âš ï¸ ì „ë¬¸ê°€ ë°°ì¹˜ ì‹¤íŒ¨ â€” ì›ì¸: {error_summary}")
                else:
                    await _broadcast_chain_status(chain, "âš ï¸ ì „ë¬¸ê°€ ë°°ì¹˜ ê²°ê³¼ ì—†ìŒ â€” íŒ€ì¥ ì§ì ‘ ì²˜ë¦¬ë¡œ ì „í™˜")

            # â”€â”€ í’ˆì§ˆê²€ìˆ˜ HOOK: ì „ë¬¸ê°€ ê²°ê³¼ ê²€ìˆ˜ â”€â”€
            if spec_count > 0 and app_state.quality_gate:
                target_id_qa = chain.get("target_id", "chief_of_staff")
                if target_id_qa not in _DORMANT_MANAGERS:
                    await _broadcast_chain_status(chain, "ğŸ” ì „ë¬¸ê°€ ë³´ê³ ì„œ í’ˆì§ˆê²€ìˆ˜ ì‹œì‘...")
                    failed_specs = await _quality_review_specialists(chain)
                    if failed_specs:
                        _save_chain(chain)
                        await _handle_specialist_rework(chain, failed_specs)
                        _save_chain(chain)
                    qa_msg = f"âœ… í’ˆì§ˆê²€ìˆ˜ ì™„ë£Œ (í•©ê²© {spec_count - len(failed_specs)}/{spec_count}ëª…)"
                    await _broadcast_chain_status(chain, qa_msg)

            # ì¢…í•© ë‹¨ê³„ë¡œ ì§„í–‰ â€” íŒ€ì¥ ì´ˆë¡ë¶ˆ ì¼œê¸°
            target_id = chain.get("target_id", "chief_of_staff")
            target_name = _AGENT_NAMES.get(target_id, target_id)
            await _broadcast_status(target_id, "working", 0.7, f"{target_name} ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘...")

            chain["step"] = "synthesis"
            _save_chain(chain)
            await _broadcast_chain_status(chain, f"ğŸ“¦ ì „ë¬¸ê°€ {spec_count}ëª… ì™„ë£Œ â†’ ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì‹œì‘")
            await _chain_submit_synthesis(chain)

    # â”€â”€ 4ë‹¨ê³„: ì¢…í•©ë³´ê³ ì„œ â”€â”€
    elif step == "synthesis":
        all_done = True
        synth_errors = []  # ì˜¤ë¥˜ ì¶”ì 
        for batch_info in chain["batches"].get("synthesis", []):
            if batch_info.get("status") in ("pending", "processing"):
                try:
                    status_result = await batch_check(batch_info["batch_id"], batch_info["provider"])
                    if "error" not in status_result:
                        batch_info["status"] = status_result["status"]
                    else:
                        err = status_result.get("error", "ë°°ì¹˜ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
                        _log(f"[CHAIN] ì¢…í•© ë°°ì¹˜ í™•ì¸ ì˜¤ë¥˜ ({batch_info['provider']}): {err}")
                        synth_errors.append(f"{batch_info['provider']}: {err[:80]}")
                except Exception as e:
                    _log(f"[CHAIN] ì¢…í•© ë°°ì¹˜ í™•ì¸ ì˜ˆì™¸ ({batch_info.get('provider','?')}): {e}")
                    synth_errors.append(f"{batch_info.get('provider','?')}: {str(e)[:80]}")

            if batch_info.get("status") not in ("completed", "failed"):
                all_done = False

        _save_chain(chain)

        if not all_done:
            # â”€â”€ ì¢…í•©ë³´ê³ ì„œ ë°°ì¹˜ ëŒ€ê¸° ì¤‘ â†’ íŒ€ì¥ ì´ˆë¡ë¶ˆ ìœ ì§€ â”€â”€
            target_id = chain.get("target_id", "chief_of_staff")
            target_name = _AGENT_NAMES.get(target_id, target_id)
            await _broadcast_status(target_id, "working", 0.8, f"{target_name} ì¢…í•©ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
            return

        if all_done:
            # ì¢…í•©ë³´ê³ ì„œ ê²°ê³¼ ìˆ˜ì§‘
            retrieve_errors = []
            for batch_info in chain["batches"]["synthesis"]:
                if batch_info.get("status") != "completed":
                    if batch_info.get("status") == "failed":
                        err_detail = batch_info.get("error", "ë°°ì¹˜ ì‹¤íŒ¨")
                        retrieve_errors.append(f"{batch_info['provider']}: {err_detail[:80]}")
                    continue

                result = await batch_retrieve(batch_info["batch_id"], batch_info["provider"])
                if "error" in result:
                    retrieve_errors.append(f"{batch_info['provider']}: {result['error'][:80]}")
                    _log(f"[CHAIN] ì¢…í•© ê²°ê³¼ ìˆ˜ì§‘ ì‹¤íŒ¨ ({batch_info['provider']}): {result['error']}")
                    continue

                for r in result.get("results", []):
                    custom_id = r.get("custom_id", "")
                    mapping = chain["custom_id_map"].get(custom_id, {})
                    agent_id = mapping.get("agent_id", custom_id)

                    chain["results"]["synthesis"][agent_id] = {
                        "content": r.get("content", ""),
                        "model": r.get("model", ""),
                        "cost_usd": r.get("cost_usd", 0),
                        "error": r.get("error"),
                    }
                    chain["total_cost_usd"] += r.get("cost_usd", 0)

            synth_count = len(chain["results"]["synthesis"])
            _log(f"[CHAIN] {chain['chain_id']} â€” ì¢…í•©ë³´ê³ ì„œ {synth_count}ê°œ ì™„ë£Œ")

            # ì¢…í•© ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ì›ì¸ ì•Œë¦¼ + ì‹¤ì‹œê°„ í´ë°±
            if synth_count == 0:
                all_errors = synth_errors + retrieve_errors
                if all_errors:
                    error_summary = " | ".join(all_errors[:3])
                    await _broadcast_chain_status(chain, f"âš ï¸ ì¢…í•© ë°°ì¹˜ ì‹¤íŒ¨ â€” ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬ì²˜ë¦¬: {error_summary}")
                else:
                    await _broadcast_chain_status(chain, "âš ï¸ ì¢…í•© ë°°ì¹˜ ê²°ê³¼ ì—†ìŒ â€” ì‹¤ì‹œê°„ìœ¼ë¡œ ì¬ì²˜ë¦¬")

                # ì‹¤ì‹œê°„ í´ë°±: ask_ai()ë¡œ ì§ì ‘ ì¢…í•©ë³´ê³ ì„œ ìƒì„±
                await _synthesis_realtime_fallback(chain)
                return

            # â”€â”€ í’ˆì§ˆê²€ìˆ˜ HOOK #2: ì¢…í•©ë³´ê³ ì„œ ê²€ìˆ˜ (ê²½ê³  ë±ƒì§€ë§Œ, ì¬ì‘ì—… ì—†ìŒ) â”€â”€
            if app_state.quality_gate and synth_count > 0:
                target_id_qa2 = chain.get("target_id", "chief_of_staff")
                if target_id_qa2 not in _DORMANT_MANAGERS:
                    division = _MANAGER_DIVISION.get(target_id_qa2, "default")
                    reviewer_model = _get_model_override(target_id_qa2) or "claude-sonnet-4-6"
                    task_desc = chain.get("original_command", "")[:500]
                    for agent_id, synth_data in chain["results"]["synthesis"].items():
                        try:
                            review = await app_state.quality_gate.hybrid_review(
                                result_data=synth_data.get("content", ""),
                                task_description=task_desc,
                                model_router=_qa_router,
                                reviewer_id=target_id_qa2,
                                reviewer_model=reviewer_model,
                                division=division,
                                target_agent_id=agent_id,
                            )
                            app_state.quality_gate.record_review(review, target_id_qa2, agent_id, task_desc)
                            if not review.passed:
                                synth_data["quality_warning"] = (
                                    " / ".join(review.rejection_reasons)[:200]
                                    if review.rejection_reasons else "í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬"
                                )
                                _log(f"[QA] âš ï¸ ì¢…í•©ë³´ê³ ì„œ ë¶ˆí•©ê²©: {agent_id} (ì ìˆ˜={review.weighted_average:.1f})")
                            else:
                                synth_data["quality_score"] = round(review.weighted_average, 1)
                                _log(f"[QA] âœ… ì¢…í•©ë³´ê³ ì„œ í•©ê²©: {agent_id} (ì ìˆ˜={review.weighted_average:.1f})")
                        except Exception as e:
                            _log(f"[QA] ì¢…í•©ë³´ê³ ì„œ ê²€ìˆ˜ ì˜¤ë¥˜ ({agent_id}): {e}")
                    _save_chain(chain)

            # íŒ€ì¥ ì´ˆë¡ë¶ˆ ë„ê¸°
            target_id = chain.get("target_id", "chief_of_staff")
            await _broadcast_status(target_id, "done", 1.0, "ë³´ê³  ì™„ë£Œ")

            # ìµœì¢… ì „ë‹¬
            await _deliver_chain_result(chain)


@app.get("/api/batch/chains")
async def get_batch_chains():
    """ì§„í–‰ ì¤‘ì¸ ë°°ì¹˜ ì²´ì¸ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    chains = load_setting("batch_chains") or []
    active = [c for c in chains if c.get("status") in ("running", "pending")]
    recent_done = [c for c in chains if c.get("status") in ("completed", "failed")][-10:]
    return {"active": active, "recent": recent_done}


# â”€â”€ í¬ë¡  ì‹¤í–‰ ì—”ì§„ (asyncio ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬) â”€â”€

# app_state.cron_task â†’ app_state.cron_task ì§ì ‘ ì‚¬ìš©



def _parse_cron_preset(preset: str) -> dict:
    """í¬ë¡  í”„ë¦¬ì…‹ì„ ì‹¤í–‰ ì¡°ê±´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    presets = {
        "every_minute": {"interval_seconds": 60},
        "every_5min": {"interval_seconds": 300},
        "every_30min": {"interval_seconds": 1800},
        "hourly": {"interval_seconds": 3600},
        "daily_9am": {"hour": 9, "minute": 0},
        "daily_6pm": {"hour": 18, "minute": 0},
        "weekday_9am": {"hour": 9, "minute": 0, "weekday_only": True},
        "monday_9am": {"hour": 9, "minute": 0, "day_of_week": 0},
    }
    return presets.get(preset, {"interval_seconds": 3600})


def _match_cron_field(field: str, value: int, max_val: int) -> bool:
    """í¬ë¡  í•„ë“œ í•˜ë‚˜ë¥¼ ë§¤ì¹­í•©ë‹ˆë‹¤. (ì˜ˆ: "1-5" â†’ ì›”~ê¸ˆ, "*/10" â†’ 0,10,20,30,40,50)"""
    if field == "*":
        return True
    for part in field.split(","):
        part = part.strip()
        if "/" in part:
            base, step = part.split("/", 1)
            step = int(step)
            if base == "*":
                if value % step == 0:
                    return True
            else:
                start = int(base)
                if value >= start and (value - start) % step == 0:
                    return True
        elif "-" in part:
            lo, hi = part.split("-", 1)
            if int(lo) <= value <= int(hi):
                return True
        else:
            if int(part) == value:
                return True
    return False


def _match_cron_expr(cron: str, now: datetime) -> bool:
    """5í•„ë“œ í¬ë¡  í‘œí˜„ì‹ê³¼ í˜„ì¬ ì‹œê°„ì„ ë§¤ì¹­í•©ë‹ˆë‹¤.
    í˜•ì‹: ë¶„ ì‹œ ì¼ ì›” ìš”ì¼ (0=ì¼, 1=ì›” ... 6=í†  / ë˜ëŠ” 0=ì›” ... 6=ì¼ ë¦¬ëˆ…ìŠ¤ í‘œì¤€)
    ì—¬ê¸°ì„œëŠ” ë¦¬ëˆ…ìŠ¤ í‘œì¤€: 0=ì¼, 1=ì›”, ..., 6=í† 
    """
    fields = cron.strip().split()
    if len(fields) != 5:
        return False
    minute, hour, dom, month, dow = fields
    # Python weekday(): 0=ì›” â†’ í¬ë¡  ë³€í™˜: (python_weekday + 1) % 7 â†’ 0=ì¼
    cron_dow = (now.weekday() + 1) % 7
    return (
        _match_cron_field(minute, now.minute, 59)
        and _match_cron_field(hour, now.hour, 23)
        and _match_cron_field(dom, now.day, 31)
        and _match_cron_field(month, now.month, 12)
        and _match_cron_field(dow, cron_dow, 6)
    )


def _should_run_schedule(schedule: dict, now: datetime) -> bool:
    """í˜„ì¬ ì‹œê°„ì— ì´ ì˜ˆì•½ì„ ì‹¤í–‰í•´ì•¼ í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    if not schedule.get("enabled", False):
        return False

    # ë§ˆì§€ë§‰ ì‹¤í–‰ ì‹œê°„ í™•ì¸
    last_run = schedule.get("last_run_ts", 0)
    elapsed = now.timestamp() - last_run

    # 1ìˆœìœ„: ì‹¤ì œ í¬ë¡  í‘œí˜„ì‹ì´ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ íŒë‹¨
    cron_expr = schedule.get("cron", "")
    if cron_expr and cron_expr.strip().count(" ") >= 3:
        if _match_cron_expr(cron_expr, now):
            return elapsed >= 55  # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€
        return False

    # 2ìˆœìœ„: í”„ë¦¬ì…‹ ê¸°ë°˜ (í•˜ìœ„í˜¸í™˜)
    preset = schedule.get("cron_preset", "")
    cron_config = _parse_cron_preset(preset)

    if "interval_seconds" in cron_config:
        return elapsed >= cron_config["interval_seconds"]

    # ì‹œ/ë¶„ ê¸°ë°˜ ìŠ¤ì¼€ì¤„
    if now.hour == cron_config.get("hour", -1) and now.minute == cron_config.get("minute", -1):
        if cron_config.get("weekday_only") and now.weekday() >= 5:
            return False
        if "day_of_week" in cron_config and now.weekday() != cron_config["day_of_week"]:
            return False
        # ê°™ì€ ì‹œê°ì— ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (ìµœì†Œ 55ì´ˆ ê°„ê²©)
        return elapsed >= 55
    return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹ ë¢°ë„ ê²€ì¦ íŒŒì´í”„ë¼ì¸ â€” í•™ìŠµ ì—”ì§„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_CIO_ANALYSTS = [
    "cio_manager", "market_condition_specialist", "stock_analysis_specialist",
    "technical_analysis_specialist", "risk_management_specialist",
]


def _run_confidence_learning_pipeline(verified_7d_ids: list[int]) -> None:
    """7ì¼ ê²€ì¦ ì™„ë£Œëœ ì˜ˆì¸¡ì— ëŒ€í•´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰.
    â‘  ELO ì—…ë°ì´íŠ¸ â†’ â‘¡ ì¹¼ë¦¬ë¸Œë ˆì´ì…˜ ê°±ì‹  â†’ â‘¢ ë„êµ¬ íš¨ê³¼ â†’ â‘£ ì˜¤ë‹µ íŒ¨í„´ íƒì§€
    """
    _lp = logging.getLogger("corthex.confidence")
    try:
        for pred_id in verified_7d_ids:
            _update_analyst_elos_for_prediction(pred_id)
        _lp.info("[í•™ìŠµ] ELO ì—…ë°ì´íŠ¸ ì™„ë£Œ: %dê±´", len(verified_7d_ids))
    except Exception as e:
        _lp.warning("[í•™ìŠµ] ELO ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", e)

    try:
        _rebuild_calibration_buckets()
        _lp.info("[í•™ìŠµ] ì¹¼ë¦¬ë¸Œë ˆì´ì…˜ ë²„í‚· ê°±ì‹  ì™„ë£Œ")
    except Exception as e:
        _lp.warning("[í•™ìŠµ] ì¹¼ë¦¬ë¸Œë ˆì´ì…˜ ê°±ì‹  ì‹¤íŒ¨: %s", e)

    try:
        for pred_id in verified_7d_ids:
            _update_tool_effectiveness_for_prediction(pred_id)
        _lp.info("[í•™ìŠµ] ë„êµ¬ íš¨ê³¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    except Exception as e:
        _lp.warning("[í•™ìŠµ] ë„êµ¬ íš¨ê³¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", e)

    try:
        _detect_error_patterns()
        _lp.info("[í•™ìŠµ] ì˜¤ë‹µ íŒ¨í„´ íƒì§€ ì™„ë£Œ")
    except Exception as e:
        _lp.warning("[í•™ìŠµ] ì˜¤ë‹µ íŒ¨í„´ íƒì§€ ì‹¤íŒ¨: %s", e)


def _update_analyst_elos_for_prediction(prediction_id: int) -> None:
    """ë‹¨ì¼ ì˜ˆì¸¡ì— ëŒ€í•´ 5ëª… ì „ë¬¸ê°€ ELOë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    import math
    from db import (
        get_prediction_specialists, get_analyst_elo, upsert_analyst_elo,
        save_elo_history,
    )

    conn = get_connection()
    try:
        pred = conn.execute(
            "SELECT correct_7d, return_pct_7d, direction, confidence "
            "FROM cio_predictions WHERE id=?", (prediction_id,)
        ).fetchone()
    finally:
        conn.close()
    if not pred or pred[0] is None:
        return

    correct_7d = pred[0]
    return_pct = pred[1] or 0.0
    direction = pred[2]

    # ì „ë¬¸ê°€ ë°ì´í„°
    spec_data = get_prediction_specialists(prediction_id)
    spec_map = {s["agent_id"]: s for s in spec_data}

    # í˜„ì¬ ELO ì¡°íšŒ + í‰ê·  ELO ê³„ì‚°
    elos = {aid: get_analyst_elo(aid) for aid in _CIO_ANALYSTS}
    avg_elo = sum(e["elo_rating"] for e in elos.values()) / len(elos)

    for agent_id in _CIO_ANALYSTS:
        current = elos[agent_id]
        agent_elo = current["elo_rating"]
        total = current["total_predictions"]

        # ì „ë¬¸ê°€ê°€ ì´ ì˜ˆì¸¡ì— ì°¸ì—¬í–ˆëŠ”ì§€ í™•ì¸
        spec_info = spec_map.get(agent_id)
        if spec_info:
            # ê°œë³„ ì „ë¬¸ê°€ì˜ ì¶”ì²œì´ ì‹¤ì œ ê²°ê³¼ì™€ ì¼ì¹˜í•˜ëŠ”ì§€
            rec = spec_info.get("recommendation", "HOLD")
            if rec in ("BUY", "SELL"):
                agent_correct = 1 if (
                    (rec == direction and correct_7d == 1) or
                    (rec != direction and correct_7d == 0)
                ) else 0
                outcome = 1.0 if agent_correct else 0.0
                # ë¶€ë¶„ì ì¤‘: ë°©í–¥ ë§ìœ¼ë‚˜ ìˆ˜ìµ < 0.5%
                if agent_correct and abs(return_pct) < 0.5:
                    outcome = 0.5
            else:
                # HOLD ì¶”ì²œ â†’ ê´€ë§ì€ ì•½ê°„ì˜ ë³´ìƒ/íŒ¨ë„í‹°
                outcome = 0.5
        else:
            # ì „ë¬¸ê°€ ë°ì´í„° ì—†ìœ¼ë©´ ì „ì²´ ê²°ê³¼ ì‚¬ìš©
            outcome = 1.0 if correct_7d else 0.0

        # K-factor: ì²« 30ê±´ì€ K=48 (ë¹ ë¥¸ ì¡°ì •), ì´í›„ K=32
        k = 48 if total < 30 else 32

        # ELO ë³€ë™ ê³„ì‚°
        expected = 1.0 / (1.0 + math.pow(10, (avg_elo - agent_elo) / 400.0))
        elo_change = round(k * (outcome - expected), 2)
        new_elo = round(agent_elo + elo_change, 1)

        # DB ì—…ë°ì´íŠ¸
        new_total = total + 1
        new_correct = current["correct_predictions"] + (1 if outcome >= 0.75 else 0)
        # ì´ë™ í‰ê·  ìˆ˜ìµë¥ 
        old_avg_ret = current["avg_return_pct"]
        new_avg_ret = round(
            (old_avg_ret * total + return_pct) / new_total if new_total > 0 else 0, 2
        )

        upsert_analyst_elo(agent_id, new_elo, new_total, new_correct, new_avg_ret)
        save_elo_history(agent_id, prediction_id, agent_elo, new_elo, elo_change,
                         1 if outcome >= 0.75 else 0, return_pct)


def _rebuild_calibration_buckets() -> None:
    """cio_predictions ì „ì²´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹¼ë¦¬ë¸Œë ˆì´ì…˜ ë²„í‚·ì„ ì¬ê³„ì‚°í•©ë‹ˆë‹¤."""
    import math
    from db import upsert_calibration_bucket

    conn = get_connection()
    try:
        rows = conn.execute(
            """SELECT
                 CASE
                   WHEN confidence < 60 THEN '50-60'
                   WHEN confidence < 70 THEN '60-70'
                   WHEN confidence < 80 THEN '70-80'
                   WHEN confidence < 90 THEN '80-90'
                   ELSE '90-100'
                 END as bucket,
                 COUNT(*) as total,
                 SUM(CASE WHEN correct_7d=1 THEN 1 ELSE 0 END) as correct
               FROM cio_predictions
               WHERE correct_7d IS NOT NULL
               GROUP BY bucket"""
        ).fetchall()
    finally:
        conn.close()

    for r in rows:
        bucket, total, correct = r[0], r[1], r[2]
        # Beta ë¶„í¬: ì‚¬ì „ë¶„í¬ Beta(1,1) + ë°ì´í„°
        alpha = 1.0 + correct
        beta_val = 1.0 + (total - correct)
        actual_rate = round(alpha / (alpha + beta_val), 4)
        # 95% CI: ì •ê·œ ê·¼ì‚¬ (scipy ë¶ˆí•„ìš”)
        ab = alpha + beta_val
        var = (alpha * beta_val) / (ab * ab * (ab + 1))
        std = math.sqrt(var) if var > 0 else 0
        ci_lower = round(max(0, actual_rate - 1.96 * std), 4)
        ci_upper = round(min(1, actual_rate + 1.96 * std), 4)

        upsert_calibration_bucket(
            bucket, total, correct, actual_rate, alpha, beta_val, ci_lower, ci_upper
        )


def _update_tool_effectiveness_for_prediction(prediction_id: int) -> None:
    """ë‹¨ì¼ ì˜ˆì¸¡ì— ëŒ€í•´ ë„êµ¬ë³„ íš¨ê³¼ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    import json as _json_te
    from db import get_prediction_specialists, upsert_tool_effectiveness, get_tool_effectiveness_all

    conn = get_connection()
    try:
        pred = conn.execute(
            "SELECT correct_7d FROM cio_predictions WHERE id=?", (prediction_id,)
        ).fetchone()
    finally:
        conn.close()
    if not pred or pred[0] is None:
        return

    correct = pred[0] == 1
    spec_data = get_prediction_specialists(prediction_id)

    # ê¸°ì¡´ ë„êµ¬ íš¨ê³¼ ìºì‹œ
    existing = {t["tool_name"]: t for t in get_tool_effectiveness_all()}

    tools_seen = set()
    for spec in spec_data:
        try:
            tools = _json_te.loads(spec.get("tools_used", "[]"))
        except (ValueError, TypeError):
            tools = []
        for tool in tools:
            if tool in tools_seen:
                continue
            tools_seen.add(tool)
            e = existing.get(tool, {"used_correct": 0, "used_incorrect": 0, "total_uses": 0})
            new_correct = e["used_correct"] + (1 if correct else 0)
            new_incorrect = e["used_incorrect"] + (0 if correct else 1)
            new_total = e["total_uses"] + 1
            eff = round(new_correct / new_total, 4) if new_total > 0 else 0.5
            upsert_tool_effectiveness(tool, new_correct, new_incorrect, new_total, eff)


def _detect_error_patterns() -> None:
    """ê²€ì¦ëœ ì˜ˆì¸¡ì—ì„œ ì˜¤ë‹µ íŒ¨í„´ì„ íƒì§€í•©ë‹ˆë‹¤."""
    from db import upsert_error_pattern

    conn = get_connection()
    try:
        # íŒ¨í„´ 1: ì‹ ë¢°ë„ êµ¬ê°„ë³„ ê³¼ì‹  íƒì§€
        overconf_rows = conn.execute(
            """SELECT
                 CASE WHEN confidence >= 80 THEN 'high_confidence_overfit'
                      WHEN confidence >= 70 THEN 'mid_confidence_overfit'
                      ELSE NULL END as ptype,
                 COUNT(*) as total,
                 SUM(CASE WHEN correct_7d=1 THEN 1 ELSE 0 END) as correct
               FROM cio_predictions
               WHERE correct_7d IS NOT NULL AND confidence >= 70
               GROUP BY ptype HAVING ptype IS NOT NULL"""
        ).fetchall()
        for r in overconf_rows:
            ptype, total, correct = r[0], r[1], r[2]
            miss = total - correct
            hit_rate = round(correct / total * 100, 1) if total > 0 else 0
            if total >= 5 and hit_rate < 60:
                conf_range = "80%+" if "high" in ptype else "70-80%"
                upsert_error_pattern(
                    ptype,
                    f"ì‹ ë¢°ë„ {conf_range} ì‹œê·¸ë„ì˜ ì‹¤ì œ ì ì¤‘ë¥ ì´ {hit_rate}%ë¡œ ë‚®ìŒ ({correct}/{total}ê±´)",
                    correct, miss, hit_rate,
                )

        # íŒ¨í„´ 2: ê°™ì€ ì¢…ëª© ì—°ì† ì˜¤ë‹µ (3íšŒ+)
        streak_rows = conn.execute(
            """SELECT ticker, ticker_name, COUNT(*) as miss_streak
               FROM cio_predictions
               WHERE correct_7d = 0
               GROUP BY ticker HAVING miss_streak >= 3
               ORDER BY miss_streak DESC LIMIT 5"""
        ).fetchall()
        for r in streak_rows:
            ticker, name, streak = r[0], r[1] or r[0], r[2]
            # í•´ë‹¹ ì¢…ëª©ì˜ ì „ì²´ ê¸°ë¡
            ticker_total = conn.execute(
                "SELECT COUNT(*), SUM(CASE WHEN correct_7d=1 THEN 1 ELSE 0 END) "
                "FROM cio_predictions WHERE ticker=? AND correct_7d IS NOT NULL",
                (ticker,),
            ).fetchone()
            t_total = ticker_total[0] or 0
            t_correct = ticker_total[1] or 0
            hit_rate = round(t_correct / t_total * 100, 1) if t_total > 0 else 0
            upsert_error_pattern(
                f"ticker_streak_{ticker}",
                f"{name}({ticker}) ì—°ì† {streak}íšŒ ì˜¤ë‹µ, ì „ì²´ ì ì¤‘ë¥  {hit_rate}% ({t_correct}/{t_total})",
                t_correct, t_total - t_correct, hit_rate,
            )

        # íŒ¨í„´ 3: ë§¤ìˆ˜/ë§¤ë„ í¸í–¥
        dir_rows = conn.execute(
            """SELECT direction, COUNT(*) as total,
                      SUM(CASE WHEN correct_7d=1 THEN 1 ELSE 0 END) as correct
               FROM cio_predictions WHERE correct_7d IS NOT NULL
               GROUP BY direction"""
        ).fetchall()
        for r in dir_rows:
            direction, total, correct = r[0], r[1], r[2]
            miss = total - correct
            hit_rate = round(correct / total * 100, 1) if total > 0 else 0
            if total >= 5 and hit_rate < 45:
                upsert_error_pattern(
                    f"direction_bias_{direction.lower()}",
                    f"{direction} ì‹œê·¸ë„ ì ì¤‘ë¥  {hit_rate}% ({correct}/{total}ê±´) â€” í¸í–¥ ì£¼ì˜",
                    correct, miss, hit_rate,
                )
    finally:
        conn.close()


def _capture_specialist_contributions_sync(
    parsed_signals: list[dict],
    spec_results: list[dict],
    cio_solo_content: str,
    sig_id: str,
) -> None:
    """ì „ë¬¸ê°€ë³„ ê¸°ì—¬ë¥¼ prediction_specialist_data í…Œì´ë¸”ì— ê¸°ë¡í•©ë‹ˆë‹¤.

    parsed_signalsì—ì„œ ì˜ˆì¸¡ IDë¥¼ ì°¾ê³ , spec_resultsì—ì„œ ê° ì „ë¬¸ê°€ì˜
    ì¶”ì²œ(BUY/SELL/HOLD)ì„ íŒŒì‹±í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    import json as _json_cap
    import re as _re_cap
    from db import save_prediction_specialist, get_connection

    if not parsed_signals or not spec_results:
        return

    try:
        conn = get_connection()
        # sig_id(task_id)ë¡œ ì €ì¥ëœ ì˜ˆì¸¡ IDë“¤ ì¡°íšŒ
        pred_rows = conn.execute(
            "SELECT id, ticker, direction FROM cio_predictions WHERE task_id=? ORDER BY id DESC",
            (sig_id,),
        ).fetchall()
        conn.close()

        if not pred_rows:
            logger.debug("[ì‹ ë¢°ë„] ì˜ˆì¸¡ ID ì¡°íšŒ ì‹¤íŒ¨ (sig_id=%s)", sig_id)
            return

        # ì „ë¬¸ê°€ë³„ ì¶”ì²œ ì¶”ì¶œ íŒ¨í„´
        _buy_pat = _re_cap.compile(r"(?:ë§¤ìˆ˜|BUY|buy|ê°•ë ¥\s*ë§¤ìˆ˜|ì ê·¹\s*ë§¤ìˆ˜)", _re_cap.IGNORECASE)
        _sell_pat = _re_cap.compile(r"(?:ë§¤ë„|SELL|sell|ê°•ë ¥\s*ë§¤ë„)", _re_cap.IGNORECASE)

        for pred_row in pred_rows:
            pred_id = pred_row[0]

            # CIO íŒ€ì¥ ë…ìë¶„ì„ ê¸°ì—¬ ì €ì¥
            if cio_solo_content:
                cio_rec = "HOLD"
                if _buy_pat.search(cio_solo_content[:500]):
                    cio_rec = "BUY"
                elif _sell_pat.search(cio_solo_content[:500]):
                    cio_rec = "SELL"
                save_prediction_specialist(
                    prediction_id=pred_id,
                    agent_id="cio_manager",
                    recommendation=cio_rec,
                    confidence=0.0,
                    tools_used="[]",
                    cost_usd=0.0,
                )

            # ê° ì „ë¬¸ê°€ ê¸°ì—¬ ì €ì¥
            for r in spec_results:
                if not isinstance(r, dict) or "error" in r:
                    continue
                agent_id = r.get("agent_id", "unknown")
                content = r.get("content", "")
                tools = r.get("tools_used", [])
                cost = r.get("cost_usd", 0)

                # ì¶”ì²œ ì¶”ì¶œ
                rec = "HOLD"
                snippet = content[:800] if content else ""
                if _buy_pat.search(snippet):
                    rec = "BUY"
                elif _sell_pat.search(snippet):
                    rec = "SELL"

                save_prediction_specialist(
                    prediction_id=pred_id,
                    agent_id=agent_id,
                    recommendation=rec,
                    confidence=0.0,
                    tools_used=_json_cap.dumps(tools[:20]) if tools else "[]",
                    cost_usd=cost or 0.0,
                )

        logger.info("[ì‹ ë¢°ë„] ì „ë¬¸ê°€ ê¸°ì—¬ %dê±´ Ã— %dì˜ˆì¸¡ ìº¡ì²˜ ì™„ë£Œ",
                     len(spec_results) + (1 if cio_solo_content else 0), len(pred_rows))
    except Exception as e:
        logger.warning("[ì‹ ë¢°ë„] ì „ë¬¸ê°€ ê¸°ì—¬ ìº¡ì²˜ ì‹¤íŒ¨: %s", e)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CIO ìê¸°í•™ìŠµ í¬ë¡  + Shadow Trading ì•Œë¦¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def _cio_prediction_verifier():
    """CIO ì˜ˆì¸¡ ì‚¬í›„ê²€ì¦: 3ì¼Â·7ì¼ ê²½ê³¼í•œ ì˜ˆì¸¡ì˜ ì‹¤ì œ ì£¼ê°€ ì¡°íšŒ â†’ ë§ìŒ/í‹€ë¦¼ DB ì €ì¥ (ë§¤ì¼ KST 03:00)."""
    import pytz as _pytz_v
    _KST_v = _pytz_v.timezone("Asia/Seoul")
    _logger_v = logging.getLogger("corthex.cio_verify")
    _logger_v.info("[CIOê²€ì¦] ì£¼ê°€ ì‚¬í›„ê²€ì¦ ë£¨í”„ ì‹œì‘")

    while True:
        try:
            now = datetime.now(_KST_v)
            # ë§¤ì¼ 03:00 KSTì— ì‹¤í–‰
            target = now.replace(hour=3, minute=0, second=0, microsecond=0)
            if now >= target:
                target = target + timedelta(days=1)
            wait_sec = (target - now).total_seconds()
            await asyncio.sleep(wait_sec)

            _logger_v.info("[CIOê²€ì¦] ì‚¬í›„ê²€ì¦ ì‹œì‘")
            try:
                from db import get_pending_verifications, update_cio_prediction_result
                from kis_client import get_current_price

                verified_count = 0
                verified_results = []

                verified_7d_ids = []  # 7ì¼ ê²€ì¦ ì™„ë£Œëœ prediction_id (í•™ìŠµ íŒŒì´í”„ë¼ì¸ìš©)

                for days in [3, 7]:
                    pending = get_pending_verifications(days_threshold=days)
                    for p in pending:
                        try:
                            price = await get_current_price(p["ticker"])
                            if days == 3:
                                result = update_cio_prediction_result(p["id"], actual_price_3d=price)
                                correct = bool(result.get("correct_3d"))
                                verified_results.append({
                                    "correct_3d": correct, "ticker": p["ticker"],
                                    "direction": p.get("direction", "BUY"),
                                })
                                verified_count += 1
                            else:
                                result = update_cio_prediction_result(p["id"], actual_price_7d=price)
                                if result:
                                    verified_7d_ids.append(p["id"])
                            _logger_v.info("[CIOê²€ì¦] %s %dì¼ ê²€ì¦ ì™„ë£Œ: %dì›", p["ticker"], days, price)
                        except Exception as e:
                            _logger_v.warning("[CIOê²€ì¦] %s ì£¼ê°€ ì¡°íšŒ ì‹¤íŒ¨: %s", p["ticker"], e)

                save_activity_log("system", f"âœ… CIO ì˜ˆì¸¡ ì‚¬í›„ê²€ì¦ ì™„ë£Œ (3ì¼ {verified_count}ê±´, 7ì¼ {len(verified_7d_ids)}ê±´)", "info")

                # â”€â”€ ì‹ ë¢°ë„ í•™ìŠµ íŒŒì´í”„ë¼ì¸ (7ì¼ ê²€ì¦ ì™„ë£Œëœ ê±´ì— ëŒ€í•´) â”€â”€
                if verified_7d_ids:
                    try:
                        _run_confidence_learning_pipeline(verified_7d_ids)
                        _logger_v.info("[CIOí•™ìŠµ] ì‹ ë¢°ë„ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: %dê±´", len(verified_7d_ids))
                    except Exception as le:
                        _logger_v.warning("[CIOí•™ìŠµ] í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: %s", le)

                # ê²€ì¦ ì™„ë£Œ í›„ í…”ë ˆê·¸ë¨ ì•Œë¦¼ (ìˆ˜ì •: direction ë²„ê·¸ ìˆ˜ì •)
                if verified_count > 0:
                    try:
                        ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
                        if app_state.telegram_app and ceo_id:
                            correct_count = sum(1 for r in verified_results if r.get("correct_3d"))
                            accuracy = round(correct_count / verified_count * 100) if verified_count > 0 else 0
                            # ELO ìš”ì•½ ì¶”ê°€
                            from db import get_all_analyst_elos, get_cio_performance_summary
                            elo_data = get_all_analyst_elos()
                            perf = get_cio_performance_summary()
                            elo_section = "\n".join(
                                f"  {e['agent_id'].split('_')[0]}: {e['elo_rating']:.0f}"
                                for e in elo_data[:5]
                            ) if elo_data else "  (ì´ˆê¸°í™” ëŒ€ê¸° ì¤‘)"
                            brier_text = f"\nBrier Score: {perf.get('avg_brier_score', '-')}" if perf.get('avg_brier_score') else ""
                            msg = (
                                f"ğŸ“Š CIO ìê¸°í•™ìŠµ ê²€ì¦ ì™„ë£Œ\n"
                                f"ì˜¤ëŠ˜ ê²€ì¦: {verified_count}ê±´\n"
                                f"3ì¼ ì •í™•ë„: {accuracy}% ({correct_count}/{verified_count})\n"
                                f"ì „ì²´ 7ì¼ ì •í™•ë„: {perf.get('overall_accuracy', '-')}%{brier_text}\n"
                                f"ì „ë¬¸ê°€ ELO:\n{elo_section}"
                            )
                            await app_state.telegram_app.bot.send_message(
                                chat_id=int(ceo_id),
                                text=msg,
                            )
                    except Exception as te:
                        _logger_v.warning("[CIOê²€ì¦] í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹¤íŒ¨: %s", te)

            except ImportError as e:
                _logger_v.warning("[CIOê²€ì¦] í•„ìš” í•¨ìˆ˜ ë¯¸êµ¬í˜„ â€” ìŠ¤í‚µ: %s", e)
        except Exception as e:
            _logger_v.error("[CIOê²€ì¦] ì—ëŸ¬: %s", e)
            await asyncio.sleep(3600)  # ì—ëŸ¬ ì‹œ 1ì‹œê°„ í›„ ì¬ì‹œë„


async def _cio_weekly_soul_update():
    """ë§¤ì£¼ ì¼ìš”ì¼ KST 02:00: CLOê°€ CIO ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„ â†’ cio_manager.md ìë™ ì—…ë°ì´íŠ¸."""
    import pytz as _pytz_s
    import re as _re_s
    _KST_s = _pytz_s.timezone("Asia/Seoul")
    _logger_s = logging.getLogger("corthex.cio_soul")
    _logger_s.info("[CIOì†Œìš¸] ì£¼ê°„ soul ì—…ë°ì´íŠ¸ ë£¨í”„ ì‹œì‘")

    while True:
        try:
            now = datetime.now(_KST_s)
            # ë‹¤ìŒ ì¼ìš”ì¼ 02:00 KST ê³„ì‚° (weekday: ì›”=0, ì¼=6)
            days_until_sunday = (6 - now.weekday()) % 7
            if days_until_sunday == 0 and now.hour >= 2:
                days_until_sunday = 7
            target = (now + timedelta(days=days_until_sunday)).replace(
                hour=2, minute=0, second=0, microsecond=0
            )
            wait_sec = (target - now).total_seconds()
            await asyncio.sleep(wait_sec)

            try:
                from db import load_cio_predictions, get_cio_performance_summary
                summary = get_cio_performance_summary()
                recent = load_cio_predictions(limit=20)
            except ImportError as e:
                _logger_s.warning("[CIOì†Œìš¸] í•„ìš” í•¨ìˆ˜ ë¯¸êµ¬í˜„ â€” ìŠ¤í‚µ: %s", e)
                continue

            # ê²€ì¦ëœ ì˜ˆì¸¡(7ì¼ ê²°ê³¼ ìˆëŠ” ê²ƒ)ë§Œ í•„í„°ë§
            verified = [p for p in recent if p.get("correct_7d") is not None]
            if len(verified) < 3:
                _logger_s.info(
                    "[CIOì†Œìš¸] ê²€ì¦ëœ ì˜ˆì¸¡ %dê±´ â€” ì—…ë°ì´íŠ¸ ìŠ¤í‚µ (ìµœì†Œ 3ê±´ í•„ìš”)", len(verified)
                )
                continue

            predictions_text = "\n".join([
                f"- {p['ticker']}({p.get('ticker_name', '')}) {p['direction']}: "
                f"{'âœ…ë§ìŒ' if p['correct_7d'] == 1 else 'âŒí‹€ë¦¼'} "
                f"(ì˜ˆì¸¡ê°€ {p.get('predicted_price', '-')}ì› â†’ 7ì¼í›„ {p.get('actual_price_7d', '-')}ì›)"
                for p in verified
            ])

            analysis_prompt = (
                "ë‹¹ì‹ ì€ CLO(ì¤€ë²•ê°ì‹œì¸)ì…ë‹ˆë‹¤. CIO(íˆ¬ìíŒ€ì¥)ì˜ ìµœê·¼ íˆ¬ì ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬,\n"
                "ë°˜ë³µë˜ëŠ” ì˜¤ë¥˜ íŒ¨í„´ì„ ì°¾ê³  cio_manager.mdì— ì¶”ê°€í•  ê·œì¹™ì„ ì œì•ˆí•˜ì„¸ìš”.\n\n"
                f"## CIO ìµœê·¼ ì˜ˆì¸¡ ê²°ê³¼\n"
                f"ì „ì²´ ì •í™•ë„: {summary.get('overall_accuracy', '-')}%\n"
                f"ìµœê·¼ 20ê±´ ì •í™•ë„: {summary.get('recent_20_accuracy', '-')}%\n"
                f"ë§¤ìˆ˜ ì •í™•ë„: {summary.get('buy_accuracy', '-')}%\n"
                f"ë§¤ë„ ì •í™•ë„: {summary.get('sell_accuracy', '-')}%\n\n"
                f"## ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼\n{predictions_text}\n\n"
                "## ìš”ì²­\n"
                "1. ë°˜ë³µ ì˜¤ë¥˜ íŒ¨í„´ 3ê°€ì§€ ë¶„ì„ (ì˜ˆ: 'ë°˜ë„ì²´ ì„¹í„° ê³¼ëŒ€í‰ê°€ ê²½í–¥')\n"
                "2. ê° íŒ¨í„´ì— ëŒ€í•œ ê°œì„  ê·œì¹™ ì œì•ˆ (cio_manager.mdì— ì¶”ê°€í•  ë§ˆí¬ë‹¤ìš´ í˜•ì‹)\n"
                "3. ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹:\n"
                "---SOUL_UPDATE_START---\n"
                "[ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ê·œì¹™ ë‚´ìš©]\n"
                "---SOUL_UPDATE_END---"
            )

            try:
                result_dict = await _call_agent("clo_manager", analysis_prompt)
                result = result_dict.get("content", "") if isinstance(result_dict, dict) else str(result_dict)
                if not result:
                    _logger_s.warning("[CIOì†Œìš¸] CLO ì‘ë‹µ ì—†ìŒ")
                    continue

                match = _re_s.search(
                    r"---SOUL_UPDATE_START---\n(.*?)\n---SOUL_UPDATE_END---",
                    result,
                    _re_s.DOTALL,
                )
                if not match:
                    _logger_s.warning("[CIOì†Œìš¸] soul ì—…ë°ì´íŠ¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨")
                    continue

                new_content = match.group(1).strip()
                soul_path = os.path.normpath(
                    os.path.join(os.path.dirname(__file__), "..", "souls", "agents", "cio_manager.md")
                )

                if os.path.exists(soul_path):
                    update_date = datetime.now(_KST_s).strftime("%Y-%m-%d")
                    update_section = (
                        f"\n\n## ìë™ í•™ìŠµ ì—…ë°ì´íŠ¸ ({update_date})\n\n{new_content}"
                    )
                    with open(soul_path, "a", encoding="utf-8") as _f:
                        _f.write(update_section)
                    _logger_s.info("[CIOì†Œìš¸] soul ì—…ë°ì´íŠ¸ ì™„ë£Œ (%s)", update_date)
                    save_activity_log("system", f"CIO soul ì£¼ê°„ ì—…ë°ì´íŠ¸ ì™„ë£Œ ({update_date})", "info")
                else:
                    _logger_s.warning("[CIOì†Œìš¸] soul íŒŒì¼ ì—†ìŒ: %s", soul_path)
            except Exception as e:
                _logger_s.error("[CIOì†Œìš¸] CLO ë¶„ì„ ì‹¤íŒ¨: %s", e)

        except Exception as e:
            _logger_s.error("[CIOì†Œìš¸] ì—ëŸ¬: %s", e)
            await asyncio.sleep(3600)


async def _shadow_trading_alert():
    """Shadow Trading ì•Œë¦¼: ëª¨ì˜íˆ¬ì 2ì£¼ ìˆ˜ìµë¥  +5% ë‹¬ì„± ì‹œ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ì‹¤ê±°ë˜ ì „í™˜ ì¶”ì²œ (ë§¤ì¼ KST 09:00)."""
    import pytz as _pytz_a
    _KST_a = _pytz_a.timezone("Asia/Seoul")
    _logger_a = logging.getLogger("corthex.shadow_alert")
    _logger_a.info("[Shadowì•Œë¦¼] Shadow Trading ì•Œë¦¼ ë£¨í”„ ì‹œì‘")

    while True:
        try:
            now = datetime.now(_KST_a)
            # ë§¤ì¼ 09:00 KSTì— ì‹¤í–‰
            target = now.replace(hour=9, minute=0, second=0, microsecond=0)
            if now >= target:
                target = target + timedelta(days=1)
            wait_sec = (target - now).total_seconds()
            await asyncio.sleep(wait_sec)

            try:
                from kis_client import get_shadow_comparison
                shadow = await get_shadow_comparison()
            except (ImportError, Exception) as e:
                _logger_a.warning("[Shadowì•Œë¦¼] shadow ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ â€” ìŠ¤í‚µ: %s", e)
                continue

            mock_data = shadow.get("mock", {})
            if not mock_data.get("available"):
                continue

            # 2ì£¼ ìˆ˜ìµë¥  íˆìŠ¤í† ë¦¬ ì¶”ì  (DBì— ë³´ê´€)
            mock_history = load_setting("shadow_mock_history") or []
            today_entry = {
                "date": now.strftime("%Y-%m-%d"),
                "total_eval": mock_data.get("total_eval", 0),
                "cash": mock_data.get("cash", 0),
            }
            mock_history.append(today_entry)
            mock_history = mock_history[-30:]  # 30ì¼ì¹˜ë§Œ ë³´ê´€
            save_setting("shadow_mock_history", mock_history)

            # 2ì£¼(14ì¼) ì „ ë°ì´í„°ì™€ ë¹„êµ
            if len(mock_history) >= 14:
                old_entry = mock_history[-14]
                old_eval = old_entry.get("total_eval", 0)
                new_eval = today_entry.get("total_eval", 0)

                if old_eval > 0:
                    profit_rate = (new_eval - old_eval) / old_eval * 100

                    if profit_rate >= 5.0:  # Bì•ˆ: 2ì£¼ +5% ì´ìƒ ê¸°ì¤€
                        msg = (
                            f"[Shadow Trading ì•Œë¦¼]\n\n"
                            f"ëª¨ì˜íˆ¬ì 2ì£¼ ìˆ˜ìµë¥ : +{profit_rate:.1f}% ë‹¬ì„±!\n"
                            f"ê¸°ì¤€: 2ì£¼ +5% ì´ìƒ -> ì‹¤ê±°ë˜ ì „í™˜ ì¶”ì²œ\n\n"
                            f"ëª¨ì˜ í˜„ì¬ í‰ê°€ì•¡: {new_eval:,}ì›\n"
                            f"2ì£¼ ì „ í‰ê°€ì•¡: {old_eval:,}ì›\n\n"
                            f"ì „ëµì‹¤ -> 'ì‹¤ê±°ë˜/ëª¨ì˜ ë¹„êµ' íƒ­ì—ì„œ í™•ì¸í•˜ì„¸ìš”."
                        )
                        if app_state.telegram_app:
                            ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
                            if ceo_id:
                                try:
                                    await app_state.telegram_app.bot.send_message(
                                        chat_id=int(ceo_id),
                                        text=msg,
                                    )
                                    _logger_a.info(
                                        "[Shadowì•Œë¦¼] ì‹¤ê±°ë˜ ì „í™˜ ì¶”ì²œ ì•Œë¦¼ ë°œì†¡ (ìˆ˜ìµë¥  %.1f%%)", profit_rate
                                    )
                                    save_activity_log(
                                        "system",
                                        f"Shadow Trading ì•Œë¦¼: +{profit_rate:.1f}%",
                                        "info",
                                    )
                                except Exception as e:
                                    _logger_a.error("[Shadowì•Œë¦¼] í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨: %s", e)

        except Exception as e:
            _logger_a.error("[Shadowì•Œë¦¼] ì—ëŸ¬: %s", e)
            await asyncio.sleep(3600)


# â”€â”€ ì‹¤ì‹œê°„ í™˜ìœ¨ ê°±ì‹  â”€â”€
_FX_UPDATE_INTERVAL = 3600  # 1ì‹œê°„ë§ˆë‹¤ ê°±ì‹ 
# app_state.last_fx_update â†’ app_state.last_fx_update ì§ì ‘ ì‚¬ìš©

async def _update_fx_rate():
    """yfinanceë¡œ USD/KRW ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì™€ DBì— ì €ì¥í•©ë‹ˆë‹¤."""

    try:
        import yfinance as yf
        ticker = yf.Ticker("USDKRW=X")
        hist = ticker.history(period="1d")
        if not hist.empty:
            rate = round(float(hist.iloc[-1]["Close"]), 2)
            if 1000 < rate < 2000:  # ë¹„ì •ìƒ ê°’ í•„í„°
                old_rate = _get_fx_rate()
                save_setting("fx_rate_usd_krw", rate)
                app_state.last_fx_update = time.time()
                if abs(rate - old_rate) >= 1:
                    _log(f"[FX] í™˜ìœ¨ ê°±ì‹ : ${1} = â‚©{rate:,.2f} (ì´ì „: â‚©{old_rate:,.2f})")
                    save_activity_log("system", f"ğŸ’± í™˜ìœ¨ ê°±ì‹ : â‚©{rate:,.2f}/$ (ì´ì „ â‚©{old_rate:,.2f})", "info")
                return rate
    except ImportError:
        _log("[FX] yfinance ë¯¸ì„¤ì¹˜ â€” í™˜ìœ¨ ê°±ì‹  ë¶ˆê°€")
    except Exception as e:
        _log(f"[FX] í™˜ìœ¨ ê°±ì‹  ì‹¤íŒ¨: {e}")
    return None


def _get_fx_rate() -> float:
    """USD/KRW í™˜ìœ¨ ë°˜í™˜. DB ì„¤ì •ê°’ ìš°ì„ , ì—†ìœ¼ë©´ 1450 í´ë°±.

    ëª¨ë“  í™˜ìœ¨ ì°¸ì¡°ì—ì„œ ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (í•˜ë“œì½”ë”© ë°©ì§€).
    """
    try:
        rate = load_setting("fx_rate_usd_krw", 1450)
        if isinstance(rate, (int, float)) and 1000 < rate < 2000:
            return float(rate)
    except Exception as e:
        logger.debug("í™˜ìœ¨ ì¡°íšŒ ì‹¤íŒ¨: %s", e)
    return 1450.0


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARGOS â€” ìë™ ë°ì´í„° ìˆ˜ì§‘ ë ˆì´ì–´ (Phase 6-5)
# ì„œë²„ê°€ ì‹¬ë¶€ë¦„(ë°ì´í„° ìˆ˜ì§‘), AIëŠ” ìƒê°(íŒë‹¨)ë§Œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_ARGOS_LAST_PRICE     = 0.0    # ë§ˆì§€ë§‰ ì£¼ê°€ ìˆ˜ì§‘ ì‹œê°
_ARGOS_LAST_NEWS      = 0.0    # ë§ˆì§€ë§‰ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œê° (30ë¶„)
_ARGOS_LAST_DART      = 0.0    # ë§ˆì§€ë§‰ DART ìˆ˜ì§‘ ì‹œê° (1ì‹œê°„)
_ARGOS_LAST_MACRO     = 0.0    # ë§ˆì§€ë§‰ ë§¤í¬ë¡œ ìˆ˜ì§‘ ì‹œê° (1ì¼)
_ARGOS_LAST_FINANCIAL = 0.0    # ë§ˆì§€ë§‰ ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘ ì‹œê° (1ì¼)
_ARGOS_LAST_SECTOR    = 0.0    # ë§ˆì§€ë§‰ ì—…ì¢…ì§€ìˆ˜ ìˆ˜ì§‘ ì‹œê° (1ì¼)
_ARGOS_LAST_MONTHLY_RL = 0.0   # ë§ˆì§€ë§‰ ì›”ê°„ RL ë¶„ì„ ì‹œê°

_ARGOS_NEWS_INTERVAL      = 1800    # 30ë¶„
_ARGOS_DART_INTERVAL      = 3600    # 1ì‹œê°„
_ARGOS_MACRO_INTERVAL     = 86400   # 1ì¼
_ARGOS_FINANCIAL_INTERVAL = 86400   # 1ì¼
_ARGOS_SECTOR_INTERVAL    = 86400   # 1ì¼
_ARGOS_MONTHLY_INTERVAL   = 2592000 # 30ì¼

_argos_logger = logging.getLogger("corthex.argos")


def _argos_update_status(data_type: str, error: str = "", count_delta: int = 0) -> None:
    """ARGOS ìˆ˜ì§‘ ìƒíƒœë¥¼ DBì— ê¸°ë¡í•©ë‹ˆë‹¤."""
    try:
        conn = get_connection()
        now = datetime.now(KST).isoformat()
        conn.execute(
            """INSERT INTO argos_collection_status(data_type, last_collected, last_error, total_count, updated_at)
               VALUES(?, ?, ?, ?, ?)
               ON CONFLICT(data_type) DO UPDATE SET
                 last_collected = CASE WHEN excluded.last_error='' THEN excluded.last_collected ELSE last_collected END,
                 last_error = excluded.last_error,
                 total_count = total_count + excluded.total_count,
                 updated_at = excluded.updated_at""",
            (data_type, now if not error else "", error, count_delta, now)
        )
        conn.commit()
        conn.close()
    except Exception as e:
        _argos_logger.debug("ìƒíƒœ ê¸°ë¡ ì‹¤íŒ¨: %s", e)


_argos_price_running = False  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€ í”Œë˜ê·¸

async def _argos_collect_prices() -> int:
    """ê´€ì‹¬ì¢…ëª© ì£¼ê°€ë¥¼ pykrx/yfinanceë¡œ ìˆ˜ì§‘í•´ DBì— ëˆ„ì í•©ë‹ˆë‹¤ (90ì¼ ë³´ì¡´).
    íƒ€ì„ì•„ì›ƒ: ì¢…ëª©ë‹¹ 20ì´ˆ. ë™ì‹œ ì‹¤í–‰ ë°©ì§€ í”Œë˜ê·¸.
    Returns: ì €ì¥ëœ í–‰ ìˆ˜
    """
    global _argos_price_running
    if _argos_price_running:
        _argos_logger.debug("ARGOS ì£¼ê°€ ìˆ˜ì§‘ ì´ë¯¸ ì§„í–‰ ì¤‘ â€” ìŠ¤í‚µ")
        return 0

    _argos_price_running = True
    try:
        watchlist = _load_data("trading_watchlist", [])
        if not watchlist:
            return 0

        conn = get_connection()
        saved = 0
        now_str = datetime.now(KST).isoformat()
        today = datetime.now(KST).strftime("%Y%m%d")
        # ì²« ìˆ˜ì§‘ì€ 7ì¼ë§Œ (ë¹ ë¥´ê²Œ), DBì— ë°ì´í„° ìˆìœ¼ë©´ 3ì¼ë§Œ ë³´ì¶©
        try:
            existing = conn.execute("SELECT COUNT(*) FROM argos_price_history").fetchone()[0]
        except Exception:
            existing = 0
        fetch_days = 7 if existing == 0 else 3
        start = (datetime.now(KST) - timedelta(days=fetch_days)).strftime("%Y%m%d")

        kr_tickers = [w for w in watchlist if w.get("market", "KR") == "KR"]
        us_tickers = [w for w in watchlist if w.get("market") == "US"]
        PER_TICKER_TIMEOUT = 20  # ì´ˆ

        try:
            # â”€â”€ í•œêµ­ ì£¼ì‹ (pykrx) â”€â”€
            if kr_tickers:
                try:
                    from pykrx import stock as pykrx_stock
                    for w in kr_tickers:
                        ticker = w["ticker"]
                        try:
                            df = await asyncio.wait_for(
                                asyncio.to_thread(
                                    pykrx_stock.get_market_ohlcv_by_date, start, today, ticker
                                ),
                                timeout=PER_TICKER_TIMEOUT,
                            )
                            if df is None or df.empty:
                                _argos_logger.debug("PRICE KR %s: ë°ì´í„° ì—†ìŒ", ticker)
                                continue
                            ticker_saved = 0
                            for dt_idx, row in df.iterrows():
                                trade_date = str(dt_idx)[:10]
                                close = float(row.get("ì¢…ê°€", 0))
                                if close <= 0:
                                    continue
                                prev_rows = df[df.index < dt_idx]
                                prev_close = float(prev_rows.iloc[-1]["ì¢…ê°€"]) if not prev_rows.empty else close
                                change_pct = round((close - prev_close) / prev_close * 100, 2) if prev_close else 0
                                conn.execute(
                                    """INSERT OR IGNORE INTO argos_price_history
                                       (ticker, market, trade_date, open_price, high_price, low_price,
                                        close_price, volume, change_pct, collected_at)
                                       VALUES(?,?,?,?,?,?,?,?,?,?)""",
                                    (ticker, "KR", trade_date,
                                     float(row.get("ì‹œê°€", close)), float(row.get("ê³ ê°€", close)),
                                     float(row.get("ì €ê°€", close)), close,
                                     int(row.get("ê±°ë˜ëŸ‰", 0)), change_pct, now_str)
                                )
                                ticker_saved += 1
                            conn.commit()  # ì¢…ëª©ë³„ ì¦‰ì‹œ ì»¤ë°‹ â†’ DB ì ê¸ˆ ìµœì†Œí™”
                            saved += ticker_saved
                            _argos_logger.info("PRICE KR %s: %dí–‰ ì €ì¥ (%dì¼)", ticker, ticker_saved, fetch_days)
                        except asyncio.TimeoutError:
                            _argos_logger.warning("KR %s: %dì´ˆ íƒ€ì„ì•„ì›ƒ â€” ìŠ¤í‚µ", ticker, PER_TICKER_TIMEOUT)
                        except Exception as e:
                            _argos_logger.debug("KR ì£¼ê°€ íŒŒì‹± ì‹¤íŒ¨ (%s): %s", ticker, e)
                except ImportError:
                    _argos_logger.debug("pykrx ë¯¸ì„¤ì¹˜ â€” êµ­ë‚´ ì£¼ê°€ ìˆ˜ì§‘ ë¶ˆê°€")

            # â”€â”€ ë¯¸êµ­ ì£¼ì‹ (yfinance) â”€â”€
            if us_tickers:
                try:
                    import yfinance as yf
                    period = "7d" if existing == 0 else "3d"
                    for w in us_tickers:
                        ticker = w["ticker"]
                        try:
                            t_obj = yf.Ticker(ticker)
                            hist = await asyncio.wait_for(
                                asyncio.to_thread(lambda t=t_obj, p=period: t.history(period=p)),
                                timeout=PER_TICKER_TIMEOUT,
                            )
                            if hist is None or hist.empty:
                                _argos_logger.debug("PRICE US %s: ë°ì´í„° ì—†ìŒ", ticker)
                                continue
                            ticker_saved = 0
                            prev_close_val = None
                            for dt_idx, row in hist.iterrows():
                                trade_date = str(dt_idx)[:10]
                                close = round(float(row["Close"]), 4)
                                if close <= 0:
                                    continue
                                chg = round((close - prev_close_val) / prev_close_val * 100, 2) if prev_close_val else 0
                                conn.execute(
                                    """INSERT OR IGNORE INTO argos_price_history
                                       (ticker, market, trade_date, open_price, high_price, low_price,
                                        close_price, volume, change_pct, collected_at)
                                       VALUES(?,?,?,?,?,?,?,?,?,?)""",
                                    (ticker, "US", trade_date,
                                     round(float(row.get("Open", close)), 4),
                                     round(float(row.get("High", close)), 4),
                                     round(float(row.get("Low", close)), 4),
                                     close, int(row.get("Volume", 0)), chg, now_str)
                                )
                                ticker_saved += 1
                                prev_close_val = close
                            conn.commit()  # ì¢…ëª©ë³„ ì¦‰ì‹œ ì»¤ë°‹ â†’ DB ì ê¸ˆ ìµœì†Œí™”
                            saved += ticker_saved
                            _argos_logger.info("PRICE US %s: %dí–‰ ì €ì¥ (%s)", ticker, ticker_saved, period)
                        except asyncio.TimeoutError:
                            _argos_logger.warning("US %s: %dì´ˆ íƒ€ì„ì•„ì›ƒ â€” ìŠ¤í‚µ", ticker, PER_TICKER_TIMEOUT)
                        except Exception as e:
                            _argos_logger.debug("US ì£¼ê°€ íŒŒì‹± ì‹¤íŒ¨ (%s): %s", ticker, e)
                except ImportError:
                    _argos_logger.debug("yfinance ë¯¸ì„¤ì¹˜ â€” í•´ì™¸ ì£¼ê°€ ìˆ˜ì§‘ ë¶ˆê°€")

            conn.commit()

            # 90ì¼ ì´ˆê³¼ ë°ì´í„° ì •ë¦¬
            cutoff = (datetime.now(KST) - timedelta(days=90)).strftime("%Y-%m-%d")
            conn.execute("DELETE FROM argos_price_history WHERE trade_date < ?", (cutoff,))
            conn.commit()
        finally:
            conn.close()

        _argos_logger.info("ARGOS ì£¼ê°€ ìˆ˜ì§‘ ì™„ë£Œ: %dí–‰ (fetch_days=%d)", saved, fetch_days)
        return saved
    finally:
        _argos_price_running = False


async def _argos_collect_news() -> int:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¡œ ê´€ì‹¬ì¢…ëª© ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ DBì— ì €ì¥í•©ë‹ˆë‹¤ (30ì¼ ë³´ì¡´).
    Returns: ì €ì¥ëœ í–‰ ìˆ˜
    """
    naver_id = os.getenv("NAVER_CLIENT_ID", "")
    naver_secret = os.getenv("NAVER_CLIENT_SECRET", "")
    if not naver_id or not naver_secret:
        _argos_logger.debug("NAVER_CLIENT_ID/SECRET ë¯¸ì„¤ì • â€” ë‰´ìŠ¤ ìˆ˜ì§‘ ë¶ˆê°€")
        return 0

    watchlist = _load_data("trading_watchlist", [])
    if not watchlist:
        return 0

    import urllib.request
    import urllib.parse
    conn = get_connection()
    saved = 0
    now_str = datetime.now(KST).isoformat()

    try:
        for w in watchlist[:10]:  # ê³¼ë¶€í•˜ ë°©ì§€: ìµœëŒ€ 10ì¢…ëª©
            keyword = w.get("name") or w.get("ticker", "")
            if not keyword:
                continue
            try:
                encoded = urllib.parse.quote(keyword)
                url = f"https://openapi.naver.com/v1/search/news.json?query={encoded}&display=20&sort=date"
                req = urllib.request.Request(url, headers={
                    "X-Naver-Client-Id": naver_id,
                    "X-Naver-Client-Secret": naver_secret,
                })
                def _fetch(r=req):
                    with urllib.request.urlopen(r, timeout=5) as resp:
                        return json.loads(resp.read().decode("utf-8"))
                data = await asyncio.to_thread(_fetch)
                for item in data.get("items", []):
                    title = re.sub(r"<[^>]+>", "", item.get("title", ""))
                    desc = re.sub(r"<[^>]+>", "", item.get("description", ""))
                    pub_date = item.get("pubDate", now_str)
                    link = item.get("link", "")
                    conn.execute(
                        """INSERT OR IGNORE INTO argos_news_cache
                           (keyword, title, description, link, pub_date, source, collected_at)
                           VALUES(?,?,?,?,?,?,?)""",
                        (keyword, title, desc, link, pub_date, "naver", now_str)
                    )
                    saved += 1
                conn.commit()  # í‚¤ì›Œë“œë³„ ì¦‰ì‹œ ì»¤ë°‹ â†’ DB ì ê¸ˆ ìµœì†Œí™”
            except Exception as e:
                _argos_logger.debug("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨ (%s): %s", keyword, e)

        cutoff = (datetime.now(KST) - timedelta(days=30)).isoformat()
        conn.execute("DELETE FROM argos_news_cache WHERE pub_date < ?", (cutoff,))
        conn.commit()
    finally:
        conn.close()

    return saved


async def _argos_collect_dart() -> int:
    """DART ê³µì‹œë¥¼ ìˆ˜ì§‘í•´ DBì— ì €ì¥í•©ë‹ˆë‹¤ (90ì¼ ë³´ì¡´).
    Returns: ì €ì¥ëœ í–‰ ìˆ˜
    """
    dart_key = os.getenv("DART_API_KEY", "")
    if not dart_key:
        _argos_logger.debug("DART_API_KEY ë¯¸ì„¤ì • â€” DART ìˆ˜ì§‘ ë¶ˆê°€")
        return 0

    watchlist = _load_data("trading_watchlist", [])
    kr_tickers = [w for w in watchlist if w.get("market", "KR") == "KR"]
    if not kr_tickers:
        return 0

    import urllib.request
    import urllib.parse
    conn = get_connection()
    saved = 0
    now_str = datetime.now(KST).isoformat()
    bgn_de = (datetime.now(KST) - timedelta(days=90)).strftime("%Y%m%d")

    try:
        for w in kr_tickers[:10]:  # ê³¼ë¶€í•˜ ë°©ì§€
            ticker = w["ticker"]
            try:
                params = urllib.parse.urlencode({
                    "crtfc_key": dart_key,
                    "stock_code": ticker,
                    "bgn_de": bgn_de,
                    "sort": "date",
                    "sort_mth": "desc",
                    "page_count": 20,
                })
                url = f"https://opendart.fss.or.kr/api/list.json?{params}"
                def _fetch(u=url):
                    with urllib.request.urlopen(u, timeout=8) as resp:
                        return json.loads(resp.read().decode("utf-8"))
                data = await asyncio.to_thread(_fetch)
                for item in data.get("list", []):
                    conn.execute(
                        """INSERT OR IGNORE INTO argos_dart_filings
                           (ticker, corp_name, report_nm, rcept_no, flr_nm, rcept_dt, collected_at)
                           VALUES(?,?,?,?,?,?,?)""",
                        (ticker, item.get("corp_name",""), item.get("report_nm",""),
                         item.get("rcept_no",""), item.get("flr_nm",""),
                         item.get("rcept_dt",""), now_str)
                    )
                    saved += 1
                conn.commit()  # ì¢…ëª©ë³„ ì¦‰ì‹œ ì»¤ë°‹ â†’ DB ì ê¸ˆ ìµœì†Œí™”
            except Exception as e:
                _argos_logger.debug("DART ìˆ˜ì§‘ ì‹¤íŒ¨ (%s): %s", ticker, e)

        cutoff = (datetime.now(KST) - timedelta(days=90)).strftime("%Y%m%d")
        conn.execute("DELETE FROM argos_dart_filings WHERE rcept_dt < ?", (cutoff,))
        conn.commit()
    finally:
        conn.close()

    return saved


async def _argos_collect_macro() -> int:
    """KOSPI/KOSDAQ/í™˜ìœ¨ ë“± ë§¤í¬ë¡œ ì§€í‘œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    íƒ€ì„ì•„ì›ƒ: í•­ëª©ë‹¹ 15ì´ˆ.
    Returns: ì €ì¥ëœ í–‰ ìˆ˜
    """
    MACRO_TIMEOUT = 15  # ì´ˆ
    conn = get_connection()
    saved = 0
    now_str = datetime.now(KST).isoformat()
    today_iso = datetime.now(KST).strftime("%Y-%m-%d")

    try:
        # USD/KRW â€” yfinance
        try:
            import yfinance as yf
            def _fetch_fx():
                t = yf.Ticker("USDKRW=X")
                h = t.history(period="5d")
                return float(h.iloc[-1]["Close"]) if h is not None and not h.empty else None
            rate = await asyncio.wait_for(asyncio.to_thread(_fetch_fx), timeout=MACRO_TIMEOUT)
            if rate:
                conn.execute(
                    "INSERT OR IGNORE INTO argos_macro_data(indicator,trade_date,value,source,collected_at) VALUES(?,?,?,?,?)",
                    ("USD_KRW", today_iso, round(rate, 2), "yfinance", now_str)
                )
                saved += 1
                conn.commit()  # ì¦‰ì‹œ ì»¤ë°‹
                _argos_logger.info("MACRO USD/KRW: %.2f", rate)
        except asyncio.TimeoutError:
            _argos_logger.warning("USD/KRW: %dì´ˆ íƒ€ì„ì•„ì›ƒ", MACRO_TIMEOUT)
        except Exception as e:
            _argos_logger.debug("USD/KRW ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)

        # KOSPI / KOSDAQ â€” pykrx
        try:
            from pykrx import stock as pykrx_stock
            today = datetime.now(KST).strftime("%Y%m%d")
            start = (datetime.now(KST) - timedelta(days=7)).strftime("%Y%m%d")
            for ticker, label in [("1001", "KOSPI"), ("2001", "KOSDAQ")]:
                try:
                    df = await asyncio.wait_for(
                        asyncio.to_thread(
                            pykrx_stock.get_index_ohlcv_by_date, start, today, ticker
                        ),
                        timeout=MACRO_TIMEOUT,
                    )
                    if df is not None and not df.empty:
                        close = float(df.iloc[-1]["ì¢…ê°€"])
                        trade_date = str(df.index[-1])[:10]
                        conn.execute(
                            "INSERT OR IGNORE INTO argos_macro_data(indicator,trade_date,value,source,collected_at) VALUES(?,?,?,?,?)",
                            (label, trade_date, round(close, 2), "pykrx", now_str)
                        )
                        conn.commit()  # ì¦‰ì‹œ ì»¤ë°‹
                        saved += 1
                        _argos_logger.info("MACRO %s: %.2f", label, close)
                except asyncio.TimeoutError:
                    _argos_logger.warning("%s: %dì´ˆ íƒ€ì„ì•„ì›ƒ", label, MACRO_TIMEOUT)
                except Exception as e:
                    _argos_logger.debug("%s ìˆ˜ì§‘ ì‹¤íŒ¨: %s", label, e)
        except ImportError:
            pass

        # VIX â€” yfinance
        try:
            import yfinance as yf
            def _fetch_vix():
                t = yf.Ticker("^VIX")
                h = t.history(period="5d")
                return float(h.iloc[-1]["Close"]) if h is not None and not h.empty else None
            vix = await asyncio.wait_for(asyncio.to_thread(_fetch_vix), timeout=MACRO_TIMEOUT)
            if vix:
                conn.execute(
                    "INSERT OR IGNORE INTO argos_macro_data(indicator,trade_date,value,source,collected_at) VALUES(?,?,?,?,?)",
                    ("VIX", today_iso, round(vix, 2), "yfinance", now_str)
                )
                conn.commit()  # ì¦‰ì‹œ ì»¤ë°‹
                saved += 1
                _argos_logger.info("MACRO VIX: %.2f", vix)
        except asyncio.TimeoutError:
            _argos_logger.warning("VIX: %dì´ˆ íƒ€ì„ì•„ì›ƒ", MACRO_TIMEOUT)
        except Exception as e:
            _argos_logger.debug("VIX ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)


        # S&P500 / ë‚˜ìŠ¤ë‹¥ / ë¯¸êµ­ 10ë…„ êµ­ì±„ê¸ˆë¦¬ â€” yfinance
        for yf_ticker, label in [("^GSPC", "SP500"), ("^IXIC", "NASDAQ"), ("^TNX", "US10Y")]:
            try:
                import yfinance as yf
                def _fetch_yf(sym=yf_ticker):
                    t = yf.Ticker(sym)
                    h = t.history(period="5d")
                    return float(h.iloc[-1]["Close"]) if h is not None and not h.empty else None
                val = await asyncio.wait_for(asyncio.to_thread(_fetch_yf), timeout=MACRO_TIMEOUT)
                if val:
                    conn.execute(
                        "INSERT OR IGNORE INTO argos_macro_data(indicator,trade_date,value,source,collected_at) VALUES(?,?,?,?,?)",
                        (label, today_iso, round(val, 4), "yfinance", now_str)
                    )
                    conn.commit()
                    saved += 1
                    _argos_logger.info("MACRO %s: %.4f", label, val)
            except asyncio.TimeoutError:
                _argos_logger.warning("%s: %dì´ˆ íƒ€ì„ì•„ì›ƒ", label, MACRO_TIMEOUT)
            except Exception as e:
                _argos_logger.debug("%s ìˆ˜ì§‘ ì‹¤íŒ¨: %s", label, e)

        # í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬ â€” ECOS API
        try:
            ecos_key = os.getenv("ECOS_API_KEY", "")
            if ecos_key:
                import urllib.request
                ecos_url = (
                    f"https://ecos.bok.or.kr/api/StatisticSearch/{ecos_key}/json/kr"
                    f"/1/5/722Y001/M/{today_iso[:4]}{today_iso[5:7]}/{today_iso[:4]}{today_iso[5:7]}"
                )
                def _fetch_ecos(url=ecos_url):
                    with urllib.request.urlopen(url, timeout=10) as r:
                        import json as _json
                        return _json.loads(r.read().decode("utf-8"))
                ecos_data = await asyncio.wait_for(asyncio.to_thread(_fetch_ecos), timeout=MACRO_TIMEOUT)
                rows_ecos = ecos_data.get("StatisticSearch", {}).get("row", [])
                if rows_ecos:
                    rate = float(rows_ecos[-1].get("DATA_VALUE", 0))
                    period = rows_ecos[-1].get("TIME", today_iso[:7])
                    trade_date_ecos = f"{period[:4]}-{period[4:6]}-01"
                    conn.execute(
                        "INSERT OR IGNORE INTO argos_macro_data(indicator,trade_date,value,source,collected_at) VALUES(?,?,?,?,?)",
                        ("KR_RATE", trade_date_ecos, rate, "ecos", now_str)
                    )
                    conn.commit()
                    saved += 1
                    _argos_logger.info("MACRO KR_RATE: %.2f%%", rate)
        except asyncio.TimeoutError:
            _argos_logger.warning("KR_RATE: %dì´ˆ íƒ€ì„ì•„ì›ƒ", MACRO_TIMEOUT)
        except Exception as e:
            _argos_logger.debug("KR_RATE ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)

        cutoff = (datetime.now(KST) - timedelta(days=365)).strftime("%Y-%m-%d")
        conn.execute("DELETE FROM argos_macro_data WHERE trade_date < ?", (cutoff,))
        conn.commit()
    finally:
        conn.close()

    _argos_logger.info("ARGOS ë§¤í¬ë¡œ ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", saved)
    return saved


_argos_seq_lock = asyncio.Lock()  # ìˆœì°¨ ìˆ˜ì§‘ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (Lock ê¸°ë°˜)

async def _argos_sequential_collect(now_ts: float):
    """ARGOS ìˆ˜ì§‘ì„ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤ (DB lock ë°©ì§€).
    ë™ì‹œì— ì—¬ëŸ¬ ìˆ˜ì§‘ì´ DBë¥¼ ì¡ì§€ ì•Šë„ë¡ í•˜ë‚˜ì”© ìˆœì„œëŒ€ë¡œ.
    """
    global _ARGOS_LAST_NEWS, _ARGOS_LAST_DART, _ARGOS_LAST_MACRO, _ARGOS_LAST_FINANCIAL, _ARGOS_LAST_SECTOR
    if _argos_seq_lock.locked():
        return
    async with _argos_seq_lock:
        try:
            # 1) ì£¼ê°€ â€” ë§¤ ì‚¬ì´í´
            await _argos_collect_prices_safe()

            # 2) ë‰´ìŠ¤ â€” 30ë¶„ë§ˆë‹¤
            if now_ts - _ARGOS_LAST_NEWS > _ARGOS_NEWS_INTERVAL:
                _ARGOS_LAST_NEWS = now_ts
                await _argos_collect_news_safe()

            # 3) DART â€” 1ì‹œê°„ë§ˆë‹¤
            if now_ts - _ARGOS_LAST_DART > _ARGOS_DART_INTERVAL:
                _ARGOS_LAST_DART = now_ts
                await _argos_collect_dart_safe()

            # 4) ë§¤í¬ë¡œ â€” 1ì¼ë§ˆë‹¤ (S&P500/ë‚˜ìŠ¤ë‹¥/êµ­ì±„ê¸ˆë¦¬/ê¸°ì¤€ê¸ˆë¦¬ í¬í•¨)
            if now_ts - _ARGOS_LAST_MACRO > _ARGOS_MACRO_INTERVAL:
                _ARGOS_LAST_MACRO = now_ts
                await _argos_collect_macro_safe()

            # 5) ì¬ë¬´ì§€í‘œ â€” 1ì¼ë§ˆë‹¤ (PER/PBR/EPS/BPS)
            if now_ts - _ARGOS_LAST_FINANCIAL > _ARGOS_FINANCIAL_INTERVAL:
                _ARGOS_LAST_FINANCIAL = now_ts
                await _argos_collect_financial_safe()

            # 6) ì—…ì¢…ì§€ìˆ˜ â€” 1ì¼ë§ˆë‹¤ (ì „ê¸°ì „ì/í™”í•™/ê¸ˆìœµ ë“± 11ê°œ)
            if now_ts - _ARGOS_LAST_SECTOR > _ARGOS_SECTOR_INTERVAL:
                _ARGOS_LAST_SECTOR = now_ts
                await _argos_collect_sector_safe()
        except Exception as e:
            _argos_logger.error("ARGOS ìˆœì°¨ ìˆ˜ì§‘ ì˜¤ë¥˜: %s", e)


async def _argos_collect_prices_safe():
    """ì£¼ê°€ ìˆ˜ì§‘ â€” ì˜ˆì™¸ ì•ˆì „ ë˜í¼. ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ."""
    try:
        n = await asyncio.wait_for(_argos_collect_prices(), timeout=180)
        if n > 0:
            _argos_update_status("price", count_delta=n)
    except asyncio.TimeoutError:
        _argos_update_status("price", error="ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ")
        _argos_logger.error("ARGOS ì£¼ê°€ ìˆ˜ì§‘: ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        _argos_update_status("price", error=str(e)[:200])
        _argos_logger.error("ARGOS ì£¼ê°€ ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)


async def _argos_collect_news_safe():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ â€” ì˜ˆì™¸ ì•ˆì „ ë˜í¼. ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ."""
    try:
        n = await asyncio.wait_for(_argos_collect_news(), timeout=120)
        _argos_update_status("news", count_delta=n)
        _argos_logger.info("ARGOS ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", n)
    except asyncio.TimeoutError:
        _argos_update_status("news", error="ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ")
        _argos_logger.error("ARGOS ë‰´ìŠ¤ ìˆ˜ì§‘: ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        _argos_update_status("news", error=str(e)[:200])
        _argos_logger.error("ARGOS ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)


async def _argos_collect_dart_safe():
    """DART ìˆ˜ì§‘ â€” ì˜ˆì™¸ ì•ˆì „ ë˜í¼. ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ."""
    try:
        n = await asyncio.wait_for(_argos_collect_dart(), timeout=120)
        _argos_update_status("dart", count_delta=n)
        _argos_logger.info("ARGOS DART ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", n)
    except asyncio.TimeoutError:
        _argos_update_status("dart", error="ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ")
        _argos_logger.error("ARGOS DART ìˆ˜ì§‘: ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        _argos_update_status("dart", error=str(e)[:200])
        _argos_logger.error("ARGOS DART ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)


async def _argos_collect_macro_safe():
    """ë§¤í¬ë¡œ ìˆ˜ì§‘ â€” ì˜ˆì™¸ ì•ˆì „ ë˜í¼. ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ."""
    try:
        n = await asyncio.wait_for(_argos_collect_macro(), timeout=120)
        _argos_update_status("macro", count_delta=n)
        _argos_logger.info("ARGOS ë§¤í¬ë¡œ ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", n)
    except asyncio.TimeoutError:
        _argos_update_status("macro", error="ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ")
        _argos_logger.error("ARGOS ë§¤í¬ë¡œ ìˆ˜ì§‘: ì „ì²´ 2ë¶„ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        _argos_update_status("macro", error=str(e)[:200])
        _argos_logger.error("ARGOS ë§¤í¬ë¡œ ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)


async def _argos_collect_financial() -> int:
    """pykrxë¡œ ê´€ì‹¬ì¢…ëª© ì¬ë¬´ì§€í‘œ(PER/PBR/EPS ë“±)ë¥¼ ìˆ˜ì§‘í•´ DBì— ì €ì¥ (1ì¼ 1íšŒ).
    Returns: ì €ì¥ëœ í–‰ ìˆ˜
    """
    conn = get_connection()
    saved = 0
    now_str = datetime.now(KST).isoformat()
    today = datetime.now(KST).strftime("%Y%m%d")
    today_iso = datetime.now(KST).strftime("%Y-%m-%d")

    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS argos_financial_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                ticker TEXT NOT NULL,
                trade_date TEXT NOT NULL,
                per REAL, pbr REAL, eps REAL, dps REAL, bps REAL,
                source TEXT DEFAULT 'pykrx',
                collected_at TEXT,
                UNIQUE(ticker, trade_date)
            )
        """)
        conn.commit()

        from pykrx import stock as pykrx_stock
        watchlist = _load_data("trading_watchlist", [])
        kr_tickers = [w for w in watchlist if w.get("market", "KR") == "KR"]
        if not kr_tickers:
            return 0

        for w in kr_tickers:
            ticker = w["ticker"]
            try:
                df = await asyncio.wait_for(
                    asyncio.to_thread(pykrx_stock.get_market_fundamental, today, ticker=ticker),
                    timeout=20,
                )
                if df is not None and not df.empty:
                    row = df.iloc[-1]
                    conn.execute(
                        """INSERT OR IGNORE INTO argos_financial_data
                           (ticker, trade_date, per, pbr, eps, dps, bps, source, collected_at)
                           VALUES(?,?,?,?,?,?,?,?,?)""",
                        (ticker, today_iso,
                         float(row.get("PER", 0) or 0),
                         float(row.get("PBR", 0) or 0),
                         float(row.get("EPS", 0) or 0),
                         float(row.get("DPS", 0) or 0),
                         float(row.get("BPS", 0) or 0),
                         "pykrx", now_str)
                    )
                    conn.commit()
                    saved += 1
                    _argos_logger.info("FINANCIAL %s: PER=%.1f PBR=%.2f", ticker,
                                       row.get("PER", 0), row.get("PBR", 0))
            except asyncio.TimeoutError:
                _argos_logger.warning("FINANCIAL %s: 20ì´ˆ íƒ€ì„ì•„ì›ƒ", ticker)
            except Exception as e:
                _argos_logger.debug("FINANCIAL %s ì‹¤íŒ¨: %s", ticker, e)

        cutoff = (datetime.now(KST) - timedelta(days=90)).strftime("%Y-%m-%d")
        conn.execute("DELETE FROM argos_financial_data WHERE trade_date < ?", (cutoff,))
        conn.commit()
    except ImportError:
        _argos_logger.debug("pykrx ë¯¸ì„¤ì¹˜ â€” ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘ ë¶ˆê°€")
    finally:
        conn.close()

    _argos_logger.info("ARGOS ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", saved)
    return saved


async def _argos_collect_financial_safe():
    """ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘ â€” ì˜ˆì™¸ ì•ˆì „ ë˜í¼. ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ."""
    try:
        n = await asyncio.wait_for(_argos_collect_financial(), timeout=180)
        _argos_update_status("financial", count_delta=n)
        _argos_logger.info("ARGOS ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", n)
    except asyncio.TimeoutError:
        _argos_update_status("financial", error="ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ")
        _argos_logger.error("ARGOS ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘: ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        _argos_update_status("financial", error=str(e)[:200])
        _argos_logger.error("ARGOS ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)


async def _argos_collect_sector() -> int:
    """pykrxë¡œ ì£¼ìš” ì—…ì¢…ì§€ìˆ˜ë¥¼ ìˆ˜ì§‘í•´ DBì— ì €ì¥ (1ì¼ 1íšŒ).
    Returns: ì €ì¥ëœ í–‰ ìˆ˜
    """
    SECTOR_CODES = [
        ("1028", "ì „ê¸°ì „ì"), ("1003", "í™”í•™"), ("1004", "ì˜ì•½í’ˆ"),
        ("1006", "ì² ê°•ê¸ˆì†"), ("1008", "ê¸°ê³„"), ("1022", "ìœ í†µì—…"),
        ("1024", "ê±´ì„¤ì—…"), ("1027", "í†µì‹ ì—…"), ("1029", "ê¸ˆìœµì—…"),
        ("1032", "ì„œë¹„ìŠ¤ì—…"), ("1005", "ë¹„ê¸ˆì†ê´‘ë¬¼"),
    ]
    conn = get_connection()
    saved = 0
    now_str = datetime.now(KST).isoformat()

    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS argos_sector_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                sector_name TEXT NOT NULL,
                trade_date TEXT NOT NULL,
                close_val REAL,
                change_pct REAL,
                source TEXT DEFAULT 'pykrx',
                collected_at TEXT,
                UNIQUE(sector_name, trade_date)
            )
        """)
        conn.commit()

        from pykrx import stock as pykrx_stock
        today = datetime.now(KST).strftime("%Y%m%d")
        start = (datetime.now(KST) - timedelta(days=7)).strftime("%Y%m%d")

        for code, name in SECTOR_CODES:
            try:
                df = await asyncio.wait_for(
                    asyncio.to_thread(pykrx_stock.get_index_ohlcv_by_date, start, today, code),
                    timeout=15,
                )
                if df is not None and not df.empty:
                    close = float(df.iloc[-1]["ì¢…ê°€"])
                    trade_date = str(df.index[-1])[:10]
                    # ì „ì¼ ëŒ€ë¹„ ë“±ë½ë¥ 
                    change_pct = 0.0
                    if len(df) >= 2:
                        prev = float(df.iloc[-2]["ì¢…ê°€"])
                        change_pct = (close - prev) / prev * 100 if prev != 0 else 0.0
                    conn.execute(
                        """INSERT OR IGNORE INTO argos_sector_data
                           (sector_name, trade_date, close_val, change_pct, source, collected_at)
                           VALUES(?,?,?,?,?,?)""",
                        (name, trade_date, round(close, 2), round(change_pct, 2), "pykrx", now_str)
                    )
                    conn.commit()
                    saved += 1
                    _argos_logger.info("SECTOR %s: %.2f (%+.2f%%)", name, close, change_pct)
            except asyncio.TimeoutError:
                _argos_logger.warning("SECTOR %s: 15ì´ˆ íƒ€ì„ì•„ì›ƒ", name)
            except Exception as e:
                _argos_logger.debug("SECTOR %s ì‹¤íŒ¨: %s", name, e)

        cutoff = (datetime.now(KST) - timedelta(days=90)).strftime("%Y-%m-%d")
        conn.execute("DELETE FROM argos_sector_data WHERE trade_date < ?", (cutoff,))
        conn.commit()
    except ImportError:
        _argos_logger.debug("pykrx ë¯¸ì„¤ì¹˜ â€” ì—…ì¢…ì§€ìˆ˜ ìˆ˜ì§‘ ë¶ˆê°€")
    finally:
        conn.close()

    _argos_logger.info("ARGOS ì—…ì¢…ì§€ìˆ˜ ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", saved)
    return saved


async def _argos_collect_sector_safe():
    """ì—…ì¢…ì§€ìˆ˜ ìˆ˜ì§‘ â€” ì˜ˆì™¸ ì•ˆì „ ë˜í¼. ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ."""
    try:
        n = await asyncio.wait_for(_argos_collect_sector(), timeout=180)
        _argos_update_status("sector", count_delta=n)
        _argos_logger.info("ARGOS ì—…ì¢…ì§€ìˆ˜ ìˆ˜ì§‘ ì™„ë£Œ: %dê±´", n)
    except asyncio.TimeoutError:
        _argos_update_status("sector", error="ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ")
        _argos_logger.error("ARGOS ì—…ì¢…ì§€ìˆ˜ ìˆ˜ì§‘: ì „ì²´ 3ë¶„ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        _argos_update_status("sector", error=str(e)[:200])
        _argos_logger.error("ARGOS ì—…ì¢…ì§€ìˆ˜ ìˆ˜ì§‘ ì‹¤íŒ¨: %s", e)


async def _argos_monthly_rl_analysis():
    """ì›” 1íšŒ: AIì—ê²Œ ìµœê·¼ ì˜¤ë‹µ íŒ¨í„´ ë¶„ì„ ìš”ì²­ â†’ error_patterns í…Œì´ë¸” ì—…ë°ì´íŠ¸.
    Phase 6-9 ê°•í™”í•™ìŠµ íŒŒì´í”„ë¼ì¸.
    """
    _argos_logger.info("ğŸ“Š ì›”ê°„ ê°•í™”í•™ìŠµ íŒ¨í„´ ë¶„ì„ ì‹œì‘")
    save_activity_log("system", "ğŸ“Š ì›”ê°„ ê°•í™”í•™ìŠµ íŒ¨í„´ ë¶„ì„ ì‹œì‘ (í¬ë¡ )", "info")
    try:
        conn = get_connection()
        # ìµœê·¼ 30ì¼ ë‚´ í‹€ë¦° ì˜ˆì¸¡ ì§‘ê³„
        rows = conn.execute(
            """SELECT ticker, direction, confidence, return_pct_7d, analyzed_at
               FROM cio_predictions
               WHERE correct_7d = 0
                 AND analyzed_at >= datetime('now', '-30 days')
               ORDER BY analyzed_at DESC
               LIMIT 30"""
        ).fetchall()
        conn.close()

        if not rows:
            _argos_logger.info("ìµœê·¼ 30ì¼ ì˜¤ë‹µ ì—†ìŒ â€” íŒ¨í„´ ë¶„ì„ ìŠ¤í‚µ")
            return

        wrong_list = [
            f"- {r[0]} ({r[1]}, ì‹ ë¢°ë„ {r[2]}%) â†’ ì‹¤ì œìˆ˜ìµ {r[3]}% ({r[4][:10]})"
            for r in rows
        ]
        prompt = (
            "ë‹¤ìŒì€ ìµœê·¼ 30ì¼ê°„ í‹€ë¦° ë§¤ë§¤ ì˜ˆì¸¡ ëª©ë¡ì…ë‹ˆë‹¤:\n"
            + "\n".join(wrong_list)
            + "\n\nê³µí†µ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: "
            "â‘  ì–´ë–¤ ì¢…ëª©/ë°©í–¥ì—ì„œ ë§ì´ í‹€ë ¸ë‚˜? "
            "â‘¡ ë†’ì€ ì‹ ë¢°ë„ì¸ë° í‹€ë¦° ì¼€ì´ìŠ¤ ì›ì¸? "
            "â‘¢ ë‹¤ìŒ ë¶„ì„ ì‹œ ì£¼ì˜ì‚¬í•­ 3ê°€ì§€ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."
        )

        from ai_handler import ask_ai
        result = await ask_ai(
            agent_id="secretary",
            messages=[{"role": "user", "content": prompt}],
            model=None,  # config/models.yamlì—ì„œ ìë™ ì„ íƒ
            task_id=f"rl_monthly_{datetime.now(KST).strftime('%Y%m')}",
        )

        analysis_text = result.get("content", "")
        if analysis_text:
            conn = get_connection()
            conn.execute(
                """INSERT INTO error_patterns
                   (pattern_type, description, ticker_filter, direction_filter,
                    confidence_threshold, active, created_at, updated_at)
                   VALUES(?,?,?,?,?,?,?,?)""",
                ("monthly_rl", analysis_text[:2000], "", "", 0.0, 1,
                 datetime.now(KST).isoformat(), datetime.now(KST).isoformat())
            )
            conn.commit()
            conn.close()
            save_activity_log("system", f"ğŸ“Š ì›”ê°„ RL íŒ¨í„´ ë¶„ì„ ì™„ë£Œ ({len(rows)}ê±´ ë¶„ì„)", "success")
            _argos_logger.info("ì›”ê°„ RL íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: %dê±´", len(rows))
    except Exception as e:
        _argos_logger.error("ì›”ê°„ RL íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: %s", e)


async def _cron_loop():
    """1ë¶„ë§ˆë‹¤ ì˜ˆì•½ëœ ì‘ì—…ì„ í™•ì¸í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤."""
    logger = logging.getLogger("corthex.cron")
    logger.info("í¬ë¡  ì‹¤í–‰ ì—”ì§„ ì‹œì‘")

    # ì„œë²„ ì‹œì‘ ì‹œ í™˜ìœ¨ ì¦‰ì‹œ ê°±ì‹ 
    await _update_fx_rate()

    while True:
        try:
            await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬

            # í™˜ìœ¨ ì£¼ê¸°ì  ê°±ì‹  (1ì‹œê°„ë§ˆë‹¤)
            if time.time() - app_state.last_fx_update > _FX_UPDATE_INTERVAL:
                asyncio.create_task(_update_fx_rate())

            # Soul ìë™ ì§„í™”: ë§¤ì£¼ ì¼ìš”ì¼ 03:00 KST
            _now_cron = datetime.now(KST)
            if _now_cron.weekday() == 6 and _now_cron.hour == 3 and _now_cron.minute == 0:
                logger.info("ğŸ§¬ ì£¼ê°„ Soul ì§„í™” í¬ë¡  ì‹¤í–‰")
                save_activity_log("system", "ğŸ§¬ ì£¼ê°„ Soul ì§„í™” ë¶„ì„ ì‹œì‘ (í¬ë¡ )", "info")
                from handlers.soul_evolution_handler import run_soul_evolution_analysis
                asyncio.create_task(run_soul_evolution_analysis())

            # Soul Gym 24/7 ìƒì‹œ ì§„í™” â€” _soul_gym_loop()ë¡œ ì´ê´€ (ì„œë²„ ì‹œì‘ ì‹œ ìë™ ì‹¤í–‰)

            # â”€â”€ ARGOS: ìë™ ë°ì´í„° ìˆ˜ì§‘ ë ˆì´ì–´ (Phase 6-5) â”€â”€
            # DB lock ë°©ì§€: ìˆœì°¨ ì‹¤í–‰ (í•˜ë‚˜ ëë‚˜ë©´ ë‹¤ìŒ ì‹¤í–‰)
            _now_ts = time.time()
            global _ARGOS_LAST_PRICE, _ARGOS_LAST_NEWS, _ARGOS_LAST_DART, _ARGOS_LAST_MACRO, _ARGOS_LAST_MONTHLY_RL
            asyncio.create_task(_argos_sequential_collect(_now_ts))

            # ì›”ê°„ ê°•í™”í•™ìŠµ íŒ¨í„´ ë¶„ì„ (Phase 6-9)
            if _now_ts - _ARGOS_LAST_MONTHLY_RL > _ARGOS_MONTHLY_INTERVAL:
                _ARGOS_LAST_MONTHLY_RL = _now_ts
                asyncio.create_task(_argos_monthly_rl_analysis())

            schedules = _load_data("schedules", [])
            now = datetime.now(KST)

            for schedule in schedules:
                if _should_run_schedule(schedule, now):
                    command = schedule.get("command", "")
                    if not command:
                        continue

                    logger.info("í¬ë¡  ì‹¤í–‰: %s â€” %s", schedule.get("name", ""), command)
                    save_activity_log("system", f"â° ì˜ˆì•½ ì‹¤í–‰: {schedule.get('name', '')} â€” {command[:50]}", "info")

                    # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
                    schedule["last_run"] = now.strftime("%Y-%m-%d %H:%M")
                    schedule["last_run_ts"] = now.timestamp()
                    _save_data("schedules", schedules)

                    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª…ë ¹ ì‹¤í–‰
                    asyncio.create_task(_run_scheduled_command(command, schedule.get("name", "")))

            # ê°€ê²© íŠ¸ë¦¬ê±° ì²´í¬ (1ë¶„ë§ˆë‹¤ â€” ì†ì ˆ/ìµì ˆ/ëª©í‘œë§¤ìˆ˜ ìë™ ì‹¤í–‰)
            asyncio.create_task(_check_price_triggers())

        except Exception as e:
            logger.error("í¬ë¡  ë£¨í”„ ì—ëŸ¬: %s", e)


def _register_default_schedules():
    """ì„œë²„ ì‹œì‘ ì‹œ ê¸°ë³¸ ìŠ¤ì¼€ì¤„ì´ ì—†ìœ¼ë©´ ìë™ ë“±ë¡í•©ë‹ˆë‹¤.
    ëŒ€í‘œë‹˜ì´ ì‚­ì œí•œ í¬ë¡ ì€ deleted_schedulesì— ê¸°ë¡ â†’ ì„œë²„ ì¬ì‹œì‘ ì‹œ ë³µì›í•˜ì§€ ì•ŠìŒ.
    """
    schedules = _load_data("schedules", [])
    deleted_ids: set = set(_load_data("deleted_schedules", []))  # ëŒ€í‘œë‹˜ì´ ì‚­ì œí•œ ê¸°ë³¸ í¬ë¡  ID ëª©ë¡
    existing_ids = {s.get("id") for s in schedules}

    # ë§ˆì´ê·¸ë ˆì´ì…˜: ê¸°ì¡´ CSO ì£¼ì‹ë¶„ì„ í¬ë¡  â†’ CIOë¡œ êµì²´ (ì£¼ì‹ë¶„ì„ì€ CIO ì—…ë¬´)
    _old_ids = {"default_cso_morning", "default_cso_weekly"}
    before_count = len(schedules)
    schedules = [s for s in schedules if s.get("id") not in _old_ids]
    _migrated = before_count - len(schedules)
    if _migrated:
        existing_ids = {s.get("id") for s in schedules}
        deleted_ids -= _old_ids  # ê¸°ì¡´ CSO ì‚­ì œ ê¸°ë¡ ì œê±° (ìƒˆ CIO IDë¡œ ëŒ€ì²´)
        _log(f"[CRON] ê¸°ì¡´ CSO ì£¼ì‹ë¶„ì„ í¬ë¡  {_migrated}ê°œ ì œê±° â†’ CIOë¡œ êµì²´ ì˜ˆì •")

    defaults = [
        {
            "id": "default_cio_morning",
            "name": "CIO ì¼ì¼ ì‹œì¥ ë¶„ì„",
            "command": "@ê¸ˆìœµë¶„ì„íŒ€ì¥ ì˜¤ëŠ˜ í•œêµ­ ì£¼ì‹ì‹œì¥ ì£¼ìš” ë™í–¥ê³¼ ì„¹í„°ë³„ ë¶„ì„ì„ ë³´ê³ í•´ì£¼ì„¸ìš”. ì£¼ìš” ì´ìŠˆì™€ íˆ¬ì ê´€ì  í¬í•¨.",
            "cron": "30 8 * * 1-5",  # í‰ì¼ 08:30
            "enabled": True,
        },
        {
            "id": "default_cio_weekly",
            "name": "CIO ì£¼ê°„ ì‹œì¥ ë¦¬ë·°",
            "command": "@ê¸ˆìœµë¶„ì„íŒ€ì¥ ì´ë²ˆ ì£¼ ì‹œì¥ ì´í‰ê³¼ ë‹¤ìŒ ì£¼ ì „ë§ì„ ì¢…í•© ë³´ê³ ì„œë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            "cron": "0 18 * * 5",  # ê¸ˆìš”ì¼ 18:00
            "enabled": True,
        },
    ]

    added = 0
    for d in defaults:
        # ëŒ€í‘œë‹˜ì´ ì‚­ì œí•œ ê¸°ë³¸ í¬ë¡ ì€ ë‹¤ì‹œ ë“±ë¡í•˜ì§€ ì•ŠìŒ
        if d["id"] in deleted_ids:
            continue
        if d["id"] not in existing_ids:
            d["last_run"] = ""
            d["last_run_ts"] = 0
            d["created_at"] = datetime.now(KST).strftime("%Y-%m-%d %H:%M")
            schedules.append(d)
            added += 1

    if added or _migrated:
        _save_data("schedules", schedules)
        _log(f"[CRON] ê¸°ë³¸ ìŠ¤ì¼€ì¤„ {added}ê°œ ë“±ë¡, {_migrated}ê°œ ë§ˆì´ê·¸ë ˆì´ì…˜ âœ…")


async def _run_scheduled_command(command: str, schedule_name: str):
    """ì˜ˆì•½ëœ ëª…ë ¹ì„ ì‹¤í–‰í•˜ê³ , ê²°ê³¼ë¥¼ í…”ë ˆê·¸ë¨ CEOì—ê²Œ ë°œì†¡í•©ë‹ˆë‹¤."""
    try:
        # @ë©˜ì…˜ íŒŒì‹± â€” í…”ë ˆê·¸ë¨ê³¼ ë™ì¼ ë¡œì§ (í¬ë¡  ëª…ë ¹ì—ì„œë„ target_agent_id ì§€ì •)
        target_agent_id = None
        actual_command = command
        stripped = command.strip()
        if stripped.startswith("@"):
            parts = stripped.split(None, 1)
            if len(parts) >= 2:
                mention = parts[0][1:]
                mention_lower = mention.lower()
                for a in AGENTS:
                    aid = a.get("agent_id", "").lower()
                    aname = a.get("name_ko", "")
                    tcode = a.get("telegram_code", "").lstrip("@")
                    if (aid == mention_lower or aid.startswith(mention_lower)
                            or mention_lower in aname.lower() or mention == tcode):
                        target_agent_id = a["agent_id"]
                        actual_command = parts[1]  # @ë©˜ì…˜ ì œê±°
                        break
                if not target_agent_id:
                    logger.warning("[CRON] @ë©˜ì…˜ '%s' ë§¤ì¹­ ì‹¤íŒ¨, ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…ìœ¼ë¡œ ì§„í–‰", mention)

        task = create_task(actual_command, source="cron")
        result = await _process_ai_command(actual_command, task["task_id"], target_agent_id=target_agent_id)
        # R-3: ì „ë ¥ë¶„ì„ ë°ì´í„°ìš© agent_id ê¸°ë¡
        update_task(task["task_id"], agent_id=result.get("agent_id", target_agent_id or "chief_of_staff"))
        save_activity_log("system", f"âœ… ì˜ˆì•½ ì™„ë£Œ: {schedule_name}", "info")

        # í¬ë¡  ê²°ê³¼ë¥¼ í…”ë ˆê·¸ë¨ CEOì—ê²Œ ë°œì†¡
        content = result.get("content", "")
        if content and app_state.telegram_app:
            ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
            if ceo_id:
                try:
                    who = result.get("handled_by", "ì‹œìŠ¤í…œ")
                    msg = f"â° [{schedule_name}]\n\n{content}"
                    if len(msg) > 3900:
                        msg = msg[:3900] + "\n\n... (ì „ì²´ëŠ” ì›¹ì—ì„œ í™•ì¸)"
                    await app_state.telegram_app.bot.send_message(chat_id=int(ceo_id), text=msg)
                except Exception as tg_err:
                    logger.warning("í¬ë¡  ê²°ê³¼ í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨: %s", tg_err)
    except Exception as e:
        save_activity_log("system", f"âŒ ì˜ˆì•½ ì‹¤íŒ¨: {schedule_name} â€” {str(e)[:100]}", "error")




# â”€â”€ ë¦¬í”Œë ˆì´ API â†’ handlers/replay_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.replay_handler import router as replay_router
app.include_router(replay_router)


# â”€â”€ Google Calendar OAuth â†’ handlers/calendar_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.calendar_handler import router as calendar_router
app.include_router(calendar_router)


# â”€â”€ ì˜ˆì•½(Schedule) Â· ì›Œí¬í”Œë¡œìš°(Workflow) CRUD â†’ handlers/schedule_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.schedule_handler import router as schedule_router
app.include_router(schedule_router)


# â”€â”€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (AI ì˜ì¡´ â€” arm_server.pyì— ìœ ì§€) â”€â”€

@app.post("/api/workflows/{wf_id}/run")
async def run_workflow(wf_id: str):
    """ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤ â€” ìŠ¤í…ì„ ìˆœì„œëŒ€ë¡œ AIë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    workflows = _load_data("workflows", [])
    wf = None
    for w in workflows:
        if w.get("id") == wf_id:
            wf = w
            break
    if not wf:
        return {"success": False, "error": "ì›Œí¬í”Œë¡œìš°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

    steps = wf.get("steps", [])
    if not steps:
        return {"success": False, "error": "ì›Œí¬í”Œë¡œìš°ì— ì‹¤í–‰í•  ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤"}

    if not is_ai_ready():
        return {"success": False, "error": "AIê°€ ì—°ê²°ë˜ì§€ ì•Šì•„ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆœì°¨ ì‹¤í–‰
    asyncio.create_task(_run_workflow_steps(wf_id, wf.get("name", ""), steps))
    return {"success": True, "message": f"ì›Œí¬í”Œë¡œìš° '{wf.get('name', '')}' ì‹¤í–‰ì„ ì‹œì‘í•©ë‹ˆë‹¤ ({len(steps)}ë‹¨ê³„)"}


async def _run_workflow_steps(wf_id: str, wf_name: str, steps: list):
    """ì›Œí¬í”Œë¡œìš° ìŠ¤í…ì„ ìˆœì°¨ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    save_activity_log("system", f"ğŸ”„ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {wf_name} ({len(steps)}ë‹¨ê³„)", "info")
    results = []
    prev_result = ""

    for i, step in enumerate(steps):
        step_name = step.get("name", f"ë‹¨ê³„ {i+1}")
        command = step.get("command", "")
        if not command:
            continue

        # ì´ì „ ë‹¨ê³„ ê²°ê³¼ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ëª…ë ¹ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        if prev_result and i > 0:
            command = f"[ì´ì „ ë‹¨ê³„ ê²°ê³¼ ì°¸ê³ : {prev_result[:500]}]\n\n{command}"

        save_activity_log("system", f"â–¶ {wf_name} â€” {step_name} ì‹¤í–‰ ì¤‘", "info")
        # ì›¹ì†Œì¼“ìœ¼ë¡œ ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
        await _broadcast_workflow_progress(i, len(steps), "running", step_name, "", workflow_id=wf_id)

        try:
            task = create_task(command, source="workflow")
            result = await _process_ai_command(command, task["task_id"])
            # R-3: ì „ë ¥ë¶„ì„ ë°ì´í„°ìš© agent_id ê¸°ë¡
            wf_agent = result.get("agent_id", "chief_of_staff") if isinstance(result, dict) else "chief_of_staff"
            update_task(task["task_id"], agent_id=wf_agent)
            content = result.get("content", "") if isinstance(result, dict) else str(result)
            prev_result = content[:500]
            results.append({"step": step_name, "status": "completed", "result": content[:200]})
            save_activity_log("system", f"âœ… {wf_name} â€” {step_name} ì™„ë£Œ", "info")
            # ì›¹ì†Œì¼“ìœ¼ë¡œ ë‹¨ê³„ ì™„ë£Œ ì•Œë¦¼
            await _broadcast_workflow_progress(i, len(steps), "completed", step_name, content[:300], workflow_id=wf_id)
        except Exception as e:
            results.append({"step": step_name, "status": "failed", "error": str(e)[:200]})
            save_activity_log("system", f"âŒ {wf_name} â€” {step_name} ì‹¤íŒ¨: {str(e)[:100]}", "error")
            await _broadcast_workflow_progress(i, len(steps), "failed", step_name, str(e)[:200], workflow_id=wf_id)
            break  # ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨

    # ì „ì²´ ì™„ë£Œ ì•Œë¦¼
    final_result = "\n\n".join([f"**{r['step']}**: {r.get('result', r.get('error', ''))}" for r in results])
    await _broadcast_workflow_progress(-1, len(steps), "done", "", final_result, workflow_done=True, workflow_id=wf_id)
    save_activity_log("system", f"ğŸ ì›Œí¬í”Œë¡œìš° ì™„ë£Œ: {wf_name} â€” {len(results)}/{len(steps)} ë‹¨ê³„ ì²˜ë¦¬", "info")


async def _broadcast_workflow_progress(step_index: int, total_steps: int, status: str,
                                        step_name: str, result: str, workflow_done: bool = False,
                                        workflow_id: str = ""):
    """ì›Œí¬í”Œë¡œìš° ì§„í–‰ ìƒíƒœë¥¼ ì›¹ì†Œì¼“ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    msg = {
        "event": "workflow_progress",
        "data": {
            "workflow_id": workflow_id,
            "step_index": step_index,
            "total_steps": total_steps,
            "status": status,
            "step_name": step_name,
            "result": result,
            "workflow_done": workflow_done,
            "final_result": result if workflow_done else "",
        },
    }
    await wm.broadcast(msg["event"], msg["data"])


# â”€â”€ ì½˜í…ì¸  íŒŒì´í”„ë¼ì¸ â€” ì œê±°ë¨ (2026-02-21, CEO ì§€ì‹œ) â”€â”€

# â”€â”€ ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ (KIS í•œêµ­íˆ¬ìì¦ê¶Œ í”„ë ˆì„ì›Œí¬) â”€â”€

# app_state.trading_bot_active, app_state.trading_bot_task â†’ app_state ì§ì ‘ ì‚¬ìš©

# â”€â”€ ì‹œì„¸ ìºì‹œ â†’ app_state ì‚¬ìš© â”€â”€
_price_cache = app_state.price_cache
_price_cache_lock = app_state.price_cache_lock


async def _auto_refresh_prices():
    """ê´€ì‹¬ì¢…ëª© ì‹œì„¸ë¥¼ 1ë¶„ë§ˆë‹¤ ìë™ ê°±ì‹ ."""
    while True:
        try:
            await asyncio.sleep(60)
            watchlist = _load_data("trading_watchlist", [])
            if not watchlist:
                continue

            new_cache = {}
            kr_tickers = [w for w in watchlist if w.get("market", "KR") == "KR"]
            us_tickers = [w for w in watchlist if w.get("market") == "US"]

            # í•œêµ­ ì£¼ì‹ (pykrx)
            if kr_tickers:
                try:
                    from pykrx import stock as pykrx_stock
                    today = datetime.now(KST).strftime("%Y%m%d")
                    start = (datetime.now(KST) - timedelta(days=7)).strftime("%Y%m%d")
                    for w in kr_tickers:
                        try:
                            df = await asyncio.to_thread(
                                pykrx_stock.get_market_ohlcv_by_date, start, today, w["ticker"]
                            )
                            if df is not None and not df.empty:
                                latest = df.iloc[-1]
                                prev = df.iloc[-2] if len(df) >= 2 else latest
                                close = int(latest["ì¢…ê°€"])
                                prev_close = int(prev["ì¢…ê°€"])
                                change = close - prev_close
                                change_pct = round((change / prev_close) * 100, 2) if prev_close else 0
                                new_cache[w["ticker"]] = {
                                    "price": close,
                                    "change_pct": change_pct,
                                    "updated_at": datetime.now(KST).isoformat(),
                                }
                        except Exception as e:
                            logger.debug("êµ­ë‚´ ì¢…ëª© ì‹œì„¸ íŒŒì‹± ì‹¤íŒ¨ (%s): %s", w.get("ticker"), e)
                except Exception as e:
                    logger.debug("pykrx ì‹œì„¸ ì¡°íšŒ ì‹¤íŒ¨: %s", e)

            # ë¯¸êµ­ ì£¼ì‹ (yfinance)
            if us_tickers:
                try:
                    import yfinance as yf
                    for w in us_tickers:
                        try:
                            ticker_obj = yf.Ticker(w["ticker"])
                            hist = await asyncio.to_thread(
                                lambda t=ticker_obj: t.history(period="5d")
                            )
                            if hist is not None and not hist.empty:
                                latest = hist.iloc[-1]
                                prev = hist.iloc[-2] if len(hist) >= 2 else latest
                                close = round(float(latest["Close"]), 2)
                                prev_close = round(float(prev["Close"]), 2)
                                change = round(close - prev_close, 2)
                                change_pct = round((change / prev_close) * 100, 2) if prev_close else 0
                                new_cache[w["ticker"]] = {
                                    "price": close,
                                    "change_pct": change_pct,
                                    "updated_at": datetime.now(KST).isoformat(),
                                }
                        except Exception as e:
                            logger.debug("í•´ì™¸ ì¢…ëª© ì‹œì„¸ íŒŒì‹± ì‹¤íŒ¨ (%s): %s", w.get("ticker"), e)
                except Exception as e:
                    logger.debug("yfinance ì‹œì„¸ ì¡°íšŒ ì‹¤íŒ¨: %s", e)

            if new_cache:
                async with _price_cache_lock:
                    _price_cache.update(new_cache)
                _log(f"[PRICE] ì‹œì„¸ ìë™ ê°±ì‹  ì™„ë£Œ â€” {len(new_cache)}ì¢…ëª©")
        except Exception as e:
            _log(f"[PRICE] ì‹œì„¸ ìë™ ê°±ì‹  ì˜¤ë¥˜: {e}")
            await asyncio.sleep(60)


def _default_portfolio() -> dict:
    """ê¸°ë³¸ í¬íŠ¸í´ë¦¬ì˜¤ ë°ì´í„°."""
    return {
        "cash": 50_000_000,    # ì´ˆê¸° í˜„ê¸ˆ (5ì²œë§Œì›)
        "initial_cash": 50_000_000,
        "holdings": [],        # [{ticker, name, qty, avg_price, current_price}]
        "updated_at": datetime.now(KST).isoformat(),
    }


# â”€â”€ íˆ¬ì ì„±í–¥ ì‹œìŠ¤í…œ (CEO Bì•ˆ ìŠ¹ì¸: ì„±í–¥ + CIO ììœ¨) â”€â”€

# ì„±í–¥ë³„ ì•ˆì „ ë²”ìœ„ â€” CIOê°€ ì´ ë²”ìœ„ ì•ˆì—ì„œë§Œ ììœ ë¡­ê²Œ ë³€ê²½ ê°€ëŠ¥
RISK_PROFILES = {
    "aggressive": {
        "label": "ê³µê²©ì ", "emoji": "ğŸ”¥",
        "cash_reserve":       {"min": 5,  "max": 20,  "default": 10},
        "max_position_pct":   {"min": 15, "max": 35,  "default": 30},
        "min_confidence":     {"min": 50, "max": 75,  "default": 55},
        "default_stop_loss":  {"min": -12,"max": -3,  "default": -8},
        "default_take_profit":{"min": 5,  "max": 40,  "default": 15},
        "max_daily_trades":   {"min": 5,  "max": 20,  "default": 15},
        "max_daily_loss_pct": {"min": 2,  "max": 8,   "default": 5},
        "order_size":         {"min": 0,  "max": 10_000_000, "default": 0},
    },
    "balanced": {
        "label": "ê· í˜•", "emoji": "âš–ï¸",
        "cash_reserve":       {"min": 15, "max": 35,  "default": 20},
        "max_position_pct":   {"min": 10, "max": 25,  "default": 20},
        "min_confidence":     {"min": 55, "max": 80,  "default": 65},
        "default_stop_loss":  {"min": -8, "max": -2,  "default": -5},
        "default_take_profit":{"min": 5,  "max": 25,  "default": 10},
        "max_daily_trades":   {"min": 3,  "max": 15,  "default": 10},
        "max_daily_loss_pct": {"min": 1,  "max": 5,   "default": 3},
        "order_size":         {"min": 0,  "max": 5_000_000, "default": 0},
    },
    "conservative": {
        "label": "ë³´ìˆ˜ì ", "emoji": "ğŸ¢",
        "cash_reserve":       {"min": 30, "max": 60,  "default": 40},
        "max_position_pct":   {"min": 5,  "max": 15,  "default": 10},
        "min_confidence":     {"min": 65, "max": 90,  "default": 75},
        "default_stop_loss":  {"min": -5, "max": -1,  "default": -3},
        "default_take_profit":{"min": 3,  "max": 15,  "default": 8},
        "max_daily_trades":   {"min": 1,  "max": 8,   "default": 5},
        "max_daily_loss_pct": {"min": 1,  "max": 3,   "default": 2},
        "order_size":         {"min": 0,  "max": 2_000_000, "default": 0},
    },
}


def _get_risk_profile() -> str:
    """í˜„ì¬ íˆ¬ì ì„±í–¥ ì¡°íšŒ (DBì—ì„œ)."""
    return load_setting("trading_risk_profile", "aggressive")


def _clamp_setting(key: str, value, profile: str = None) -> float | int:
    """ì„¤ì •ê°’ì„ í˜„ì¬ íˆ¬ì ì„±í–¥ì˜ ì•ˆì „ ë²”ìœ„ ë‚´ë¡œ í´ë¨í•‘í•©ë‹ˆë‹¤."""
    if profile is None:
        profile = _get_risk_profile()
    ranges = RISK_PROFILES.get(profile, RISK_PROFILES["balanced"])
    r = ranges.get(key)
    if r is None:
        return value
    return max(r["min"], min(r["max"], value))


def _default_trading_settings() -> dict:
    """ê¸°ë³¸ ìë™ë§¤ë§¤ ì„¤ì •."""
    return {
        "max_position_pct": 20,       # ì¢…ëª©ë‹¹ ìµœëŒ€ ë¹„ì¤‘ (%)
        "max_daily_trades": 10,       # ì¼ì¼ ìµœëŒ€ ê±°ë˜ íšŸìˆ˜
        "max_daily_loss_pct": 3,      # ì¼ì¼ ìµœëŒ€ ì†ì‹¤ (%)
        "default_stop_loss_pct": -5,  # ê¸°ë³¸ ì†ì ˆ (%)
        "default_take_profit_pct": 10, # ê¸°ë³¸ ìµì ˆ (%)
        "order_size": 0,              # 0 = CIO ë¹„ì¤‘ ììœ¨
        "trading_hours_kr": {"start": "09:00", "end": "15:20"},   # í•œêµ­ ì¥ ì‹œê°„
        "trading_hours_us": {"start": "22:30", "end": "05:00"},   # ë¯¸êµ­ ì¥ ì‹œê°„ (KST ê¸°ì¤€, ì„œë¨¸íƒ€ì„ ì‹œ 23:30)
        "trading_hours": {"start": "09:00", "end": "15:20"},      # í•˜ìœ„í˜¸í™˜
        "auto_stop_loss": True,       # ìë™ ì†ì ˆ í™œì„±í™”
        "auto_take_profit": True,     # ìë™ ìµì ˆ í™œì„±í™”
        "auto_execute": False,        # CIO ì‹œê·¸ë„ ê¸°ë°˜ ìë™ ì£¼ë¬¸ ì‹¤í–‰ (ì•ˆì „ì¥ì¹˜: ê¸°ë³¸ OFF)
        # --- ì‹ ë¢°ë„ ì„ê³„ê°’ (ì—°êµ¬ ê¸°ë°˜ ì¡°ì •) ---
        # ê·¼ê±°: LLMì€ ì‹¤ì œ ì •í™•ë„ë³´ë‹¤ 10~20% ê³¼ì‹  (FinGPT 2023, GPT-4 Trading 2024 ë…¼ë¬¸)
        # í•œêµ­ì¥ ì†ìµë¹„ 1:2 (ì†ì ˆ -5%, ìµì ˆ +10%) â†’ ì†ìµë¶„ê¸° ìŠ¹ë¥  â‰’ 33%
        # LLM ì‹¤ì œ ë°©í–¥ì„± ì˜ˆì¸¡ ì •í™•ë„ 55~65% â†’ ì„ê³„ê°’ 65% = ê³¼ì‹  í• ì¸ ì ìš© í›„ ìµœì†Œ ìˆ˜ìµì„ 
        "min_confidence": 65,         # ìë™ë§¤ë§¤ ìµœì†Œ ì‹ ë¢°ë„ (%, ì—°êµ¬ ê¸°ë°˜: ê¸°ì¡´ 70â†’65)
        "kis_connected": False,       # KIS(í•œêµ­íˆ¬ìì¦ê¶Œ) API ì—°ê²° ì—¬ë¶€
        "paper_trading": True,        # ëª¨ì˜íˆ¬ì ëª¨ë“œ (ì‹¤ê±°ë˜ ì „)
        "enable_real": True,          # ì‹¤ê±°ë˜ ê³„ì¢Œì— ì£¼ë¬¸
        "enable_mock": False,         # ëª¨ì˜íˆ¬ì ê³„ì¢Œì— ì£¼ë¬¸
        # --- AI ìê¸°ë³´ì •(Self-Calibration) ---
        # ì›ë¦¬: Platt Scaling ë‹¨ìˆœí™” â€” ì‹¤ì œ ìŠ¹ë¥ /ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¹„ìœ¨ë¡œ ë³´ì • ê³„ìˆ˜ ê³„ì‚°
        # factor < 1.0: AI ê³¼ì‹  â†’ ìœ íš¨ ì‹ ë¢°ë„ í•˜í–¥ ë³´ì • / factor > 1.0: AI ê²¸ì† â†’ ìƒí–¥
        "calibration_enabled": True,  # AI ìê¸°ë³´ì • í™œì„±í™”
        "calibration_lookback": 20,   # ë³´ì • ê³„ì‚°ì— ì‚¬ìš©í•  ìµœê·¼ ê±°ë˜ ìˆ˜
    }


def _compute_calibration_factor(lookback: int = 20) -> dict:
    """ì‹¤ì œ ìŠ¹ë¥  vs ì˜ˆì¸¡ ì‹ ë¢°ë„ ë¹„ìœ¨ë¡œ AI ìê¸°ë³´ì • ê³„ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    ë°©ë²•ë¡ : Platt Scaling ë‹¨ìˆœí™” ë²„ì „
    - LLMì€ ì˜ˆì¸¡ ì‹ ë¢°ë„ë¥¼ ì‹¤ì œ ì •í™•ë„ë³´ë‹¤ ê³¼ëŒ€ ë³´ê³ í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ
      (FinGPT 2023 / GPT-4 Trading 2024 ë…¼ë¬¸ì—ì„œ 10~20% ê³¼ì‹  í™•ì¸)
    - ë³´ì • ê³„ìˆ˜(factor) = ì‹¤ì œ ìŠ¹ë¥  / ì˜ˆì¸¡ í‰ê·  ì‹ ë¢°ë„
    - factor < 1: AI ê³¼ì‹  â†’ ìœ íš¨ ì‹ ë¢°ë„ í•˜í–¥ / factor > 1: AI ê²¸ì† â†’ ìƒí–¥
    - ì•ˆì „ ë²”ìœ„: 0.5 ~ 1.5 (ê·¹ë‹¨ì  ë³´ì • ë°©ì§€)
    """
    import re as _re
    history = _load_data("trading_history", [])
    bot_trades = [
        h for h in history
        if h.get("auto_bot", False) or "ì‹ ë¢°ë„" in h.get("strategy", "")
    ]
    recent = bot_trades[:lookback]

    if len(recent) < 5:
        return {
            "factor": 1.0, "win_rate": None, "avg_confidence": None,
            "n": len(recent), "note": f"ë°ì´í„° ë¶€ì¡± ({len(recent)}ê±´, ìµœì†Œ 5ê±´ í•„ìš”) â€” ë³´ì • ë¯¸ì ìš©",
        }

    closed = [h for h in recent if h.get("action") == "sell" and "pnl" in h]
    if not closed:
        return {
            "factor": 1.0, "win_rate": None, "avg_confidence": None,
            "n": 0, "note": "í‰ê°€ ê°€ëŠ¥í•œ ë§¤ë„ ê¸°ë¡ ì—†ìŒ â€” ë³´ì • ë¯¸ì ìš©",
        }

    wins = sum(1 for t in closed if t.get("pnl", 0) > 0)
    actual_win_rate = wins / len(closed)

    confidences = []
    for t in closed:
        m = _re.search(r"ì‹ ë¢°ë„\s*(\d+)", t.get("strategy", ""))
        if m:
            confidences.append(int(m.group(1)) / 100.0)

    if not confidences:
        return {
            "factor": 1.0, "win_rate": round(actual_win_rate * 100, 1),
            "avg_confidence": None, "n": len(closed),
            "note": "ì‹ ë¢°ë„ ê¸°ë¡ ì—†ìŒ â€” ë³´ì • ë¯¸ì ìš©",
        }

    avg_confidence = sum(confidences) / len(confidences)
    raw_factor = actual_win_rate / avg_confidence if avg_confidence > 0 else 1.0
    factor = round(max(0.5, min(1.5, raw_factor)), 3)

    diff = actual_win_rate * 100 - avg_confidence * 100
    if diff < -5:
        note = f"AI ê³¼ì‹  (ì˜ˆì¸¡ {avg_confidence*100:.0f}% â†’ ì‹¤ì œ {actual_win_rate*100:.0f}%) â†’ ì‹ ë¢°ë„ {factor:.2f}ë°° í•˜í–¥ ë³´ì •"
    elif diff > 5:
        note = f"AI ê²¸ì† (ì˜ˆì¸¡ {avg_confidence*100:.0f}% â†’ ì‹¤ì œ {actual_win_rate*100:.0f}%) â†’ ì‹ ë¢°ë„ {factor:.2f}ë°° ìƒí–¥ ë³´ì •"
    else:
        note = f"AI ë³´ì • ë¯¸ë¯¸ (ì˜ˆì¸¡â‰’ì‹¤ì œ, factor={factor:.2f})"

    return {
        "factor": factor,
        "win_rate": round(actual_win_rate * 100, 1),
        "avg_confidence": round(avg_confidence * 100, 1),
        "n": len(closed),
        "note": note,
    }


def _build_calibration_prompt_section(settings: dict | None = None) -> str:
    """CIO ë¶„ì„ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…í•  ìê¸°í•™ìŠµ ë³´ì • ì„¹ì…˜ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

    í¬í•¨ í•­ëª©:
    1. ê¸°ì¡´ Platt Scaling ë³´ì • (í˜¸í™˜ì„±)
    2. ë² ì´ì§€ì•ˆ êµ¬ê°„ë³„ ë³´ì • ë°ì´í„°
    3. ì „ë¬¸ê°€ ELO ê°€ì¤‘ì¹˜
    4. ì˜¤ë‹µ íŒ¨í„´ ê²½ê³ 
    5. ë„êµ¬ ì¶”ì²œ/ê²½ê³ 
    """
    from db import (
        get_all_calibration_buckets, get_all_analyst_elos,
        get_active_error_patterns, get_tool_effectiveness_all,
    )

    if settings is None:
        settings = {}

    parts = []

    # â”€ 1. ë² ì´ì§€ì•ˆ êµ¬ê°„ë³„ ë³´ì • â”€
    try:
        buckets = get_all_calibration_buckets()
        if buckets:
            rows = []
            for b in buckets:
                total = b.get("total_count", 0)
                if total < 3:
                    continue
                actual = b.get("actual_rate", 0)
                ci_lo = b.get("ci_lower", 0)
                ci_hi = b.get("ci_upper", 1)
                actual_pct = round(actual * 100, 1)
                ci_lo_pct = round(ci_lo * 100)
                ci_hi_pct = round(ci_hi * 100)
                # ë³´ì • ë°©í–¥ íŒë‹¨
                bucket_label = b["bucket"]
                mid = 0.5  # ê¸°ë³¸
                try:
                    lo, hi = bucket_label.split("-")
                    mid = (int(lo) + int(hi)) / 200.0
                except Exception:
                    pass
                if actual < mid - 0.05:
                    direction = "â†“ í•˜í–¥ ë³´ì • í•„ìš”"
                elif actual > mid + 0.05:
                    direction = "â†‘ ìƒí–¥ ê°€ëŠ¥"
                else:
                    direction = "â‰ˆ ì ì •"
                rows.append(f"| {bucket_label}% | {total}ê±´ | {actual_pct}% | [{ci_lo_pct}-{ci_hi_pct}%] | {direction} |")

            if rows:
                parts.append(
                    "\n## ğŸ“Š ì‹ ë¢°ë„ ë³´ì • ë°ì´í„° (Bayesian Calibration)\n"
                    "| êµ¬ê°„ | ì˜ˆì¸¡ íšŸìˆ˜ | ì‹¤ì œ ì ì¤‘ë¥  | 95% CI | ë³´ì • ë°©í–¥ |\n"
                    "|------|----------|-----------|--------|----------|\n"
                    + "\n".join(rows)
                    + "\nâ†’ ìœ„ ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ì‹ ë¢°ë„ ìˆ˜ì¹˜ë¥¼ ë³´ì •í•˜ì„¸ìš”."
                )
    except Exception:
        pass

    # â”€ 2. ì „ë¬¸ê°€ ELO ê°€ì¤‘ì¹˜ â”€
    try:
        elos = get_all_analyst_elos()
        if elos and len(elos) >= 2:
            elo_rows = []
            for e in sorted(elos, key=lambda x: x.get("elo_rating", 1500), reverse=True):
                agent = e["agent_id"].replace("_specialist", "").replace("_", " ").title()
                rating = round(e.get("elo_rating", 1500))
                total = e.get("total_predictions", 0)
                correct = e.get("correct_predictions", 0)
                hit = round(correct / total * 100) if total > 0 else 0
                weight = "â˜…â˜…â˜…" if rating >= 1560 else ("â˜…â˜…" if rating >= 1520 else "â˜…")
                elo_rows.append(f"| {agent} | {rating} | {hit}% ({correct}/{total}) | {weight} |")

            if elo_rows:
                parts.append(
                    "\n## ğŸ† ì „ë¬¸ê°€ ì‹ ë¢° ê°€ì¤‘ì¹˜ (ELO ê¸°ë°˜)\n"
                    "| ì „ë¬¸ê°€ | ELO | ì ì¤‘ë¥  | ê°€ì¤‘ì¹˜ |\n"
                    "|--------|-----|--------|--------|\n"
                    + "\n".join(elo_rows)
                    + "\nâ†’ ELO ë†’ì€ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì„¸ìš”."
                )
    except Exception:
        pass

    # â”€ 3. ì˜¤ë‹µ íŒ¨í„´ ê²½ê³  â”€
    try:
        patterns = get_active_error_patterns()
        if patterns:
            warns = []
            for p in patterns[:5]:
                warns.append(f"- {p['description']}")
            parts.append(
                "\n## âš ï¸ ì£¼ì˜ íŒ¨í„´ (ìµœê·¼ ì˜¤ë¥˜ì—ì„œ í•™ìŠµ)\n"
                + "\n".join(warns)
            )
    except Exception:
        pass

    # â”€ 4. ë„êµ¬ ì¶”ì²œ/ê²½ê³  â”€
    try:
        tools = get_tool_effectiveness_all()
        if tools and len(tools) >= 3:
            good = [t for t in tools if t.get("total_uses", 0) >= 3 and t.get("eff_score", 0.5) >= 0.6]
            bad = [t for t in tools if t.get("total_uses", 0) >= 3 and t.get("eff_score", 0.5) < 0.45]
            tool_lines = []
            if good:
                good_s = sorted(good, key=lambda x: x["eff_score"], reverse=True)[:4]
                names = ", ".join(f"{t['tool_name']}({round(t['eff_score']*100)}%)" for t in good_s)
                tool_lines.append(f"- ìš°ìˆ˜: {names}")
            if bad:
                bad_s = sorted(bad, key=lambda x: x["eff_score"])[:3]
                names = ", ".join(f"{t['tool_name']}({round(t['eff_score']*100)}%)" for t in bad_s)
                tool_lines.append(f"- ë¶€ì§„: {names} â€” ë¶„ì„ ì°¸ê³ ë§Œ, ê²°ì • ê¸°ë°˜ ê¸ˆì§€")
            if tool_lines:
                parts.append(
                    "\n## ğŸ”§ ë„êµ¬ ì¶”ì²œ (ì„±ê³¼ ê¸°ë°˜)\n"
                    + "\n".join(tool_lines)
                )
    except Exception:
        pass

    # â”€ 5. ê¸°ì¡´ Platt Scaling ë³´ì • (í•˜ìœ„ í˜¸í™˜) â”€
    if settings.get("calibration_enabled", True):
        calibration = _compute_calibration_factor(settings.get("calibration_lookback", 20))
        if calibration.get("win_rate") is not None:
            diff = calibration["win_rate"] - (calibration.get("avg_confidence") or calibration["win_rate"])
            direction = "ë³´ìˆ˜ì ìœ¼ë¡œ" if diff < -5 else ("ì ê·¹ì ìœ¼ë¡œ" if diff > 5 else "í˜„ì¬ ìˆ˜ì¤€ìœ¼ë¡œ")
            parts.append(
                f"\n## ğŸ“ˆ ë§¤ë§¤ ì„±ê³¼ ë³´ì • (Platt Scaling)\n"
                f"- ìµœê·¼ {calibration['n']}ê±´ ì‹¤ì œ ìŠ¹ë¥ : {calibration['win_rate']}%\n"
                f"- í‰ê·  ì˜ˆì¸¡ ì‹ ë¢°ë„: {calibration.get('avg_confidence', 'N/A')}%\n"
                f"- {calibration['note']}\n"
                f"â†’ ì´ë²ˆ ì‹ ë¢°ë„ë¥¼ {direction} ì„¤ì •í•˜ì„¸ìš”."
            )

    return "\n".join(parts) if parts else ""


# â”€â”€ [QUANT SCORE] ì •ëŸ‰ ì‹ ë¢°ë„ ê³„ì‚° (RSI/MACD/ë³¼ë¦°ì €ë°´ë“œ/ê±°ë˜ëŸ‰/ì´ë™í‰ê· ) â”€â”€

async def _compute_quant_score(ticker: str, market: str = "KR", lookback: int = 60) -> dict:
    """RSI(14)/MACD(12,26,9)/ë³¼ë¦°ì €ë°´ë“œ(20,2Ïƒ)/ê±°ë˜ëŸ‰/ì´ë™í‰ê· ìœ¼ë¡œ ì •ëŸ‰ ì‹ ë¢°ë„ ê³„ì‚°.

    LLMì´ ì‹ ë¢°ë„ë¥¼ ì§ì ‘ ì°ëŠ” ëŒ€ì‹ , ì´ í•¨ìˆ˜ ê³„ì‚°ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ Â±20%p ì¡°ì •ë§Œ í—ˆìš©.
    ë°˜í™˜: {ticker, direction, quant_confidence(0-99), components, summary, error}
    """
    _err = {
        "ticker": ticker, "direction": "neutral", "quant_confidence": 50,
        "components": {}, "summary": "ì •ëŸ‰ ë°ì´í„° ì—†ìŒ â€” AI íŒë‹¨ ì‚¬ìš©", "error": None,
    }
    try:
        closes: list = []
        volumes: list = []

        if market == "KR":
            try:
                from pykrx import stock as _pykrx
                _today = datetime.now(KST).strftime("%Y%m%d")
                _start = (datetime.now(KST) - timedelta(days=lookback + 30)).strftime("%Y%m%d")
                df = await asyncio.to_thread(_pykrx.get_market_ohlcv_by_date, _start, _today, ticker)
                if df is None or df.empty or len(df) < 20:
                    return {**_err, "error": f"pykrx ë°ì´í„° ë¶€ì¡± ({0 if df is None else len(df)}ì¼)"}
                closes = df["ì¢…ê°€"].astype(float).tolist()
                volumes = df["ê±°ë˜ëŸ‰"].astype(float).tolist()
            except Exception as e:
                return {**_err, "error": f"pykrx: {str(e)[:60]}"}
        else:
            try:
                import yfinance as yf
                _t = yf.Ticker(ticker)
                hist = await asyncio.to_thread(lambda: _t.history(period="3mo"))
                if hist is None or hist.empty or len(hist) < 20:
                    return {**_err, "error": "yfinance ë°ì´í„° ë¶€ì¡±"}
                closes = hist["Close"].astype(float).tolist()
                volumes = hist["Volume"].astype(float).tolist()
            except Exception as e:
                return {**_err, "error": f"yfinance: {str(e)[:60]}"}

        n = len(closes)

        # â”€â”€ RSI(14) â”€â”€
        def _rsi(prices, p=14):
            if len(prices) < p + 1:
                return 50.0
            d = [prices[i] - prices[i-1] for i in range(1, len(prices))]
            g = [max(x, 0.0) for x in d[-p:]]
            l = [abs(min(x, 0.0)) for x in d[-p:]]
            ag, al = sum(g)/p, sum(l)/p
            return 100.0 if al == 0 else 100 - 100/(1 + ag/al)

        rsi = _rsi(closes)

        # â”€â”€ RSI â†’ ë°©í–¥ íˆ¬í‘œ (ë°©í–¥ê³¼ ì‹ ë¢°ë„ ë¶„ë¦¬) â”€â”€
        if   rsi < 30: rsi_dir, rsi_str, rsi_sig = "buy",  0.8, f"ê³¼ë§¤ë„({rsi:.1f})"
        elif rsi < 40: rsi_dir, rsi_str, rsi_sig = "buy",  0.5, f"ë§¤ìˆ˜ìš°í˜¸({rsi:.1f})"
        elif rsi < 45: rsi_dir, rsi_str, rsi_sig = "neutral", 0.2, f"ì¤‘ë¦½({rsi:.1f})"
        elif rsi < 55: rsi_dir, rsi_str, rsi_sig = "neutral", 0.1, f"ì¤‘ë¦½({rsi:.1f})"
        elif rsi < 60: rsi_dir, rsi_str, rsi_sig = "neutral", 0.2, f"ì¤‘ë¦½({rsi:.1f})"
        elif rsi < 70: rsi_dir, rsi_str, rsi_sig = "sell", 0.5, f"ë§¤ë„ìš°í˜¸({rsi:.1f})"
        else:          rsi_dir, rsi_str, rsi_sig = "sell", 0.8, f"ê³¼ë§¤ìˆ˜({rsi:.1f})"

        # â”€â”€ MACD(12, 26, 9) â†’ ë°©í–¥ íˆ¬í‘œ â”€â”€
        def _ema(prices, p):
            if len(prices) < p:
                return [prices[-1]]
            k = 2 / (p + 1)
            vals = [sum(prices[:p]) / p]
            for x in prices[p:]:
                vals.append(x * k + vals[-1] * (1 - k))
            return vals

        macd_dir, macd_str, macd_sig = "neutral", 0.1, "ë°ì´í„°ë¶€ì¡±"
        if n >= 27:
            e12 = _ema(closes, 12)
            e26 = _ema(closes, 26)
            ml = min(len(e12), len(e26))
            macd_line = [e12[i] - e26[i] for i in range(-ml, 0)]
            if len(macd_line) >= 9:
                sig_line = _ema(macd_line, 9)
                if sig_line:
                    mv, sv = macd_line[-1], sig_line[-1]
                    mv2 = macd_line[-2] if len(macd_line) >= 2 else mv
                    sv2 = sig_line[-2] if len(sig_line) >= 2 else sv
                    if   mv2 < sv2 and mv > sv:           macd_dir, macd_str, macd_sig = "buy",  0.9, "ê³¨ë“ í¬ë¡œìŠ¤â†‘"
                    elif mv2 > sv2 and mv < sv:           macd_dir, macd_str, macd_sig = "sell", 0.9, "ë°ë“œí¬ë¡œìŠ¤â†“"
                    elif mv > sv and (mv-sv) > (mv2-sv2): macd_dir, macd_str, macd_sig = "buy",  0.6, "MACD>ì‹œê·¸ë„ìƒìŠ¹"
                    elif mv > sv:                         macd_dir, macd_str, macd_sig = "buy",  0.3, "MACD>ì‹œê·¸ë„"
                    elif mv < sv and (mv-sv) < (mv2-sv2): macd_dir, macd_str, macd_sig = "sell", 0.6, "MACD<ì‹œê·¸ë„í•˜ë½"
                    else:                                 macd_dir, macd_str, macd_sig = "sell", 0.3, "MACD<ì‹œê·¸ë„"

        # â”€â”€ ë³¼ë¦°ì €ë°´ë“œ(20, 2Ïƒ) â†’ ë°©í–¥ íˆ¬í‘œ â”€â”€
        bb_dir, bb_str, bb_sig, pct_b = "neutral", 0.1, "ë°ì´í„°ë¶€ì¡±", 0.5
        if n >= 20:
            sma = sum(closes[-20:]) / 20
            std = (sum((c - sma)**2 for c in closes[-20:]) / 20) ** 0.5
            bw = 4 * std
            if bw > 0:
                pct_b = (closes[-1] - (sma - 2*std)) / bw
                if   pct_b <= 0.10: bb_dir, bb_str, bb_sig = "buy",  0.9, f"í•˜ë‹¨ëŒíŒŒ(%B={pct_b:.2f})"
                elif pct_b <= 0.25: bb_dir, bb_str, bb_sig = "buy",  0.6, f"í•˜ë‹¨ê·¼ì ‘(%B={pct_b:.2f})"
                elif pct_b <= 0.40: bb_dir, bb_str, bb_sig = "buy",  0.2, f"ì¤‘í•˜ë‹¨(%B={pct_b:.2f})"
                elif pct_b <= 0.60: bb_dir, bb_str, bb_sig = "neutral", 0.1, f"ì¤‘ê°„(%B={pct_b:.2f})"
                elif pct_b <= 0.75: bb_dir, bb_str, bb_sig = "sell", 0.2, f"ì¤‘ìƒë‹¨(%B={pct_b:.2f})"
                elif pct_b <= 0.90: bb_dir, bb_str, bb_sig = "sell", 0.6, f"ìƒë‹¨ê·¼ì ‘(%B={pct_b:.2f})"
                else:               bb_dir, bb_str, bb_sig = "sell", 0.9, f"ìƒë‹¨ëŒíŒŒ(%B={pct_b:.2f})"

        # â”€â”€ ê±°ë˜ëŸ‰ (ë°©í–¥ ì•„ë‹Œ í™•ì‹  ë³´ì •ìš©) â”€â”€
        vol_adj, vol_sig = 0, "ë³´í†µ"
        vol_ratio = 1.0
        if n >= 20 and len(volumes) >= 20:
            avg_v = sum(volumes[-20:-1]) / 19
            if avg_v > 0:
                vol_ratio = volumes[-1] / avg_v
                if   vol_ratio >= 2.0: vol_adj, vol_sig = 8,  f"ê¸‰ì¦({vol_ratio:.1f}x)"
                elif vol_ratio >= 1.5: vol_adj, vol_sig = 5,  f"ì¦ê°€({vol_ratio:.1f}x)"
                elif vol_ratio < 0.8:  vol_adj, vol_sig = -5, f"ê°ì†Œ({vol_ratio:.1f}x)"
                else:                  vol_sig = f"ë³´í†µ({vol_ratio:.1f}x)"

        # â”€â”€ ì´ë™í‰ê·  ì¶”ì„¸ â†’ ë°©í–¥ íˆ¬í‘œ â”€â”€
        ma5  = round(sum(closes[-5:]) /5)  if n >= 5  else 0
        ma20 = round(sum(closes[-20:])/20) if n >= 20 else 0
        ma60 = round(sum(closes[-60:])/60) if n >= 60 else 0
        if ma5 and ma20 and ma60:
            if   ma5 > ma20 > ma60: tr_dir, tr_str, tr_sig = "buy",  0.8, "ìƒìŠ¹ì •ë ¬(5>20>60)"
            elif ma5 > ma20:        tr_dir, tr_str, tr_sig = "buy",  0.4, "ë‹¨ê¸°ë°˜ë“±"
            elif ma5 < ma20 < ma60: tr_dir, tr_str, tr_sig = "sell", 0.8, "í•˜ë½ì •ë ¬(5<20<60)"
            else:                   tr_dir, tr_str, tr_sig = "neutral", 0.2, "í˜¼ì¡°ì„¸"
        elif ma5 and ma20:
            if ma5 > ma20: tr_dir, tr_str, tr_sig = "buy",  0.4, "ë‹¨ê¸°ìƒìŠ¹"
            else:          tr_dir, tr_str, tr_sig = "sell", 0.4, "ë‹¨ê¸°í•˜ë½"
        else:
            tr_dir, tr_str, tr_sig = "neutral", 0.1, "ë°ì´í„°ë¶€ì¡±"

        # â”€â”€ ì¢…í•©: ë°©í–¥ = ë‹¤ìˆ˜ê²°, ì‹ ë¢°ë„ = í•©ì˜ìœ¨ â”€â”€
        votes = [
            ("RSI",  rsi_dir,  rsi_str),
            ("MACD", macd_dir, macd_str),
            ("BB",   bb_dir,   bb_str),
            ("MA",   tr_dir,   tr_str),
        ]
        buy_votes  = [(nm, st) for nm, d, st in votes if d == "buy"]
        sell_votes = [(nm, st) for nm, d, st in votes if d == "sell"]
        n_votes = len(votes)

        if len(buy_votes) > len(sell_votes):
            direction = "buy"
            winner_count = len(buy_votes)
            winner_avg_str = sum(s for _, s in buy_votes) / len(buy_votes)
        elif len(sell_votes) > len(buy_votes):
            direction = "sell"
            winner_count = len(sell_votes)
            winner_avg_str = sum(s for _, s in sell_votes) / len(sell_votes)
        else:
            direction = "neutral"
            winner_count = 0
            winner_avg_str = 0.3

        # í•©ì˜ìœ¨ â†’ ê¸°ë³¸ ì‹ ë¢°ë„ (30~90% ë²”ìœ„)
        if direction == "neutral":
            base_conf = 50
        else:
            consensus = winner_count / n_votes  # 0.25~1.0
            base_conf = 35 + consensus * 55     # 1/4â†’49, 2/4â†’63, 3/4â†’76, 4/4â†’90
            # ê°•ë„ ë³´ì •: ê°™ì€ 3/4ë¼ë„ ì‹ í˜¸ ê°•ë„ê°€ ë‹¤ë¦„
            strength_adj = (winner_avg_str - 0.5) * 10  # -5 ~ +4
            base_conf += strength_adj

        qconf = int(max(30, min(95, base_conf + vol_adj)))
        dir_kr = {"buy": "ë§¤ìˆ˜", "sell": "ë§¤ë„", "neutral": "ê´€ë§"}[direction]
        vote_detail = " / ".join(
            f"{nm}â†’{'ë§¤ìˆ˜' if d == 'buy' else 'ë§¤ë„' if d == 'sell' else 'ì¤‘ë¦½'}"
            for nm, d, _ in votes
        )
        summary = (
            f"RSI {rsi:.0f} / MACD {macd_sig} / BB {bb_sig} / ê±°ë˜ëŸ‰ {vol_sig}"
            f" â†’ íˆ¬í‘œ [{vote_detail}] = {winner_count}/{n_votes} í•©ì˜"
            f" â†’ ì •ëŸ‰ì‹ ë¢°ë„ {qconf}%({dir_kr})"
        )
        return {
            "ticker": ticker, "direction": direction, "quant_confidence": qconf,
            "components": {
                "rsi":       {"value": round(rsi, 1), "direction": rsi_dir, "strength": rsi_str, "signal": rsi_sig},
                "macd":      {"direction": macd_dir, "strength": macd_str, "signal": macd_sig},
                "bollinger": {"pct_b": round(pct_b, 2), "direction": bb_dir, "strength": bb_str, "signal": bb_sig},
                "volume":    {"ratio": round(vol_ratio, 1), "adj": vol_adj, "signal": vol_sig},
                "trend":     {"ma5": ma5, "ma20": ma20, "ma60": ma60, "direction": tr_dir, "strength": tr_str, "signal": tr_sig},
            },
            "votes": {"buy": len(buy_votes), "sell": len(sell_votes), "neutral": n_votes - len(buy_votes) - len(sell_votes)},
            "summary": summary, "error": None,
        }
    except Exception as e:
        return {**_err, "error": f"ê³„ì‚°ì˜¤ë¥˜: {str(e)[:80]}"}


async def _build_quant_prompt_section(market_watchlist: list, market: str = "KR") -> str:
    """ê´€ì‹¬ì¢…ëª© ì „ì²´ ì •ëŸ‰ì§€í‘œë¥¼ ë³‘ë ¬ ê³„ì‚° â†’ í”„ë¡¬í”„íŠ¸ ì‚½ì…ìš© í…Œì´ë¸” ë°˜í™˜."""
    if not market_watchlist:
        return ""
    try:
        tasks = [_compute_quant_score(w["ticker"], market) for w in market_watchlist]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        rows = []
        for w, r in zip(market_watchlist, results):
            if isinstance(r, Exception) or (isinstance(r, dict) and r.get("error")):
                rows.append(
                    f"| {w['name']}({w['ticker']}) | ì¡°íšŒì‹¤íŒ¨ | â€” | â€” | â€” | â€” | â€” | **50% íŒë‹¨ë¶ˆê°€** |"
                )
                continue
            c = r["components"]
            d_kr = {"buy": "ë§¤ìˆ˜", "sell": "ë§¤ë„", "neutral": "ê´€ë§"}[r["direction"]]
            v = r.get("votes", {})
            vote_str = f"ë§¤ìˆ˜{v.get('buy',0)}:ë§¤ë„{v.get('sell',0)}:ì¤‘ë¦½{v.get('neutral',0)}"
            rows.append(
                f"| {w['name']}({w['ticker']}) "
                f"| {c['rsi']['signal']} "
                f"| {c['macd']['signal']} "
                f"| {c['bollinger']['signal']} "
                f"| {c['volume']['signal']} "
                f"| {c['trend']['signal']} "
                f"| {vote_str} "
                f"| **{r['quant_confidence']}% {d_kr}** |"
            )
        return (
            "\n\n## ğŸ“ ì •ëŸ‰ì§€í‘œ ì‚¬ì „ë¶„ì„ (ì„œë²„ ìë™ê³„ì‚° â€” ì§€í‘œ í•©ì˜ ë°©ì‹)\n"
            "| ì¢…ëª© | RSI(14) | MACD | ë³¼ë¦°ì €ë°´ë“œ | ê±°ë˜ëŸ‰ | ì¶”ì„¸(MA) | ì§€í‘œíˆ¬í‘œ | í•©ì˜ì‹ ë¢°ë„ |\n"
            "|------|---------|------|-----------|--------|---------|---------|------------|\n"
            + "\n".join(rows)
            + "\n\nâš ï¸ ìœ„ í•©ì˜ì‹ ë¢°ë„ëŠ” 4ê°œ ê¸°ìˆ ì§€í‘œì˜ ë°©í–¥ í•©ì˜ìœ¨ì…ë‹ˆë‹¤."
            " ë‰´ìŠ¤/ì‹¤ì /ìˆ˜ê¸‰/ë§¤í¬ë¡œ ë“± ì •ì„±ë¶„ì„ì„ ë°˜ì˜í•˜ì—¬ **Â±20%p ë²”ìœ„ ë‚´**ì—ì„œ ì¡°ì •í•˜ì„¸ìš”."
            " ê·¼ê±°ë¥¼ ë°˜ë“œì‹œ ëª…ì‹œí•˜ì„¸ìš”."
        )
    except Exception as e:
        return f"\n\n## ğŸ“ ì •ëŸ‰ì§€í‘œ (ê³„ì‚° ì‹¤íŒ¨: {str(e)[:60]})\n"


async def _build_argos_context_section(market_watchlist: list, market: str = "KR") -> str:
    """ARGOS DBì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ êº¼ë‚´ íŒ€ì¥ í”„ë¡¬í”„íŠ¸ì— ì§ì ‘ ì£¼ì….

    ì„œë²„ê°€ ì‹¬ë¶€ë¦„(ë°ì´í„° ìˆ˜ì§‘)ì„ ì™„ë£Œ â†’ íŒ€ì¥ì€ í•´ì„ë§Œ.
    DBì— ë°ì´í„° ì—†ìœ¼ë©´ í•´ë‹¹ ì„¹ì…˜ ìƒëµ (íŒ€ì¥ì´ íŒë‹¨í•˜ë„ë¡).
    """
    conn = get_connection()
    sections = []

    # â‘  ì¢…ëª©ë³„ ìµœê·¼ ì£¼ê°€ (ìµœê·¼ 10ê±°ë˜ì¼)
    price_rows_all = []
    for w in market_watchlist:
        ticker = w["ticker"]
        try:
            rows = conn.execute(
                """SELECT trade_date, close_price, change_pct, volume
                   FROM argos_price_history
                   WHERE ticker=?
                   ORDER BY trade_date DESC LIMIT 10""",
                (ticker,)
            ).fetchall()
            if rows:
                price_rows_all.append((w["name"], ticker, rows))
        except Exception:
            pass

    if price_rows_all:
        lines = ["\n\n## ğŸ“ˆ ìµœê·¼ ì£¼ê°€ (ARGOS ìˆ˜ì§‘ â€” ì„œë²„ ì œê³µ)"]
        for name, ticker, rows in price_rows_all:
            latest = rows[0]
            unit = "ì›" if market == "KR" else "USD"
            lines.append(f"\n### {name} ({ticker})")
            lines.append(f"  í˜„ì¬ê°€: {latest[1]:,.0f}{unit}  ì „ì¼ëŒ€ë¹„: {(latest[2] or 0):+.2f}%")
            lines.append("  | ë‚ ì§œ | ì¢…ê°€ | ë“±ë½ë¥  | ê±°ë˜ëŸ‰ |")
            lines.append("  |------|------|--------|--------|")
            for r in rows:
                lines.append(f"  | {r[0]} | {r[1]:,.0f} | {(r[2] or 0):+.2f}% | {(r[3] or 0):,.0f} |")
        sections.append("\n".join(lines))

    # â‘¡ ë§¤í¬ë¡œ ì§€í‘œ (KOSPI, USD_KRW ë“±)
    try:
        macro_rows = conn.execute(
            """SELECT indicator, trade_date, value
               FROM argos_macro_data
               ORDER BY indicator, trade_date DESC"""
        ).fetchall()
        if macro_rows:
            macro_dict: dict = {}
            for r in macro_rows:
                if r[0] not in macro_dict:
                    macro_dict[r[0]] = (r[1], r[2])
            lines = ["\n\n## ğŸŒ ë§¤í¬ë¡œ ì§€í‘œ (ARGOS ìˆ˜ì§‘ â€” ì„œë²„ ì œê³µ)"]
            for indicator, (dt, val) in macro_dict.items():
                lines.append(f"  {indicator}: {val:,.2f} ({dt})")
            sections.append("\n".join(lines))
    except Exception:
        pass

    # â‘¢ ìµœì‹  ê³µì‹œ (DART â€” ticker ê¸°ì¤€)
    dart_found = []
    for w in market_watchlist:
        ticker = w["ticker"]
        try:
            rows = conn.execute(
                """SELECT corp_name, report_nm, rcept_dt
                   FROM argos_dart_filings
                   WHERE ticker=?
                   ORDER BY rcept_dt DESC LIMIT 5""",
                (ticker,)
            ).fetchall()
            if rows:
                dart_found.append((w["name"], ticker, rows))
        except Exception:
            pass

    if dart_found:
        lines = ["\n\n## ğŸ“‹ ìµœì‹  ê³µì‹œ (ARGOS ìˆ˜ì§‘ â€” ì„œë²„ ì œê³µ)"]
        for name, ticker, rows in dart_found:
            lines.append(f"\n### {name} ({ticker})")
            for r in rows:
                lines.append(f"  [{r[2]}] {r[1]}")
        sections.append("\n".join(lines))

    # â‘£ ë‰´ìŠ¤ ìºì‹œ (ì¢…ëª©ëª… í‚¤ì›Œë“œ)
    news_found = []
    for w in market_watchlist:
        keyword = w["name"]
        try:
            rows = conn.execute(
                """SELECT title, description, pub_date
                   FROM argos_news_cache
                   WHERE keyword=?
                   ORDER BY pub_date DESC LIMIT 5""",
                (keyword,)
            ).fetchall()
            if rows:
                news_found.append((keyword, rows))
        except Exception:
            pass

    if news_found:
        lines = ["\n\n## ğŸ“° ìµœì‹  ë‰´ìŠ¤ (ARGOS ìˆ˜ì§‘ â€” ì„œë²„ ì œê³µ)"]
        for keyword, rows in news_found:
            lines.append(f"\n### {keyword}")
            for r in rows:
                title = (r[0] or "")[:60]
                desc = (r[1] or "")[:80]
                lines.append(f"  [{r[2][:10] if r[2] else ''}] {title}")
                if desc:
                    lines.append(f"    â†’ {desc}")
        sections.append("\n".join(lines))

    # â‘¤ ì¬ë¬´ì§€í‘œ (PER/PBR/EPS â€” pykrx 1ì¼ ìˆ˜ì§‘)
    try:
        conn2 = get_connection()
        fin_found = []
        for w in market_watchlist:
            ticker = w["ticker"]
            try:
                row = conn2.execute(
                    """SELECT trade_date, per, pbr, eps, bps
                       FROM argos_financial_data
                       WHERE ticker=?
                       ORDER BY trade_date DESC LIMIT 1""",
                    (ticker,)
                ).fetchone()
                if row:
                    fin_found.append((w["name"], ticker, row))
            except Exception:
                pass
        conn2.close()
        if fin_found:
            lines = ["\n\n## ğŸ’¹ ì¬ë¬´ì§€í‘œ (ARGOS ìˆ˜ì§‘ â€” ì„œë²„ ì œê³µ)"]
            lines.append("  | ì¢…ëª© | PER | PBR | EPS | BPS | ê¸°ì¤€ì¼ |")
            lines.append("  |------|-----|-----|-----|-----|--------|")
            for name, ticker, r in fin_found:
                lines.append(f"  | {name}({ticker}) | {r[1]:.1f} | {r[2]:.2f} | {r[3]:,.0f} | {r[4]:,.0f} | {r[0]} |")
            sections.append("\n".join(lines))
    except Exception:
        pass

    # â‘¥ ì—…ì¢…ì§€ìˆ˜ (pykrx 11ê°œ ì—…ì¢… â€” 1ì¼ ìˆ˜ì§‘)
    try:
        conn3 = get_connection()
        sector_rows = conn3.execute(
            """SELECT s1.sector_name, s1.close_val, s1.change_pct, s1.trade_date
               FROM argos_sector_data s1
               INNER JOIN (
                   SELECT sector_name, MAX(trade_date) AS max_date
                   FROM argos_sector_data GROUP BY sector_name
               ) s2 ON s1.sector_name=s2.sector_name AND s1.trade_date=s2.max_date
               ORDER BY s1.change_pct DESC"""
        ).fetchall()
        conn3.close()
        if sector_rows:
            lines = ["\n\n## ğŸ­ ì—…ì¢…ì§€ìˆ˜ (ARGOS ìˆ˜ì§‘ â€” ì„œë²„ ì œê³µ)"]
            lines.append("  | ì—…ì¢… | ì§€ìˆ˜ | ë“±ë½ë¥  | ê¸°ì¤€ì¼ |")
            lines.append("  |------|------|--------|--------|")
            for r in sector_rows:
                arrow = "â–²" if r[2] > 0 else ("â–¼" if r[2] < 0 else "â”€")
                lines.append(f"  | {r[0]} | {r[1]:,.2f} | {arrow}{abs(r[2]):.2f}% | {r[3]} |")
            sections.append("\n".join(lines))
    except Exception:
        pass

    if not sections:
        return "\n\n## ğŸ“¡ ARGOS ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ (ìˆ˜ì§‘ ì¤‘ì´ê±°ë‚˜ ê´€ì‹¬ì¢…ëª© ë¯¸ë“±ë¡)"

    return "".join(sections)


# â”€â”€ [PRICE TRIGGERS] ëª©í‘œê°€/ì†ì ˆ/ìµì ˆ ìë™ ì£¼ë¬¸ â”€â”€

def _register_position_triggers(
    ticker: str, name: str, buy_price: float, qty: int,
    market: str, settings: dict, source_id: str = "",
) -> None:
    """ë§¤ìˆ˜ ì²´ê²° í›„ ìë™ ì†ì ˆ/ìµì ˆ íŠ¸ë¦¬ê±° ë“±ë¡."""
    if buy_price <= 0 or qty <= 0:
        return
    sl_pct = settings.get("default_stop_loss_pct", -5)
    tp_pct = settings.get("default_take_profit_pct", 10)
    stop_price = round(buy_price * (1 + sl_pct / 100))
    take_price = round(buy_price * (1 + tp_pct / 100))
    now_str = datetime.now(KST).isoformat()
    base_id = f"{ticker}_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}"
    new_triggers = [
        {
            "id": f"sl_{base_id}", "ticker": ticker, "name": name,
            "type": "stop_loss", "trigger_price": stop_price, "qty": qty,
            "market": market, "active": True, "created_at": now_str,
            "source": "auto_buy", "source_id": source_id,
            "note": f"ë§¤ìˆ˜ê°€ {buy_price:,.0f} Ã— {1+sl_pct/100:.2f} = {stop_price:,.0f} ì†ì ˆ",
        },
        {
            "id": f"tp_{base_id}", "ticker": ticker, "name": name,
            "type": "take_profit", "trigger_price": take_price, "qty": qty,
            "market": market, "active": True, "created_at": now_str,
            "source": "auto_buy", "source_id": source_id,
            "note": f"ë§¤ìˆ˜ê°€ {buy_price:,.0f} Ã— {1+tp_pct/100:.2f} = {take_price:,.0f} ìµì ˆ",
        },
    ]
    triggers = _load_data("price_triggers", [])
    triggers = new_triggers + triggers
    if len(triggers) > 500:
        triggers = triggers[:500]
    _save_data("price_triggers", triggers)
    save_activity_log(
        "cio_manager",
        f"ğŸ¯ íŠ¸ë¦¬ê±° ë“±ë¡: {name} ì†ì ˆ {stop_price:,.0f} / ìµì ˆ {take_price:,.0f} ({sl_pct}%/{tp_pct}%)",
        "info",
    )


async def _check_price_triggers() -> None:
    """1ë¶„ë§ˆë‹¤ ê°€ê²© ëª¨ë‹ˆí„°ë§ â†’ ëª©í‘œê°€ ë„ë‹¬ ì‹œ ìë™ ì£¼ë¬¸ ì‹¤í–‰."""
    triggers = _load_data("price_triggers", [])
    active = [t for t in triggers if t.get("active", True)]
    if not active:
        return

    settings = _load_data("trading_settings", _default_trading_settings())
    enable_mock = settings.get("enable_mock", False)
    use_kis = _KIS_AVAILABLE and _kis_configured()
    use_mock_kis = (not use_kis) and enable_mock and _KIS_AVAILABLE and _kis_mock_configured()

    async with _price_cache_lock:
        prices_snapshot = dict(_price_cache)

    triggered_ids: set = set()
    for t in active:
        ticker = t["ticker"]
        if ticker not in prices_snapshot:
            continue
        current_price = prices_snapshot[ticker]["price"]
        tp_val = t["trigger_price"]
        ttype  = t["type"]

        if   ttype == "stop_loss"   and current_price <= tp_val: pass
        elif ttype == "take_profit" and current_price >= tp_val: pass
        elif ttype == "buy_limit"   and current_price <= tp_val: pass
        else: continue

        action    = "buy" if ttype == "buy_limit" else "sell"
        action_kr = "ë§¤ìˆ˜" if action == "buy" else "ë§¤ë„"
        type_kr   = {"stop_loss": "ğŸ”´ ì†ì ˆ", "take_profit": "âœ… ìµì ˆ", "buy_limit": "ğŸ¯ ëª©í‘œë§¤ìˆ˜"}[ttype]
        name      = t.get("name", ticker)
        qty       = t.get("qty", 1)
        market    = t.get("market", "KR")
        is_us     = market == "US"

        save_activity_log(
            "cio_manager",
            f"{type_kr} ë°œë™: {name}({ticker}) í˜„ì¬ê°€ {current_price:,.0f} / ëª©í‘œ {tp_val:,.0f} â†’ {action_kr} {qty}ì£¼",
            "info",
        )
        try:
            order_result = {"success": False, "message": "ë¯¸ì‹¤í–‰", "order_no": ""}
            if use_kis:
                order_result = await (
                    _kis_us_order(ticker, action, qty, price=current_price) if is_us
                    else _kis_order(ticker, action, qty, price=0)
                )
            elif use_mock_kis:
                order_result = await (
                    _kis_mock_us_order(ticker, action, qty, price=current_price) if is_us
                    else _kis_mock_order(ticker, action, qty, price=0)
                )
            else:
                portfolio = _load_data("trading_portfolio", _default_portfolio())
                if action == "sell":
                    holding = next((h for h in portfolio["holdings"] if h["ticker"] == ticker), None)
                    if holding and holding["qty"] >= qty:
                        sell_qty = min(qty, holding["qty"])
                        holding["qty"] -= sell_qty
                        if holding["qty"] == 0:
                            portfolio["holdings"] = [h for h in portfolio["holdings"] if h["ticker"] != ticker]
                        portfolio["cash"] += sell_qty * current_price
                        portfolio["updated_at"] = datetime.now(KST).isoformat()
                        _save_data("trading_portfolio", portfolio)
                        order_result = {"success": True, "order_no": "virtual"}
                elif action == "buy" and portfolio.get("cash", 0) >= current_price * qty:
                    holding = next((h for h in portfolio["holdings"] if h["ticker"] == ticker), None)
                    if holding:
                        old_total = holding["avg_price"] * holding["qty"]
                        holding["qty"] += qty
                        holding["avg_price"] = int((old_total + current_price * qty) / holding["qty"])
                        holding["current_price"] = int(current_price)
                    else:
                        portfolio["holdings"].append({
                            "ticker": ticker, "name": name, "qty": qty,
                            "avg_price": int(current_price), "current_price": int(current_price),
                            "market": market,
                        })
                    portfolio["cash"] -= current_price * qty
                    portfolio["updated_at"] = datetime.now(KST).isoformat()
                    _save_data("trading_portfolio", portfolio)
                    order_result = {"success": True, "order_no": "virtual"}

            if order_result["success"]:
                triggered_ids.add(t["id"])
                mode = "ì‹¤ê±°ë˜" if use_kis else ("ëª¨ì˜íˆ¬ì" if use_mock_kis else "ê°€ìƒ")
                history = _load_data("trading_history", [])
                history.insert(0, {
                    "id": f"trigger_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}",
                    "date": datetime.now(KST).isoformat(),
                    "ticker": ticker, "name": name, "action": action,
                    "qty": qty, "price": current_price, "total": qty * current_price, "pnl": 0,
                    "strategy": f"{type_kr} ìë™ì‹¤í–‰ ({mode})",
                    "status": "executed", "market": market,
                    "order_no": order_result.get("order_no", ""),
                })
                _save_data("trading_history", history)
                save_activity_log(
                    "cio_manager",
                    f"âœ… {type_kr} ìë™{action_kr} ì™„ë£Œ: {name} {qty}ì£¼ @ {current_price:,.0f} ({mode})",
                    "info",
                )
                if action == "buy":
                    _register_position_triggers(ticker, name, current_price, qty, market, settings,
                                                source_id=t["id"])
                # ë°˜ëŒ€ìª½ íŠ¸ë¦¬ê±° ë¹„í™œì„±í™” (ì†ì ˆ ë°œë™ â†’ ìµì ˆ ì œê±°, ìµì ˆ ë°œë™ â†’ ì†ì ˆ ì œê±°)
                pair_prefix = "tp_" if ttype == "stop_loss" else ("sl_" if ttype == "take_profit" else "")
                base_key = t["id"].split("_", 1)[1] if "_" in t["id"] else ""
                if pair_prefix and base_key:
                    for other in triggers:
                        if other.get("active") and other["id"] == f"{pair_prefix}{base_key}":
                            other["active"] = False
            else:
                save_activity_log(
                    "cio_manager",
                    f"âŒ {type_kr} ì£¼ë¬¸ ì‹¤íŒ¨: {name} â€” {order_result.get('message','ì›ì¸ ë¶ˆëª…')[:80]}",
                    "error",
                )
        except Exception as ex:
            save_activity_log(
                "cio_manager",
                f"âŒ {type_kr} íŠ¸ë¦¬ê±° ì˜¤ë¥˜: {name} â€” {str(ex)[:80]}",
                "error",
            )

    if triggered_ids:
        for t in triggers:
            if t["id"] in triggered_ids:
                t["active"] = False
                t["triggered_at"] = datetime.now(KST).isoformat()
        _save_data("price_triggers", triggers)


# â”€â”€ íŠ¸ë ˆì´ë”© CRUD ì—”ë“œí¬ì¸íŠ¸ â†’ handlers/trading_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
# summary, portfolio, strategies, watchlist, prices, chart, order,
# history, signals, decisions (CRUD) ë“±ì€ trading_handler.pyì—ì„œ ì œê³µ


@app.post("/api/trading/signals/generate")
async def generate_trading_signals():
    """íˆ¬ìíŒ€ì¥ì´ ê´€ì‹¬ì¢…ëª©ì„ ë¶„ì„ â†’ ë§¤ë§¤ ì‹œê·¸ë„ ìƒì„±.

    íë¦„:
    1. ì‹œí™©ë¶„ì„ Specialist â†’ ê±°ì‹œê²½ì œ/ì‹œì¥ ë¶„ìœ„ê¸° ë¶„ì„
    2. ì¢…ëª©ë¶„ì„ Specialist â†’ ì¬ë¬´ì œí‘œ/ì‹¤ì /ë°¸ë¥˜ì—ì´ì…˜ ë¶„ì„
    3. ê¸°ìˆ ì ë¶„ì„ Specialist â†’ RSI/MACD/ë³¼ë¦°ì €ë°´ë“œ/ì´í‰ì„  ë¶„ì„
    4. ë¦¬ìŠ¤í¬ê´€ë¦¬ Specialist â†’ ì†ì ˆ/í¬ì§€ì…˜/ë¦¬ìŠ¤í¬ í‰ê°€
    5. CIOê°€ 4ëª… ê²°ê³¼ ì·¨í•© â†’ ì¢…ëª©ë³„ ë§¤ìˆ˜/ë§¤ë„/ê´€ë§ íŒë‹¨
    """
    watchlist = _load_data("trading_watchlist", [])
    strategies = _load_data("trading_strategies", [])
    active_strategies = [s for s in strategies if s.get("active")]

    if not watchlist and not active_strategies:
        return {"success": False, "error": "ê´€ì‹¬ì¢…ëª©ì´ë‚˜ í™œì„± ì „ëµì´ ì—†ìŠµë‹ˆë‹¤"}

    # ì¢…ëª© ì •ë³´ ì •ë¦¬ (í•œêµ­/ë¯¸êµ­ êµ¬ë¶„)
    kr_tickers = [w for w in watchlist if w.get("market", "KR") == "KR"]
    us_tickers = [w for w in watchlist if w.get("market") == "US"]
    tickers_info = ", ".join([f"{w['name']}({w['ticker']})" for w in watchlist])
    strats_info = ", ".join([s["name"] for s in active_strategies[:5]])

    # íˆ¬ì ì„±í–¥ ì •ë³´
    _profile = _get_risk_profile()
    _profile_info = RISK_PROFILES.get(_profile, RISK_PROFILES["balanced"])
    _profile_label = f"{_profile_info['label']} ({_profile})"
    _max_pos = _profile_info["max_position_pct"]["max"]
    _cash_reserve = _profile_info["cash_reserve"]["default"]

    # ì •ëŸ‰ì§€í‘œ ì‚¬ì „ë¶„ì„ (ë³‘ë ¬ ê³„ì‚°)
    _auto_market = "US" if (len(us_tickers) > len(kr_tickers)) else "KR"
    save_activity_log("cio_manager", "ğŸ“ ì •ëŸ‰ì§€í‘œ ì‚¬ì „ê³„ì‚° ì‹œì‘ (ìë™ë§¤ë§¤)...", "info")
    quant_section_auto = await _build_quant_prompt_section(watchlist, _auto_market)

    # ARGOS DB ìˆ˜ì§‘ ë°ì´í„° ì£¼ì… (ìë™ë§¤ë§¤)
    save_activity_log("cio_manager", "ğŸ“¡ ARGOS ìˆ˜ì§‘ ë°ì´í„° ë¡œë”© (ìë™ë§¤ë§¤)...", "info")
    argos_section_auto = await _build_argos_context_section(watchlist, _auto_market)

    # CIOì—ê²Œ ë³´ë‚´ëŠ” ë¶„ì„ ëª…ë ¹
    prompt = f"""[ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ] ê´€ì‹¬ì¢…ëª© ì¢…í•© ë¶„ì„ì„ ìš”ì²­í•©ë‹ˆë‹¤.

## CEO íˆ¬ì ì„±í–¥: {_profile_label} {_profile_info['emoji']}
- ì¢…ëª©ë‹¹ ìµœëŒ€ ë¹„ì¤‘: {_max_pos}%
- í˜„ê¸ˆ ìœ ë³´: {_cash_reserve}%
- ì „ ì¢…ëª© ë¹„ì¤‘ í•©ê³„ â‰¤ {100 - _cash_reserve}% (í˜„ê¸ˆ ìœ ë³´ë¶„ ì œì™¸)
- Kelly Criterion, í˜„ëŒ€ í¬íŠ¸í´ë¦¬ì˜¤ ì´ë¡ , ë¶„ì‚°íˆ¬ì ì›ì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ë¹„ì¤‘ì„ ì‚°ì¶œí•˜ì„¸ìš”

## ê´€ì‹¬ì¢…ëª© ({len(watchlist)}ê°œ)
{tickers_info or 'ì—†ìŒ'}
{f'- í•œêµ­ ì£¼ì‹: {len(kr_tickers)}ê°œ' if kr_tickers else ''}
{f'- ë¯¸êµ­ ì£¼ì‹: {len(us_tickers)}ê°œ' if us_tickers else ''}

## í™œì„± ë§¤ë§¤ ì „ëµ
{strats_info or 'ê¸°ë³¸ ì „ëµ (RSI/MACD ê¸°ë°˜)'}{quant_section_auto}{argos_section_auto}

## ë¶„ì„ ìš”ì²­ì‚¬í•­ (ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ë¶ˆí•„ìš” â€” ìœ„ ì„œë²„ ì œê³µ ë°ì´í„°ë§Œ í™œìš©)
ì•„ë˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
- **ì‹œí™©ë¶„ì„**: ìœ„ ë§¤í¬ë¡œ ì§€í‘œ/ë‰´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œì¥ ë¶„ìœ„ê¸°, ê¸ˆë¦¬/í™˜ìœ¨ ë™í–¥, ì—…ì¢…ë³„ íë¦„ í•´ì„
- **ì¢…ëª©ë¶„ì„**: ìœ„ ê³µì‹œ/ë‰´ìŠ¤/ì£¼ê°€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¬ë¬´ ê±´ì „ì„±, PER/PBR, ì‹¤ì  ì „ë§ í•´ì„
- **ê¸°ìˆ ì ë¶„ì„**: ìœ„ ì •ëŸ‰ì§€í‘œ(RSI/MACD ë“±)ì™€ ìµœê·¼ ì£¼ê°€ íë¦„ì„ ì¢…í•©í•˜ì—¬ ë°©í–¥ì„± íŒë‹¨
- **ë¦¬ìŠ¤í¬ê´€ë¦¬**: í¬ì§€ì…˜ í¬ê¸° ì ì •ì„±, ì†ì ˆê°€, ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬

## ìµœì¢… ì‚°ì¶œë¬¼ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ ê·¸ëŒ€ë¡œ â€” ì˜ˆì‹œì²˜ëŸ¼ ì •í™•íˆ)
[ì‹œê·¸ë„] ì‚¼ì„±ì „ì (005930) | ë§¤ìˆ˜ | ì‹ ë¢°ë„ 72% | ë¹„ì¤‘ 15% | ëª©í‘œê°€ 85000 | ë°˜ë„ì²´ ìˆ˜ìš” íšŒë³µ + RSI ê³¼ë§¤ë„ êµ¬ê°„
[ì‹œê·¸ë„] ì¹´ì¹´ì˜¤ (035720) | ë§¤ë„ | ì‹ ë¢°ë„ 61% | ë¹„ì¤‘ 0% | ëª©í‘œê°€ 42000 | PER ê³¼ëŒ€í‰ê°€, ê¸ˆë¦¬ ë¯¼ê° ì„¹í„° ì•½ì„¸
[ì‹œê·¸ë„] LGì—ë„ˆì§€ì†”ë£¨ì…˜ (373220) | ê´€ë§ | ì‹ ë¢°ë„ 45% | ë¹„ì¤‘ 5% | ëª©í‘œê°€ 0 | í˜¼ì¡°ì„¸, ë°©í–¥ì„± ë¶ˆëª…í™•

â€» ì‹ ë¢°ë„ëŠ” ì •ëŸ‰ê¸°ì¤€ê°’ Â±20%p ë²”ìœ„ ë‚´ì—ì„œ ê²°ì •. ë°˜ë“œì‹œ 0~100 ìˆ«ì + % ê¸°í˜¸.
â€» ë¹„ì¤‘: í¬íŠ¸í´ë¦¬ì˜¤ ë‚´ í•´ë‹¹ ì¢…ëª© ë¹„ì¤‘(%). ë§¤ë„ ì¢…ëª©ì€ 0%. ì „ ì¢…ëª© ë¹„ì¤‘ í•©ê³„ â‰¤ {100 - _cash_reserve}%.
â€» ëª©í‘œê°€: ë§¤ìˆ˜ ì¢…ëª©ì€ ëª©í‘œ ë§¤ë„ê°€, ë§¤ë„ ì¢…ëª©ì€ ëª©í‘œ ì¬ì§„ì…ê°€, ê´€ë§ì€ 0. ë°˜ë“œì‹œ ìˆ«ìë§Œ (ì‰¼í‘œ ì—†ì´)."""

    if not is_ai_ready():
        # AI ë¯¸ì—°ê²° ì‹œ ë”ë¯¸ ì‹œê·¸ë„
        signals = _load_data("trading_signals", [])
        for w in watchlist[:5]:
            signal = {
                "id": f"sig_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{w['ticker']}",
                "date": datetime.now(KST).isoformat(),
                "ticker": w["ticker"],
                "name": w["name"],
                "market": w.get("market", "KR"),
                "action": "hold",
                "confidence": 50,
                "reason": "AI ë¯¸ì—°ê²° â€” ë¶„ì„ ë¶ˆê°€ (API í‚¤ ë“±ë¡ í•„ìš”)",
                "strategy": "auto",
                "analyzed_by": "system",
            }
            signals.insert(0, signal)
        if len(signals) > 200:
            signals = signals[:200]
        _save_data("trading_signals", signals)
        return {"success": True, "signals": signals[:20]}

    # CIO + 4ëª… ì „ë¬¸ê°€ì—ê²Œ ìœ„ì„ (ì‹¤ì œ ë„êµ¬ ì‚¬ìš© + ë³‘ë ¬ ë¶„ì„)
    save_activity_log("cio_manager", f"ğŸ“Š ìë™ë§¤ë§¤ ì‹œê·¸ë„ ìƒì„± â€” {len(watchlist)}ê°œ ì¢…ëª© ë¶„ì„ ì‹œì‘", "info")

    # 1ë‹¨ê³„: íˆ¬ìíŒ€ì¥ ë…ì ë¶„ì„ + ë„êµ¬ í™œìš© (P2-4: ë³‘ë ¬í™”)
    cio_solo_prompt = (
        f"CEO íˆ¬ì ì„±í–¥: {_profile_label}. ê´€ì‹¬ì¢…ëª© ë…ì ë¶„ì„ì„ ì‘ì„±í•˜ì„¸ìš”:\n{tickers_info or 'ì—†ìŒ'}\n\n"
        f"í™œì„± ì „ëµ: {strats_info or 'ê¸°ë³¸ ì „ëµ'}\n\n"
        f"ê° ì¢…ëª©ì— ëŒ€í•´ í˜„ì¬ ì‹œì¥ í™˜ê²½, ì„¹í„° ë™í–¥, ë°¸ë¥˜ì—ì´ì…˜ ê´€ì ì—ì„œ ë…ë¦½ì ìœ¼ë¡œ íŒë‹¨í•˜ê³  "
        f"ë§¤ìˆ˜/ë§¤ë„/ê´€ë§ + í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘(%) + ëª©í‘œê°€ë¥¼ ì œì‹œí•˜ì„¸ìš”. ìµœì¢… ì‚°ì¶œë¬¼ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ:\n"
        f"[ì‹œê·¸ë„] ì‚¼ì„±ì „ì (005930) | ë§¤ìˆ˜ | ì‹ ë¢°ë„ 72% | ë¹„ì¤‘ 15% | ëª©í‘œê°€ 85000 | ë°˜ë„ì²´ ìˆ˜ìš” íšŒë³µ ì‹ í˜¸\n"
        f"[ì‹œê·¸ë„] ì¹´ì¹´ì˜¤ (035720) | ê´€ë§ | ì‹ ë¢°ë„ 48% | ë¹„ì¤‘ 5% | ëª©í‘œê°€ 0 | ë°©í–¥ì„± ë¶ˆëª…í™•\n"
        f"â€» ì‹ ë¢°ë„ëŠ” ì¢…ëª©ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ 0~100 ìˆ«ì + % ê¸°í˜¸. ë¹„ì¤‘ì€ ì „ ì¢…ëª© í•©ê³„ â‰¤ {100 - _cash_reserve}%. ëª©í‘œê°€ëŠ” ìˆ«ìë§Œ."
    )
    cio_soul = _load_agent_prompt("cio_manager")
    cio_solo_model = select_model(cio_solo_prompt, override=_get_model_override("cio_manager"))
    save_activity_log("cio_manager", "ğŸ“Š CIO ë…ì ë¶„ì„ + ì „ë¬¸ê°€ ìœ„ì„ ë³‘ë ¬ ì‹œì‘", "info")
    # CIO ë…ì ë¶„ì„ ì‹œì‘ êµì‹  ë¡œê·¸
    try:
        from db import save_delegation_log as _sdl
        _sdl(sender="íˆ¬ìíŒ€ì¥", receiver="CIO ë…ì ë¶„ì„", message="ì „ë¬¸ê°€ ìœ„ì„ê³¼ ë³‘ë ¬ë¡œ ë…ë¦½ íŒë‹¨ ì‹œì‘", log_type="delegation")
    except Exception as e:
        logger.debug("CIO ìœ„ì„ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: %s", e)

    # CIO ë…ì ë¶„ì„ìš© ë„êµ¬ ë¡œë“œ
    cio_detail = _AGENTS_DETAIL.get("cio_manager", {})
    cio_allowed = cio_detail.get("allowed_tools", [])
    cio_solo_tools = None
    cio_solo_executor = None
    cio_solo_tools_used: list[str] = []
    if cio_allowed:
        cio_schemas = _load_tool_schemas(allowed_tools=cio_allowed)
        if cio_schemas.get("anthropic"):
            cio_solo_tools = cio_schemas["anthropic"]
            async def cio_solo_executor(tool_name: str, tool_input: dict):
                cio_solo_tools_used.append(tool_name)
                pool = _init_tool_pool()
                if pool:
                    return await pool.execute(tool_name, tool_input)
                return {"error": f"ë„êµ¬ í’€ ë¯¸ì´ˆê¸°í™”: {tool_name}"}

    # CIO ë…ì ë¶„ì„ê³¼ ì „ë¬¸ê°€ ìœ„ì„ì„ ë™ì‹œì— ì‹¤í–‰ (asyncio.gather)
    async def _cio_solo_analysis():
        result = await ask_ai(cio_solo_prompt, system_prompt=cio_soul, model=cio_solo_model,
                              tools=cio_solo_tools, tool_executor=cio_solo_executor)
        content = result.get("content", "") if isinstance(result, dict) else ""
        cost = result.get("cost_usd", 0) if isinstance(result, dict) else 0
        # êµì‹  ë¡œê·¸ ê¸°ë¡
        try:
            preview = content[:300] if content else "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
            _sdl(sender="CIO ë…ì ë¶„ì„", receiver="íˆ¬ìíŒ€ì¥", message=preview, log_type="report")
            await _broadcast_comms({"id": f"cio_solo_{datetime.now(KST).strftime('%H%M%S')}", "sender": "CIO ë…ì ë¶„ì„", "receiver": "íˆ¬ìíŒ€ì¥", "message": preview, "log_type": "report", "source": "delegation", "created_at": datetime.now(KST).isoformat()})
        except Exception as e:
            logger.debug("CIO ë…ì ë¶„ì„ êµì‹  ë¡œê·¸ ì‹¤íŒ¨: %s", e)
        return {"content": content, "cost_usd": cost}

    # ë³‘ë ¬ ì‹¤í–‰: CIO ë…ì ë¶„ì„ + ì „ë¬¸ê°€ ìœ„ì„
    await _broadcast_status("cio_manager", "working", 0.1, "íˆ¬ìíŒ€ì¥ ë¶„ì„ ì§„í–‰ ì¤‘...")
    cio_solo_task = _cio_solo_analysis()
    spec_task = _delegate_to_specialists("cio_manager", prompt)
    cio_solo_result, spec_results = await asyncio.gather(cio_solo_task, spec_task)

    cio_solo_content = cio_solo_result.get("content", "")
    cio_solo_cost = cio_solo_result.get("cost_usd", 0)

    # 2ë‹¨ê³„: CIOê°€ ë…ì ë¶„ì„ + ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ì¢…í•©
    spec_parts = []
    spec_cost = 0.0
    for r in (spec_results or []):
        name = r.get("name", r.get("agent_id", "?"))
        if "error" in r:
            spec_parts.append(f"[{name}] ì˜¤ë¥˜: {r['error'][:80]}")
        else:
            spec_parts.append(f"[{name}]\n{r.get('content', 'ì‘ë‹µ ì—†ìŒ')}")
            spec_cost += r.get("cost_usd", 0)

    mgr_name = _AGENT_NAMES.get("cio_manager", "CIO")
    synthesis_prompt = (
        f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì•„ë˜ ë‘ ê°€ì§€ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì‹œê·¸ë„ì„ ê²°ì •í•˜ì„¸ìš”.\n\n"
        f"## CEO ì›ë³¸ ëª…ë ¹\n{prompt}\n\n"
        f"## CIO ë…ì ì‚¬ì „ ë¶„ì„ (ì „ë¬¸ê°€ ë³´ê³ ì„œ ì°¸ê³  ì „ ì‘ì„±í•œ ë…ë¦½ íŒë‹¨)\n"
        f"{cio_solo_content[:1000] if cio_solo_content else 'ë¶„ì„ ì—†ìŒ'}\n\n"
        f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts) + "\n\n"
        f"ìœ„ ë…ì ë¶„ì„ê³¼ ì „ë¬¸ê°€ ë³´ê³ ì„œë¥¼ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ìµœì¢… ì‹œê·¸ë„ì„ ê²°ì •í•˜ì„¸ìš”."
    )
    override = _get_model_override("cio_manager")
    synth_model = select_model(synthesis_prompt, override=override)
    await _broadcast_status("cio_manager", "working", 0.7, "ë…ì ë¶„ì„ + ì „ë¬¸ê°€ ê²°ê³¼ ì¢…í•© ì¤‘...")
    synthesis = await ask_ai(synthesis_prompt, system_prompt=cio_soul, model=synth_model)
    await _broadcast_status("cio_manager", "done", 1.0, "ë³´ê³  ì™„ë£Œ")

    specialists_used = len([r for r in (spec_results or []) if "error" not in r])
    if "error" in synthesis:
        content = f"**{mgr_name} ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼**\n\n" + "\n\n---\n\n".join(spec_parts)
    else:
        content = synthesis.get("content", "")
    cost = spec_cost + cio_solo_cost + synthesis.get("cost_usd", 0)

    # CIO ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹œê·¸ë„ íŒŒì‹±
    parsed_signals = _parse_cio_signals(content, watchlist)

    signals = _load_data("trading_signals", [])
    new_signal = {
        "id": f"sig_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}",
        "date": datetime.now(KST).isoformat(),
        "analysis": content,
        "tickers": [w["ticker"] for w in watchlist],
        "parsed_signals": parsed_signals,
        "strategy": "cio_analysis",
        "analyzed_by": f"CIO í¬í•¨ {specialists_used + 1}ëª…",
        "cost_usd": cost,
    }
    signals.insert(0, new_signal)
    if len(signals) > 200:
        signals = signals[:200]
    _save_data("trading_signals", signals)

    buy_count = len([s for s in parsed_signals if s.get("action") == "buy"])
    sell_count = len([s for s in parsed_signals if s.get("action") == "sell"])
    save_activity_log("cio_manager",
        f"ğŸ“Š CIO ì‹œê·¸ë„ ì™„ë£Œ: {len(watchlist)}ê°œ ì¢…ëª© (ë§¤ìˆ˜ {buy_count}, ë§¤ë„ {sell_count}, ë¹„ìš© ${cost:.4f})",
        "info")

    # CIO ì„±ê³¼ ì¶”ì : ì˜ˆì¸¡ì„ cio_predictions í…Œì´ë¸”ì— ì €ì¥
    try:
        from db import save_cio_prediction
        sig_id = new_signal["id"]
        for sig in parsed_signals:
            action_raw = sig.get("action", "hold")
            if action_raw in ("buy", "sell"):
                direction = "BUY" if action_raw == "buy" else "SELL"
                # í˜„ì¬ê°€ ì¡°íšŒ (ê²€ì¦ ê¸°ì¤€ê°€ â€” 3ì¼/7ì¼ í›„ ë¹„êµìš©)
                current_price = 0
                try:
                    from kis_client import get_overseas_price as _gop
                    _pd = await _gop(sig["ticker"])
                    current_price = int(float(_pd.get("price", 0) or 0))
                except Exception as e:
                    logger.debug("í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨ (%s): %s", sig.get("ticker"), e)
                save_cio_prediction(
                    ticker=sig.get("ticker", ""),
                    direction=direction,
                    ticker_name=sig.get("name", ""),
                    confidence=sig.get("confidence", 0),
                    predicted_price=current_price or None,
                    target_price=sig.get("target_price"),
                    analysis_summary=sig.get("reason", ""),
                    task_id=sig_id,
                )
        logger.info("[CIOì„±ê³¼] %dê±´ ì˜ˆì¸¡ ì €ì¥ ì™„ë£Œ (sig_id=%s)", len([s for s in parsed_signals if s.get("action") in ("buy", "sell")]), sig_id)
    except Exception as e:
        logger.warning("[CIOì„±ê³¼] ì˜ˆì¸¡ ì €ì¥ ì‹¤íŒ¨: %s", e)

    # ì‹ ë¢°ë„ íŒŒì´í”„ë¼ì¸: ì „ë¬¸ê°€ ê¸°ì—¬ ìº¡ì²˜
    _capture_specialist_contributions_sync(
        parsed_signals, spec_results or [], cio_solo_content or "", sig_id if 'sig_id' in dir() else ""
    )

    # P2-7: CIO ëª©í‘œê°€ â†’ ê´€ì‹¬ì¢…ëª© ìë™ ë°˜ì˜
    try:
        _wl = _load_data("trading_watchlist", [])
        _updated = 0
        for sig in parsed_signals:
            tp = sig.get("target_price", 0)
            if not tp or tp <= 0:
                continue
            for w in _wl:
                if w.get("ticker") == sig.get("ticker"):
                    w["target_price"] = tp
                    _updated += 1
                    break
        if _updated > 0:
            _save_data("trading_watchlist", _wl)
            logger.info("[P2-7] ê´€ì‹¬ì¢…ëª© ëª©í‘œê°€ %dê±´ ìë™ ê°±ì‹ ", _updated)
    except Exception as e:
        logger.warning("[P2-7] ê´€ì‹¬ì¢…ëª© ëª©í‘œê°€ ë°˜ì˜ ì‹¤íŒ¨: %s", e)

    # ê¸°ë°€ë¬¸ì„œ ìë™ ì €ì¥ (CIO ë…ìë¶„ì„ + ì „ì²´ ë¶„ì„ í¬í•¨)
    try:
        now_str = datetime.now(KST).strftime("%Y-%m-%d %H:%M")
        archive_lines = [f"# CIO ë§¤ë§¤ ì‹œê·¸ë„ ë¶„ì„ â€” {now_str}\n"]
        # CIO ë…ì ë¶„ì„ ë‚´ìš© í¬í•¨
        if cio_solo_content:
            archive_lines.append("## CIO ë…ì ì‚¬ì „ ë¶„ì„ (ì „ë¬¸ê°€ ë³´ê³  ì „ ë…ë¦½ íŒë‹¨)\n")
            archive_lines.append(cio_solo_content[:2000])
            archive_lines.append("\n---\n")
        # CIO ìµœì¢… ì¢…í•© ë¶„ì„ ì „ë¬¸
        archive_lines.append("## CIO ìµœì¢… ì¢…í•© ë¶„ì„\n")
        archive_lines.append(content[:3000] if content else "ë¶„ì„ ë‚´ìš© ì—†ìŒ")
        archive_lines.append("\n---\n")
        # ì¢…ëª©ë³„ ì‹œê·¸ë„ ìš”ì•½
        archive_lines.append("## ì¢…ëª©ë³„ ì‹œê·¸ë„ ìš”ì•½\n")
        for sig in parsed_signals:
            ticker = sig.get("ticker", "")
            name = sig.get("name", ticker)
            action_raw = sig.get("action", "hold")
            action_label = "ë§¤ìˆ˜" if action_raw == "buy" else ("ë§¤ë„" if action_raw == "sell" else "ê´€ë§")
            conf = sig.get("confidence", 0)
            reason = sig.get("reason", "")
            archive_lines.append(f"### {name} ({ticker}) â€” {action_label}")
            archive_lines.append(f"- ì‹ ë¢°ë„: {conf}%")
            archive_lines.append(f"- ë¶„ì„: {reason}\n")
        if len(parsed_signals) == 0:
            archive_lines.append("### ì¢…ëª©ë³„ ì‹œê·¸ë„ íŒŒì‹± ê²°ê³¼ ì—†ìŒ\n")
            archive_lines.append(content[:2000] if content else "")
        archive_content = "\n".join(archive_lines)
        filename = f"CIO_ì‹œê·¸ë„_{datetime.now(KST).strftime('%Y%m%d_%H%M')}.md"
        save_archive(
            division="finance",
            filename=filename,
            content=archive_content,
            agent_id="cio_manager",
        )
    except Exception as e:
        logger.debug("CIO ì•„ì¹´ì´ë¸Œ ì €ì¥ ì‹¤íŒ¨: %s", e)

    # ë§¤ë§¤ ê²°ì • ì¼ì§€ ì €ì¥
    _save_decisions(parsed_signals)

    return {"success": True, "signal": new_signal, "parsed_signals": parsed_signals}


def _save_decisions(parsed_signals: list) -> None:
    """ì‹œê·¸ë„ì„ ë§¤ë§¤ ê²°ì • ì¼ì§€(trading_decisions)ì— ì €ì¥í•©ë‹ˆë‹¤.

    P2-1 ìˆ˜ì •: ìˆ˜ë™ ë¶„ì„(run_trading_now), ìë™ë´‡(_trading_bot_loop),
    ìŠ¤ì¼€ì¤„ ë¶„ì„(generate_trading_signals) ëª¨ë‘ì—ì„œ í˜¸ì¶œ.
    """
    try:
        decisions = load_setting("trading_decisions", [])
        for sig in parsed_signals:
            action_raw = sig.get("action", "hold")
            action_label = "ë§¤ìˆ˜" if action_raw == "buy" else ("ë§¤ë„" if action_raw == "sell" else "ê´€ë§")
            decision = {
                "id": str(_uuid.uuid4()),
                "created_at": datetime.now(KST).isoformat(),
                "ticker": sig.get("ticker", ""),
                "ticker_name": sig.get("name", sig.get("ticker", "")),
                "action": action_label,
                "confidence": sig.get("confidence", 0),
                "reason": sig.get("reason", ""),
                "expert_opinions": sig.get("expert_opinions", []),
                "executed": False,
            }
            decisions.append(decision)
        if len(decisions) > 50:
            decisions = decisions[-50:]
        save_setting("trading_decisions", decisions)
    except Exception as e:
        logger.debug("ë§¤ë§¤ ê²°ì • ì €ì¥ ì‹¤íŒ¨: %s", e)


def _cio_confidence_weight(confidence: float) -> float:
    """CIO ì‹ ë¢°ë„ ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ë¹„ì¤‘ í´ë°± (CIOê°€ ë¹„ì¤‘ì„ ì‚°ì¶œí•˜ì§€ ì•Šì€ ê²½ìš°).
    75%+ â†’ 20%, 65%+ â†’ 15%, 55%+ â†’ 10%, ê¸°íƒ€ â†’ 5%
    """
    if confidence >= 75:
        return 0.20
    elif confidence >= 65:
        return 0.15
    elif confidence >= 55:
        return 0.10
    return 0.05


def _get_signal_weight(sig: dict, fallback_conf: float = 50) -> float:
    """ì‹œê·¸ë„ì—ì„œ ë¹„ì¤‘(0~1 ë¹„ìœ¨)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. CIO ë¹„ì¤‘ ìš°ì„ , ì—†ìœ¼ë©´ ì‹ ë¢°ë„ ê¸°ë°˜ í´ë°±."""
    w = sig.get("weight", 0)
    if w and w > 0:
        return w / 100.0
    return _cio_confidence_weight(fallback_conf)


def _parse_cio_signals(content: str, watchlist: list) -> list:
    """CIO ë¶„ì„ ê²°ê³¼ì—ì„œ ì¢…ëª©ë³„ ë§¤ìˆ˜/ë§¤ë„/ê´€ë§ ì‹œê·¸ë„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    import re
    parsed = []
    seen_tickers = set()

    # [ì‹œê·¸ë„] íŒ¨í„´ â€” ë¹„ì¤‘ + ëª©í‘œê°€ í¬í•¨ (ìµœì‹  í˜•ì‹)
    # ì˜ˆ: [ì‹œê·¸ë„] ì‚¼ì„±ì „ì (005930) | ë§¤ìˆ˜ | ì‹ ë¢°ë„ 72% | ë¹„ì¤‘ 15% | ëª©í‘œê°€ 85000 | ì´ìœ 
    pattern = r'\[ì‹œê·¸ë„\]\s*(.+?)\s*[\(ï¼ˆ]([A-Za-z0-9]+)[\)ï¼‰]\s*\|\s*[^\|]*?(ë§¤ìˆ˜|ë§¤ë„|ê´€ë§|buy|sell|hold)\b[^\|]*\|\s*(?:ì‹ ë¢°ë„[:\s]*)?\s*(\d+)\s*%?\s*\|\s*(?:ë¹„ì¤‘\s*(\d+)\s*%?\s*\|\s*)?(?:ëª©í‘œê°€\s*(\d+)\s*\|\s*)?(.*)'
    matches = re.findall(pattern, content, re.IGNORECASE)

    # ê¸°ì¡´ í˜•ì‹ (ë¹„ì¤‘/ëª©í‘œê°€ ì—†ëŠ” ê²ƒ) í˜¸í™˜ìš© í´ë°±
    if not matches:
        pattern_legacy = r'\[ì‹œê·¸ë„\]\s*(.+?)\s*[\(ï¼ˆ]([A-Za-z0-9]+)[\)ï¼‰]\s*\|\s*[^\|]*?(ë§¤ìˆ˜|ë§¤ë„|ê´€ë§|buy|sell|hold)\b[^\|]*\|\s*(?:ì‹ ë¢°ë„[:\s]*)?\s*(\d+)\s*%?\s*\|?\s*()()(.*)'
        matches = re.findall(pattern_legacy, content, re.IGNORECASE)

    for name, ticker, action, confidence, weight_str, target_price_str, reason in matches:
        ticker = ticker.strip()
        if ticker in seen_tickers:
            continue  # ê°™ì€ ì¢…ëª© ì¤‘ë³µ ì‹œê·¸ë„ ë°©ì§€ (ìš”ì•½ ì„¹ì…˜ ì¤‘ë³µ)
        seen_tickers.add(ticker)
        action_map = {"ë§¤ìˆ˜": "buy", "ë§¤ë„": "sell", "ê´€ë§": "hold", "buy": "buy", "sell": "sell", "hold": "hold"}
        market = "US" if any(c.isalpha() and c.isupper() for c in ticker) and not ticker.isdigit() else "KR"
        # ì´ìœ ê°€ ë¹ˆ ì¤„ì´ë©´ ì‹œê·¸ë„ ë‹¤ìŒ ì¤„ì—ì„œ ì¶”ì¶œ
        reason_text = reason.strip()
        if not reason_text:
            sig_pos = content.find(f"[ì‹œê·¸ë„] {name.strip()}")
            if sig_pos >= 0:
                after = content[sig_pos:sig_pos + 500]
                lines = after.split("\n")
                for line in lines[1:4]:  # ë‹¤ìŒ 1~3ì¤„ì—ì„œ ì´ìœ  ì°¾ê¸°
                    line = line.strip()
                    if line and not line.startswith("[ì‹œê·¸ë„]") and not line.startswith("â”"):
                        reason_text = line
                        break
        parsed.append({
            "ticker": ticker,
            "name": name.strip(),
            "market": market,
            "action": action_map.get(action.lower(), "hold"),
            "confidence": int(confidence),
            "weight": int(weight_str) if weight_str and weight_str.isdigit() else 0,
            "target_price": int(target_price_str) if target_price_str and target_price_str.isdigit() else 0,
            "reason": reason_text or "CIO ì¢…í•© ë¶„ì„ ì°¸ì¡°",
        })

    # ë¹„ì¤‘ ì•ˆì „ì¥ì¹˜: ì¢…ëª©ë‹¹ ìµœëŒ€ ë¹„ì¤‘ + ì´í•© ì œí•œ (íˆ¬ì ì„±í–¥ ê¸°ë°˜)
    if parsed:
        _profile = _get_risk_profile()
        _ranges = RISK_PROFILES.get(_profile, RISK_PROFILES["balanced"])
        _max_pos = _ranges["max_position_pct"]["max"]
        _cash_reserve = _ranges["cash_reserve"]["default"]
        _max_total = 100 - _cash_reserve
        # ì¢…ëª©ë‹¹ í´ë¨í•‘
        for sig in parsed:
            if sig["weight"] > _max_pos:
                sig["weight"] = _max_pos
        # ì´í•© ì œí•œ
        total_weight = sum(s["weight"] for s in parsed)
        if total_weight > _max_total and total_weight > 0:
            ratio = _max_total / total_weight
            for sig in parsed:
                sig["weight"] = max(1, int(sig["weight"] * ratio))

    # [ì‹œê·¸ë„] íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê´€ì‹¬ì¢…ëª© ê¸°ë°˜ìœ¼ë¡œ í‚¤ì›Œë“œ íŒŒì‹± (ì¢…ëª©ë³„ ê°œë³„ ì»¨í…ìŠ¤íŠ¸ ê¸°ì¤€)
    if not parsed:
        for w in watchlist:
            action = "hold"
            confidence = 50
            reason = ""
            name = w.get("name", w["ticker"])
            ticker = w["ticker"]
            # ì´ ì¢…ëª©ì´ ë³´ê³ ì„œì— ì–¸ê¸‰ëëŠ”ì§€ í™•ì¸
            name_idx = content.find(name)
            ticker_idx = content.find(ticker)
            ref_idx = name_idx if name_idx >= 0 else ticker_idx
            if ref_idx < 0:
                continue  # ì–¸ê¸‰ ì•ˆ ëœ ì¢…ëª©ì€ ì œì™¸
            # í•´ë‹¹ ì¢…ëª© ì£¼ë³€ 300ìë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš© (ì „ì²´ ë³´ê³ ì„œ X)
            ctx = content[ref_idx:ref_idx + 300]
            if any(k in ctx for k in ["ë§¤ìˆ˜", "ì ê·¹ ë§¤ìˆ˜", "buy", "ì§„ì…"]):
                action = "buy"
            elif any(k in ctx for k in ["ë§¤ë„", "sell", "ì²­ì‚°", "ìµì ˆ"]):
                action = "sell"
            # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‹ ë¢°ë„ ìˆ«ì ì¶”ì¶œ (ì˜ˆ: "ì‹ ë¢°ë„ 72%" / "72%")
            conf_match = re.search(r'ì‹ ë¢°ë„[:\s]*(\d+)\s*%?', ctx)
            if conf_match:
                confidence = int(conf_match.group(1))
            else:
                pct_match = re.search(r'(\d{2,3})\s*%', ctx)
                if pct_match:
                    confidence = int(pct_match.group(1))
            # ê·¼ê±° ì¶”ì¶œ
            reason = ctx.split("\n")[0].strip()
            parsed.append({
                "ticker": ticker,
                "name": name,
                "market": w.get("market", "KR"),
                "action": action,
                "confidence": confidence,
                "reason": reason or "CIO ì¢…í•© ë¶„ì„ ì°¸ì¡°",
            })

    return parsed


# â”€â”€ settings, risk-profile, cio-update â†’ handlers/trading_handler.pyë¡œ ë¶„ë¦¬ â”€â”€

@app.post("/api/trading/bot/toggle")
async def toggle_trading_bot():
    """ìë™ë§¤ë§¤ ë´‡ ON/OFF í† ê¸€."""


    app_state.trading_bot_active = not app_state.trading_bot_active
    # DBì— ìƒíƒœ ì €ì¥ â†’ ë°°í¬/ì¬ì‹œì‘ í›„ì—ë„ ìœ ì§€
    save_setting("trading_bot_active", app_state.trading_bot_active)

    if app_state.trading_bot_active:
        if app_state.trading_bot_task is None or app_state.trading_bot_task.done():
            app_state.trading_bot_task = asyncio.create_task(_trading_bot_loop())
        save_activity_log("system", "ğŸ¤– ìë™ë§¤ë§¤ ë´‡ ê°€ë™ ì‹œì‘!", "info")
        _log("[TRADING] ìë™ë§¤ë§¤ ë´‡ ì‹œì‘ âœ…")
    else:
        save_activity_log("system", "â¹ï¸ ìë™ë§¤ë§¤ ë´‡ ì¤‘ì§€", "info")
        _log("[TRADING] ìë™ë§¤ë§¤ ë´‡ ì¤‘ì§€")

    return {"success": True, "bot_active": app_state.trading_bot_active}


# â”€â”€ bot/status, calibration â†’ handlers/trading_handler.pyë¡œ ë¶„ë¦¬ â”€â”€

@app.post("/api/trading/watchlist/analyze-selected")
async def analyze_selected_watchlist(request: Request):
    """ê´€ì‹¬ì¢…ëª© ì¤‘ ì„ íƒí•œ ì¢…ëª©ë§Œ ì¦‰ì‹œ ë¶„ì„ + ìë™ë§¤ë§¤."""
    body = await request.json()
    tickers = body.get("tickers", [])
    if not tickers:
        return {"success": False, "message": "ë¶„ì„í•  ì¢…ëª©ì„ ì„ íƒí•˜ì„¸ìš”."}

    existing = app_state.bg_tasks.get("trading_run_now")
    if existing and not existing.done():
        return {"success": True, "message": "CIO ë¶„ì„ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.", "already_running": True}

    async def _bg():
        try:
            result = await _run_trading_now_inner(selected_tickers=tickers)
            app_state.bg_results["trading_run_now"] = {**result, "_completed_at": __import__("time").time()}
        except Exception as e:
            logger.error("[ì„ íƒ ë¶„ì„] ë°±ê·¸ë¼ìš´ë“œ ì˜¤ë¥˜: %s", e, exc_info=True)
            app_state.bg_results["trading_run_now"] = {
                "success": False, "message": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)[:200]}",
                "signals": [], "signals_count": 0, "orders_triggered": 0,
                "_completed_at": __import__("time").time(),
            }
        finally:
            result = app_state.bg_results.get("trading_run_now", {})
            await wm.broadcast({"type": "trading_run_complete",
                "success": result.get("success", False),
                "signals_count": result.get("signals_count", 0),
                "orders_triggered": result.get("orders_triggered", 0)})

    app_state.bg_tasks["trading_run_now"] = asyncio.create_task(_bg())
    return {"success": True, "message": f"{len(tickers)}ê°œ ì¢…ëª© ë¶„ì„ ì‹œì‘ë¨.", "background": True}


@app.post("/api/trading/bot/run-now")
async def run_trading_now():
    """ì§€ê¸ˆ ì¦‰ì‹œ CIO ë¶„ì„ + ë§¤ë§¤ íŒë‹¨ ì‹¤í–‰ (ì¥ ì‹œê°„ ë¬´ê´€, ìˆ˜ë™ íŠ¸ë¦¬ê±°).

    ë´‡ ON/OFF ìƒíƒœì™€ ë¬´ê´€í•˜ê²Œ ì¦‰ì‹œ 1íšŒ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ìˆ˜ë™ ì‹¤í–‰ì´ë¯€ë¡œ auto_execute ì„¤ì • ë¬´ê´€í•˜ê²Œ í•­ìƒ ë§¤ë§¤ê¹Œì§€ ì§„í–‰í•©ë‹ˆë‹¤.

    Cloudflare 100ì´ˆ íƒ€ì„ì•„ì›ƒ ëŒ€ì‘: ì¦‰ì‹œ ì‘ë‹µ + ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰.
    í”„ë¡ íŠ¸ì—”ë“œëŠ” CIO SSE + í´ë§ìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶”ì .
    """
    # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ì¤‘ë³µ ë°©ì§€
    existing = app_state.bg_tasks.get("trading_run_now")
    if existing and not existing.done():
        return {"success": True, "message": "CIO ë¶„ì„ì´ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.", "already_running": True}

    async def _bg_run_trading():
        try:
            result = await _run_trading_now_inner()
            app_state.bg_results["trading_run_now"] = {
                **result, "_completed_at": __import__("time").time()
            }
        except Exception as e:
            logger.error("[ìˆ˜ë™ ë¶„ì„] ë°±ê·¸ë¼ìš´ë“œ ì˜¤ë¥˜: %s", e, exc_info=True)
            signals = _load_data("trading_signals", [])
            latest = signals[0] if signals else {}
            app_state.bg_results["trading_run_now"] = {
                "success": False,
                "message": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)[:200]}",
                "signals": latest.get("parsed_signals", []),
                "signals_count": len(latest.get("parsed_signals", [])),
                "orders_triggered": 0,
                "error": str(e)[:200],
                "_completed_at": __import__("time").time(),
            }
        finally:
            # ì™„ë£Œ ì•Œë¦¼ ë¸Œë¡œë“œìºìŠ¤íŠ¸
            result = app_state.bg_results.get("trading_run_now", {})
            await wm.broadcast({
                "type": "trading_run_complete",
                "success": result.get("success", False),
                "signals_count": result.get("signals_count", 0),
                "orders_triggered": result.get("orders_triggered", 0),
            })

    app_state.bg_tasks["trading_run_now"] = asyncio.create_task(_bg_run_trading())
    return {"success": True, "message": "CIO ë¶„ì„ ì‹œì‘ë¨. ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì€ í™”ë©´ì—ì„œ í™•ì¸í•˜ì„¸ìš”.", "background": True}


@app.get("/api/trading/bot/run-status")
async def get_trading_run_status():
    """ë°±ê·¸ë¼ìš´ë“œ CIO ë¶„ì„ ì§„í–‰ ìƒíƒœ í™•ì¸."""
    task = app_state.bg_tasks.get("trading_run_now")
    result = app_state.bg_results.get("trading_run_now")

    if task and not task.done():
        return {"status": "running", "message": "CIO ë¶„ì„ ì§„í–‰ ì¤‘..."}
    elif result:
        return {"status": "completed", **result}
    else:
        return {"status": "idle", "message": "ì‹¤í–‰ ëŒ€ê¸° ì¤‘"}


@app.post("/api/trading/bot/stop")
async def stop_trading_now():
    """ì§„í–‰ ì¤‘ì¸ CIO ë¶„ì„ì„ ì¦‰ì‹œ ì¤‘ì§€í•©ë‹ˆë‹¤."""
    task = app_state.bg_tasks.get("trading_run_now")
    if task and not task.done():
        task.cancel()
        save_activity_log("cio_manager", "ğŸ›‘ CEOê°€ ìˆ˜ë™ìœ¼ë¡œ ë¶„ì„ì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.", "info")
        await wm.broadcast({"type": "trading_run_complete", "success": False, "stopped": True, "signals_count": 0, "orders_triggered": 0})
        return {"success": True, "message": "ë¶„ì„ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."}
    return {"success": False, "message": "ì§„í–‰ ì¤‘ì¸ ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤."}


async def _run_trading_now_inner(selected_tickers: list[str] | None = None):
    """run_trading_nowì˜ ì‹¤ì œ ë¡œì§ (ì—ëŸ¬ í•¸ë“¤ë§ì€ í˜¸ì¶œìê°€ ë‹´ë‹¹).

    selected_tickers: ì§€ì • ì‹œ í•´ë‹¹ ì¢…ëª©ë§Œ ë¶„ì„. Noneì´ë©´ ì „ì²´ ê´€ì‹¬ì¢…ëª©.
    """
    settings = _load_data("trading_settings", _default_trading_settings())
    watchlist = _load_data("trading_watchlist", [])

    if not watchlist:
        return {"success": False, "message": "ê´€ì‹¬ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¢…ëª©ì„ ì¶”ê°€í•˜ì„¸ìš”."}

    # ì¥ ì‹œê°„ í™•ì¸ (ìˆ˜ë™ ì‹¤í–‰ì€ ê°•ì œ ì‹¤í–‰ â€” ì¥ ë§ˆê°ì´ì–´ë„ ì§„í–‰)
    is_open, market = _is_market_open(settings)
    if not is_open:
        market = "KR"  # ì¥ ë§ˆê° ì‹œ í•œêµ­ì¥ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„
    market_watchlist = [w for w in watchlist if w.get("market", "KR") == market] or watchlist

    # ì„ íƒ ì¢…ëª© í•„í„°ë§ (selected_tickers ì§€ì • ì‹œ)
    if selected_tickers:
        upper_sel = [t.upper() for t in selected_tickers]
        market_watchlist = [w for w in watchlist if w.get("ticker", "").upper() in upper_sel]
        if not market_watchlist:
            return {"success": False, "message": f"ì„ íƒí•œ ì¢…ëª©({', '.join(selected_tickers)})ì´ ê´€ì‹¬ì¢…ëª©ì— ì—†ìŠµë‹ˆë‹¤."}
        # ì„ íƒ ì¢…ëª©ì˜ ë§ˆì¼“ ìë™ ê²°ì •
        markets = set(w.get("market", "KR") for w in market_watchlist)
        market = "US" if "US" in markets else "KR"

    # ìê¸°í•™ìŠµ ë³´ì • ì„¹ì…˜ (ë² ì´ì§€ì•ˆ + ELO + ì˜¤ë‹µíŒ¨í„´ + Platt Scaling í†µí•©)
    cal_section = _build_calibration_prompt_section(settings)

    # ì •ëŸ‰ì§€í‘œ ì‚¬ì „ë¶„ì„ (RSI/MACD/ë³¼ë¦°ì €/ê±°ë˜ëŸ‰/ì¶”ì„¸ â€” ë³‘ë ¬ ê³„ì‚°)
    save_activity_log("cio_manager", "ğŸ“ ì •ëŸ‰ì§€í‘œ ì‚¬ì „ê³„ì‚° ì‹œì‘...", "info")
    quant_section = await _build_quant_prompt_section(market_watchlist, market)

    # ARGOS DB ìˆ˜ì§‘ ë°ì´í„° ì£¼ì… (ì£¼ê°€/ë§¤í¬ë¡œ/ê³µì‹œ/ë‰´ìŠ¤ â€” ì„œë²„ê°€ ì§ì ‘ ì œê³µ)
    save_activity_log("cio_manager", "ğŸ“¡ ARGOS ìˆ˜ì§‘ ë°ì´í„° ë¡œë”©...", "info")
    argos_section = await _build_argos_context_section(market_watchlist, market)

    tickers_info = ", ".join([f"{w['name']}({w['ticker']})" for w in market_watchlist])
    strategies = _load_data("trading_strategies", [])
    active_strats = [s for s in strategies if s.get("active")]
    strats_info = ", ".join([s["name"] for s in active_strats[:5]]) or "ê¸°ë³¸ ì „ëµ"

    market_label = "í•œêµ­" if market == "KR" else "ë¯¸êµ­"
    prompt = f"""[ìˆ˜ë™ ì¦‰ì‹œ ë¶„ì„ ìš”ì²­ â€” {market_label}ì¥]

## ë¶„ì„ ëŒ€ìƒ ({len(market_watchlist)}ê°œ ì¢…ëª©)
{tickers_info}

## í™œì„± ì „ëµ: {strats_info}{cal_section}{quant_section}{argos_section}

## ë¶„ì„ ìš”ì²­ (ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ë¶ˆí•„ìš” â€” ìœ„ ì„œë²„ ì œê³µ ë°ì´í„°ë§Œ í™œìš©)
ì•„ë˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
- **ì‹œí™©ë¶„ì„**: ìœ„ ë§¤í¬ë¡œ ì§€í‘œ/ë‰´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ {'ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ íë¦„, ì™¸êµ­ì¸/ê¸°ê´€ ë™í–¥, ê¸ˆë¦¬/í™˜ìœ¨' if market == 'KR' else 'S&P500/ë‚˜ìŠ¤ë‹¥, ë¯¸êµ­ ê¸ˆë¦¬/ê³ ìš©ì§€í‘œ, ë‹¬ëŸ¬ ê°•ì„¸'} í•´ì„
- **ì¢…ëª©ë¶„ì„**: ìœ„ ê³µì‹œ/ë‰´ìŠ¤/ì£¼ê°€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¬ë¬´ ê±´ì „ì„±, PER/PBR, ì‹¤ì  ë°©í–¥ í•´ì„
- **ê¸°ìˆ ì ë¶„ì„**: ìœ„ ì •ëŸ‰ì§€í‘œ(RSI/MACD ë“±)ì™€ ì£¼ê°€ íë¦„ì„ ì¢…í•©í•˜ì—¬ ë°©í–¥ì„± íŒë‹¨
- **ë¦¬ìŠ¤í¬ê´€ë¦¬**: ì†ì ˆê°€, ì ì • í¬ì§€ì…˜ í¬ê¸°, ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬

## ìµœì¢… ì‚°ì¶œë¬¼ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ ê·¸ëŒ€ë¡œ â€” ì˜ˆì‹œì²˜ëŸ¼ ì •í™•íˆ)
[ì‹œê·¸ë„] ì‚¼ì„±ì „ì (005930) | ë§¤ìˆ˜ | ì‹ ë¢°ë„ 72% | ë¹„ì¤‘ 15% | ëª©í‘œê°€ 78000 | ë°˜ë„ì²´ ìˆ˜ìš” íšŒë³µ + RSI ê³¼ë§¤ë„ êµ¬ê°„
[ì‹œê·¸ë„] ì¹´ì¹´ì˜¤ (035720) | ë§¤ë„ | ì‹ ë¢°ë„ 61% | ë¹„ì¤‘ 10% | ëª©í‘œê°€ 0 | PER ê³¼ëŒ€í‰ê°€, ê¸ˆë¦¬ ë¯¼ê° ì„¹í„° ì•½ì„¸
[ì‹œê·¸ë„] LGì—ë„ˆì§€ì†”ë£¨ì…˜ (373220) | ê´€ë§ | ì‹ ë¢°ë„ 45% | ë¹„ì¤‘ 0% | ëª©í‘œê°€ 390000 | í˜¼ì¡°ì„¸, ì´ ê°€ê²© ë„ë‹¬ ì‹œ ì§„ì… ê²€í† 

â€» ì£¼ì˜:
- ì‹ ë¢°ë„ëŠ” ìœ„ ì •ëŸ‰ê¸°ì¤€ê°’ Â±20%p ë²”ìœ„ ë‚´ì—ì„œ ê²°ì •. ì¢…ëª©ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ, 0~100 ìˆ«ì + % ê¸°í˜¸ë¡œ í‘œê¸°
- ëª©í‘œê°€(ê¶Œì¥ ë§¤ìˆ˜ ì§„ì…ê°€): ë§¤ìˆ˜/ê´€ë§ ì¢…ëª©ì€ ë°˜ë“œì‹œ ì…ë ¥. í˜„ì¬ê°€ë³´ë‹¤ ë‚®ì€ ëª©í‘œ ì§„ì…ê°€ ì„¤ì •. ë¯¸êµ­ ì£¼ì‹ì€ USD ë‹¨ìœ„. ë§¤ë„ ì¢…ëª©ì€ 0
- ëª©í‘œê°€ ë„ë‹¬ ì‹œ ì„œë²„ê°€ ìë™ìœ¼ë¡œ ë§¤ìˆ˜ ì‹¤í–‰ â€” ì‹ ì¤‘í•˜ê²Œ ì„¤ì •í•  ê²ƒ"""

    save_activity_log("cio_manager", f"ğŸ” ìˆ˜ë™ ì¦‰ì‹œ ë¶„ì„ ì‹œì‘: {market_label}ì¥ {len(market_watchlist)}ê°œ ì¢…ëª©", "info")
    cio_result = await _call_agent("cio_manager", prompt)
    content = cio_result.get("content", "")
    cost = cio_result.get("cost_usd", 0)

    # â”€â”€ ë¹„ì„œì‹¤ì¥ QA: íŒ€ì¥ ë³´ê³ ì„œ ê²€ìˆ˜ â”€â”€
    qa_passed, qa_reason = await _chief_qa_review(content, "ê¸ˆìœµë¶„ì„íŒ€ì¥")
    save_activity_log("chief_of_staff",
        f"ğŸ“‹ ê¸ˆìœµë¶„ì„íŒ€ì¥ ë³´ê³ ì„œ QA: {'âœ… ìŠ¹ì¸' if qa_passed else 'âŒ ë°˜ë ¤'} â€” {qa_reason[:80]}",
        "info" if qa_passed else "warning")
    await _broadcast_comms({
        "type": "comms",
        "agent_id": "chief_of_staff",
        "agent_name": "ë¹„ì„œì‹¤ì¥",
        "message": f"ê¸ˆìœµë¶„ì„íŒ€ì¥ ë³´ê³ ì„œ QA {'âœ… ìŠ¹ì¸' if qa_passed else 'âŒ ë°˜ë ¤'}: {qa_reason[:100]}",
        "timestamp": datetime.now(KST).isoformat(),
        "channel": "cio",
    })

    parsed_signals = _parse_cio_signals(content, market_watchlist)

    # ì‹ í˜¸ ì €ì¥ (QA ê²°ê³¼ í¬í•¨)
    signals = _load_data("trading_signals", [])
    new_signal = {
        "id": f"sig_manual_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}",
        "date": datetime.now(KST).isoformat(),
        "market": market,
        "analysis": content,
        "tickers": [w["ticker"] for w in market_watchlist[:10]],
        "parsed_signals": parsed_signals,
        "strategy": "cio_manual_analysis",
        "analyzed_by": "ê¸ˆìœµë¶„ì„íŒ€ì¥ ë‹¨ë… ë¶„ì„ (ìˆ˜ë™ ì‹¤í–‰)",
        "cost_usd": cost,
        "auto_bot": False,
        "manual_run": True,
        "qa_passed": qa_passed,
        "qa_reason": qa_reason[:200],
    }
    signals.insert(0, new_signal)
    if len(signals) > 200:
        signals = signals[:200]
    _save_data("trading_signals", signals)

    # ë§¤ë§¤ ê²°ì • ì¼ì§€ ì €ì¥ (P2-1: ìˆ˜ë™ ë¶„ì„ì—ì„œë„ decisions ì €ì¥)
    _save_decisions(parsed_signals)

    # QA ë°˜ë ¤ ì‹œ 1íšŒ ì¬ë¶„ì„
    if not qa_passed:
        save_activity_log("chief_of_staff",
            f"ğŸ”„ QA ë°˜ë ¤ â†’ ì¬ë¶„ì„ ìš”ì²­: {qa_reason[:100]}", "warning")
        retry_prompt = (
            f"{prompt}\n\n"
            f"## âš ï¸ ë¹„ì„œì‹¤ì¥ ì¬ê²€í†  ìš”ì²­\n"
            f"ì´ì „ ë³´ê³ ì„œê°€ ë°˜ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤. ë°˜ë ¤ ì‚¬ìœ : {qa_reason[:200]}\n"
            f"ìœ„ ì‚¬ìœ ë¥¼ ë°˜ë“œì‹œ í•´ê²°í•˜ì—¬ ë‹¤ì‹œ ë¶„ì„í•˜ì„¸ìš”. ì‹ ë¢°ë„ ê·¼ê±°ë¥¼ êµ¬ì²´ì  ìˆ˜ì¹˜ë¡œ ë³´ì™„í•˜ì„¸ìš”."
        )
        content2, cost2 = await ask_ai(
            agent_id="cio_manager", prompt=retry_prompt,
            use_tools=True, tools=cio_tools,
        )
        cost += cost2
        qa_passed2, qa_reason2 = await _qa_check_cio_report(content2, market_watchlist)
        save_activity_log("chief_of_staff",
            f"ğŸ“‹ ì¬ë¶„ì„ QA: {'âœ… ìŠ¹ì¸' if qa_passed2 else 'âŒ ìµœì¢… ë°˜ë ¤'} â€” {qa_reason2[:100]}", "info" if qa_passed2 else "warning")
        if qa_passed2:
            content = content2
            parsed_signals = _parse_cio_signals(content, market_watchlist)
            _save_decisions(parsed_signals)
        else:
            return {
                "signals": parsed_signals,
                "analysis": content2[:500],
                "cost_usd": cost,
                "qa_passed": False,
                "qa_reason": qa_reason2,
                "orders": [],
                "message": f"ë¹„ì„œì‹¤ì¥ QA ìµœì¢… ë°˜ë ¤ (ì¬ë¶„ì„ í›„): {qa_reason2[:100]}"
            }

    # ìˆ˜ë™ ì¦‰ì‹œ ì‹¤í–‰ â†’ auto_execute ì„¤ì • ë¬´ê´€í•˜ê²Œ í•­ìƒ ì£¼ë¬¸ ì§„í–‰
    # (CEOê°€ ë²„íŠ¼ì„ ì§ì ‘ ëˆ„ë¥¸ ê²ƒ = ë§¤ë§¤ ì˜ì‚¬ í‘œì‹œ)
    min_confidence = settings.get("min_confidence", 65)
    order_size = settings.get("order_size", 0)  # 0 = CIO ë¹„ì¤‘ ììœ¨, >0 = ê³ ì • ê¸ˆì•¡
    orders_triggered = 0

    # ìê¸°ë³´ì • ê³„ìˆ˜ ê³„ì‚° (Platt Scaling) â€” ë¯¸ì •ì˜ ì‹œ NameError ë°©ì§€
    calibration = _compute_calibration_factor(settings.get("calibration_lookback", 20))
    calibration_factor = calibration.get("factor", 1.0)
    if calibration.get("win_rate") is not None:
        save_activity_log("cio_manager",
            f"ğŸ“Š ìê¸°ë³´ì • ì ìš©: factor={calibration_factor} ({calibration.get('note', '')})", "info")
    if True:  # ìˆ˜ë™ ì‹¤í–‰ì€ í•­ìƒ ë§¤ë§¤ ì§„í–‰ (auto_execute ì²´í¬ ì œê±°)
        # ìˆ˜ë™ ì‹¤í–‰: KISê°€ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´ ì‹¤ì œ ì£¼ë¬¸ (paper_trading ì„¤ì • ë¬´ì‹œ)
        # CEOê°€ "ì¦‰ì‹œ ë¶„ì„Â·ë§¤ë§¤ê²°ì •" ë²„íŠ¼ì„ ëˆ„ë¥¸ ê²ƒ = ë§¤ë§¤ ì˜ì‚¬ ëª…ì‹œì  í‘œì‹œ
        enable_mock = settings.get("enable_mock", False)
        use_kis = _KIS_AVAILABLE and _kis_configured()
        use_mock_kis = (not use_kis) and enable_mock and _KIS_AVAILABLE and _kis_mock_configured()
        paper_mode = not use_kis and not use_mock_kis  # ë‘˜ ë‹¤ ë¶ˆê°€í•  ë•Œë§Œ ê°€ìƒ ëª¨ë“œ

        # CIO ë¹„ì¤‘ ê¸°ë°˜ ë§¤ìˆ˜(Bì•ˆ): order_size=0ì´ë©´ ì”ê³ Ã—ë¹„ì¤‘ìœ¼ë¡œ ìë™ ì‚°ì¶œ
        account_balance = 0
        if order_size == 0:
            try:
                if use_kis:
                    _bal = await _kis_balance()
                    account_balance = _bal.get("cash", 0) if _bal.get("success") else 0
                elif use_mock_kis:
                    _bal = await _kis_mock_balance()
                    account_balance = _bal.get("cash", 0) if _bal.get("success") else 0
                else:
                    _port = _load_data("trading_portfolio", _default_portfolio())
                    account_balance = _port.get("cash", 0)
            except Exception as e:
                logger.debug("ì”ê³  ì¡°íšŒ ì‹¤íŒ¨: %s", e)
            if account_balance <= 0:
                account_balance = 1_000_000
                save_activity_log("cio_manager", "CIO ë¹„ì¤‘ ëª¨ë“œ: ì”ê³  ì¡°íšŒ ì‹¤íŒ¨, ê¸°ë³¸ 100ë§Œì› ì‚¬ìš©", "warning")
            save_activity_log("cio_manager",
                f"CIO ë¹„ì¤‘ ëª¨ë“œ: ê³„ì¢Œì”ê³  {account_balance:,.0f}ì› ê¸°ì¤€ ìë™ ì£¼ìˆ˜ ì‚°ì¶œ", "info")

        mode_label = ("ì‹¤ê±°ë˜" if not KIS_IS_MOCK else "ëª¨ì˜íˆ¬ì") if use_kis else ("ëª¨ì˜íˆ¬ì" if use_mock_kis else "ê°€ìƒ")
        save_activity_log("cio_manager",
            f"ğŸ“‹ ë§¤ë§¤ ì‹¤í–‰ ì‹œì‘: ì‹œê·¸ë„ {len(parsed_signals)}ê±´, ìµœì†Œì‹ ë¢°ë„ {min_confidence}%, order_size={order_size}, KIS={use_kis}, MOCK={use_mock_kis}, ëª¨ë“œ={mode_label}", "info")

        for sig in parsed_signals:
            if sig["action"] not in ("buy", "sell"):
                continue
            effective_conf = sig.get("confidence", 0) * calibration_factor
            if effective_conf < min_confidence:
                save_activity_log("cio_manager",
                    f"[ìˆ˜ë™] {sig.get('name', sig['ticker'])} ì‹ ë¢°ë„ ë¶€ì¡± ({effective_conf:.0f}% < {min_confidence}%) â€” ê±´ë„ˆëœ€",
                    "info")
                continue

            ticker = sig["ticker"]
            sig_market = sig.get("market", market)
            is_us = sig_market.upper() in ("US", "USA", "OVERSEAS") or (ticker.isalpha() and len(ticker) <= 5)
            action_kr = "ë§¤ìˆ˜" if sig["action"] == "buy" else "ë§¤ë„"
            save_activity_log("cio_manager",
                f"ğŸ¯ {action_kr} ì‹œë„: {sig.get('name', ticker)} ({ticker}) ì‹ ë¢°ë„ {effective_conf:.0f}% ë¹„ì¤‘ {sig.get('weight', 0)}%", "info")

            try:
                # í˜„ì¬ê°€ ì¡°íšŒ
                if is_us:
                    if _KIS_AVAILABLE and _kis_configured():
                        us_price_data = await _kis_us_price(ticker)
                        price = us_price_data.get("price", 0) if us_price_data.get("success") else 0
                        save_activity_log("cio_manager", f"  ğŸ’µ {ticker} í˜„ì¬ê°€: ${price:.2f} (KIS ì¡°íšŒ)", "info")
                    else:
                        target_w = next((w for w in market_watchlist if w.get("ticker", "").upper() == ticker.upper()), None)
                        price = float(target_w.get("target_price", 0)) if target_w else 0
                    if price <= 0:
                        save_activity_log("cio_manager", f"[ìˆ˜ë™/US] {ticker} í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨ (price={price}) â€” ê±´ë„ˆëœ€", "warning")
                        continue
                    _fx = _get_fx_rate()
                    _sig_weight = _get_signal_weight(sig, effective_conf)
                    _order_amt = order_size if order_size > 0 else int(account_balance * _sig_weight)
                    qty = max(1, int(_order_amt / (price * _fx)))
                    save_activity_log("cio_manager",
                        f"  ğŸ“ ì£¼ë¬¸ ê³„ì‚°: ì”ê³  {account_balance:,.0f}ì› Ã— ë¹„ì¤‘ {_sig_weight:.1%} = {_order_amt:,.0f}ì› â†’ ${price:.2f} Ã— â‚©{_fx:.0f} = {qty}ì£¼", "info")
                else:
                    if _KIS_AVAILABLE and _kis_configured():
                        price = await _kis_price(ticker)
                    else:
                        target_w = next((w for w in market_watchlist if w["ticker"] == ticker), None)
                        price = target_w.get("target_price", 0) if target_w else 0
                    if price <= 0:
                        price = 50000
                    _order_amt = order_size if order_size > 0 else int(account_balance * _get_signal_weight(sig, effective_conf))
                    qty = max(1, int(_order_amt / price))

                if use_kis:
                    mode_str = "ì‹¤ê±°ë˜" if not KIS_IS_MOCK else "ëª¨ì˜íˆ¬ì(KIS)"
                    save_activity_log("cio_manager",
                        f"  ğŸš€ KIS ì£¼ë¬¸ ì „ì†¡: {action_kr} {ticker} {qty}ì£¼ @ {'$'+str(round(price,2)) if is_us else str(price)+'ì›'} ({mode_str})", "info")
                    if is_us:
                        order_result = await _kis_us_order(ticker, sig["action"], qty, price=price)
                    else:
                        order_result = await _kis_order(ticker, sig["action"], qty, price=0)
                    save_activity_log("cio_manager",
                        f"  ğŸ“¨ KIS ì‘ë‹µ: success={order_result.get('success')}, msg={order_result.get('message', '')[:100]}", "info")
                    if order_result["success"]:
                        orders_triggered += 1
                        save_activity_log("cio_manager",
                            f"âœ… [ìˆ˜ë™/{mode_str}] {action_kr} ì„±ê³µ: {sig.get('name', ticker)} {qty}ì£¼ (ì‹ ë¢°ë„ {effective_conf:.0f}%)",
                            "info")
                        history = _load_data("trading_history", [])
                        _h_id = f"manual_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}"
                        history.insert(0, {
                            "id": _h_id,
                            "date": datetime.now(KST).isoformat(),
                            "ticker": ticker, "name": sig.get("name", ticker),
                            "action": sig["action"], "qty": qty, "price": price,
                            "total": qty * price, "pnl": 0,
                            "strategy": f"CIO ìˆ˜ë™ë¶„ì„ ({mode_str}, ì‹ ë¢°ë„ {sig['confidence']}%)",
                            "status": "executed", "market": "US" if is_us else "KR",
                            "order_no": order_result.get("order_no", ""),
                        })
                        _save_data("trading_history", history)
                        if sig["action"] == "buy":
                            _register_position_triggers(ticker, sig.get("name", ticker), price, qty,
                                                        "US" if is_us else "KR", settings, source_id=_h_id)
                    else:
                        save_activity_log("cio_manager",
                            f"âŒ [ìˆ˜ë™/{mode_str}] ì£¼ë¬¸ ì‹¤íŒ¨: {sig.get('name', ticker)} â€” {order_result.get('message', 'ì›ì¸ ë¶ˆëª…')}", "error")
                elif use_mock_kis:
                    # â”€â”€ KIS ëª¨ì˜íˆ¬ì ê³„ì¢Œë¡œ ì‹¤ì œ ì£¼ë¬¸ â”€â”€
                    save_activity_log("cio_manager",
                        f"  ğŸš€ KIS ëª¨ì˜íˆ¬ì ì£¼ë¬¸ ì „ì†¡: {action_kr} {ticker} {qty}ì£¼ @ {'$'+str(round(price,2)) if is_us else str(price)+'ì›'}", "info")
                    if is_us:
                        order_result = await _kis_mock_us_order(ticker, sig["action"], qty, price=price)
                    else:
                        order_result = await _kis_mock_order(ticker, sig["action"], qty, price=0)
                    save_activity_log("cio_manager",
                        f"  ğŸ“¨ KIS ëª¨ì˜íˆ¬ì ì‘ë‹µ: success={order_result.get('success')}, msg={order_result.get('message', '')[:100]}", "info")
                    if order_result["success"]:
                        orders_triggered += 1
                        save_activity_log("cio_manager",
                            f"âœ… [ìˆ˜ë™/ëª¨ì˜íˆ¬ì] {action_kr} ì„±ê³µ: {sig.get('name', ticker)} {qty}ì£¼ (ì‹ ë¢°ë„ {effective_conf:.0f}%)", "info")
                        history = _load_data("trading_history", [])
                        _h_id2 = f"mock_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}"
                        history.insert(0, {
                            "id": _h_id2,
                            "date": datetime.now(KST).isoformat(),
                            "ticker": ticker, "name": sig.get("name", ticker),
                            "action": sig["action"], "qty": qty, "price": price,
                            "total": qty * price, "pnl": 0,
                            "strategy": f"CIO ìˆ˜ë™ë¶„ì„ (ëª¨ì˜íˆ¬ì, ì‹ ë¢°ë„ {sig['confidence']}%)",
                            "status": "mock_executed", "market": "US" if is_us else "KR",
                            "order_no": order_result.get("order_no", ""),
                        })
                        _save_data("trading_history", history)
                        if sig["action"] == "buy":
                            _register_position_triggers(ticker, sig.get("name", ticker), price, qty,
                                                        "US" if is_us else "KR", settings, source_id=_h_id2)
                    else:
                        save_activity_log("cio_manager",
                            f"âŒ [ìˆ˜ë™/ëª¨ì˜íˆ¬ì] ì£¼ë¬¸ ì‹¤íŒ¨: {sig.get('name', ticker)} â€” {order_result.get('message', 'ì›ì¸ ë¶ˆëª…')}", "error")
                else:
                    # ê°€ìƒ í¬íŠ¸í´ë¦¬ì˜¤ (paper trading)
                    portfolio = _load_data("trading_portfolio", _default_portfolio())
                    if sig["action"] == "buy" and portfolio["cash"] >= price * qty:
                        holding = next((h for h in portfolio["holdings"] if h["ticker"] == ticker), None)
                        total_amount = qty * price
                        if holding:
                            old_total = holding["avg_price"] * holding["qty"]
                            holding["qty"] += qty
                            holding["avg_price"] = int((old_total + total_amount) / holding["qty"])
                            holding["current_price"] = price
                        else:
                            portfolio["holdings"].append({
                                "ticker": ticker, "name": sig.get("name", ticker),
                                "qty": qty, "avg_price": price, "current_price": price,
                                "market": sig.get("market", market),
                            })
                        portfolio["cash"] -= total_amount
                        portfolio["updated_at"] = datetime.now(KST).isoformat()
                        _save_data("trading_portfolio", portfolio)
                        orders_triggered += 1
                        history = _load_data("trading_history", [])
                        _h_id3 = f"manual_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}"
                        history.insert(0, {
                            "id": _h_id3,
                            "date": datetime.now(KST).isoformat(),
                            "ticker": ticker, "name": sig.get("name", ticker),
                            "action": "buy", "qty": qty, "price": price,
                            "total": total_amount, "pnl": 0,
                            "strategy": f"CIO ìˆ˜ë™ë¶„ì„ (ê°€ìƒ, ì‹ ë¢°ë„ {sig['confidence']}%)",
                            "status": "executed", "market": sig.get("market", market),
                        })
                        _save_data("trading_history", history)
                        save_activity_log("cio_manager",
                            f"[ìˆ˜ë™/ê°€ìƒ] ë§¤ìˆ˜: {sig.get('name', ticker)} {qty}ì£¼ x {price:,.0f}ì› (ì‹ ë¢°ë„ {effective_conf:.0f}%)", "info")
                        _register_position_triggers(ticker, sig.get("name", ticker), price, qty,
                                                    sig.get("market", market), settings, source_id=_h_id3)
                    elif sig["action"] == "sell":
                        holding = next((h for h in portfolio["holdings"] if h["ticker"] == ticker), None)
                        if holding and holding["qty"] > 0:
                            sell_qty = min(qty, holding["qty"])
                            total_amount = sell_qty * price
                            pnl = (price - holding["avg_price"]) * sell_qty
                            holding["qty"] -= sell_qty
                            if holding["qty"] == 0:
                                portfolio["holdings"] = [h for h in portfolio["holdings"] if h["ticker"] != ticker]
                            portfolio["cash"] += total_amount
                            portfolio["updated_at"] = datetime.now(KST).isoformat()
                            _save_data("trading_portfolio", portfolio)
                            orders_triggered += 1
                            history = _load_data("trading_history", [])
                            history.insert(0, {
                                "id": f"manual_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}",
                                "date": datetime.now(KST).isoformat(),
                                "ticker": ticker, "name": sig.get("name", ticker),
                                "action": "sell", "qty": sell_qty, "price": price,
                                "total": total_amount, "pnl": pnl,
                                "strategy": f"CIO ìˆ˜ë™ë¶„ì„ (ê°€ìƒ, ì‹ ë¢°ë„ {sig['confidence']}%)",
                                "status": "executed", "market": sig.get("market", market),
                            })
                            _save_data("trading_history", history)
                            pnl_str = f"{'+'if pnl>=0 else ''}{pnl:,.0f}ì›"
                            save_activity_log("cio_manager",
                                f"[ìˆ˜ë™/ê°€ìƒ] ë§¤ë„: {sig.get('name', ticker)} {sell_qty}ì£¼ x {price:,.0f}ì› (ì†ìµ {pnl_str})", "info")
            except Exception as order_err:
                import traceback
                _tb = traceback.format_exc()
                logger.error("[ìˆ˜ë™ ë¶„ì„] ìë™ì£¼ë¬¸ ì˜¤ë¥˜ (%s): %s\n%s", ticker, order_err, _tb)
                save_activity_log("cio_manager", f"âŒ [ìˆ˜ë™] ì£¼ë¬¸ ì˜¤ë¥˜: {ticker} â€” {order_err}", "error")

    # â”€â”€ CIO ëª©í‘œê°€ ê¸°ë°˜ buy_limit íŠ¸ë¦¬ê±° ìë™ ë“±ë¡ (ìˆ˜ë™ ì¦‰ì‹œë¶„ì„) â”€â”€
    _today_str2 = datetime.now(KST).strftime("%Y%m%d")
    for sig in parsed_signals:
        _tp = sig.get("target_price", 0)
        if _tp <= 0 or sig["action"] not in ("buy", "hold"):
            continue
        _bl2_ticker = sig["ticker"]
        _bl2_name = sig.get("name", _bl2_ticker)
        _bl2_market = sig.get("market", market)
        _bl2_is_us = _bl2_market.upper() in ("US", "USA", "OVERSEAS") or (
            _bl2_ticker.isalpha() and len(_bl2_ticker) <= 5
        )
        _all2 = _load_data("price_triggers", [])
        _all2 = [
            t for t in _all2
            if not (
                t.get("type") == "buy_limit"
                and t.get("ticker") == _bl2_ticker
                and t.get("created_at", "").startswith(_today_str2)
            )
        ]
        _w2 = _get_signal_weight(sig, sig.get("confidence", 50))
        _amt2 = int(account_balance * _w2) if account_balance > 0 else 500_000
        _fx2 = _get_fx_rate()
        _qty2 = max(1, int(_amt2 / (_tp * _fx2))) if _bl2_is_us else max(1, int(_amt2 / _tp))
        _all2.insert(0, {
            "id": f"bl_{_bl2_ticker}_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}",
            "ticker": _bl2_ticker, "name": _bl2_name,
            "type": "buy_limit", "trigger_price": _tp, "qty": _qty2,
            "market": _bl2_market, "active": True,
            "created_at": datetime.now(KST).isoformat(),
            "source": "cio_manual", "source_id": new_signal["id"],
            "note": f"CIO ëª©í‘œë§¤ìˆ˜: {_tp:,.0f} ({sig.get('confidence', 0)}% ì‹ ë¢°ë„) â€” {sig.get('reason', '')[:60]}",
        })
        if len(_all2) > 500:
            _all2 = _all2[:500]
        _save_data("price_triggers", _all2)
        save_activity_log(
            "cio_manager",
            f"ğŸ¯ ëª©í‘œë§¤ìˆ˜ ìë™ë“±ë¡: {_bl2_name}({_bl2_ticker}) ëª©í‘œê°€ {_tp:,.0f} Ã— {_qty2}ì£¼",
            "info",
        )

    save_activity_log("cio_manager",
        f"âœ… ìˆ˜ë™ ë¶„ì„ ì™„ë£Œ: {len(parsed_signals)}ê°œ ì‹œê·¸ë„ (ì£¼ë¬¸ {orders_triggered}ê±´, ë¹„ìš© ${cost:.4f})", "info")

    return {
        "success": True,
        "market": market_label,
        "signals_count": len(parsed_signals),
        "signals": parsed_signals,
        "orders_triggered": orders_triggered,
        "calibration": calibration,
        "calibration_factor": calibration_factor,
        "cost_usd": cost,
        "analysis_preview": content[:500] + "..." if len(content) > 500 else content,
    }


def _is_us_dst() -> bool:
    """ë¯¸êµ­ ì„œë¨¸íƒ€ì„(EDT) ì—¬ë¶€ íŒì • â€” 3ì›” ë‘˜ì§¸ ì¼ìš”ì¼ 02:00 ~ 11ì›” ì²«ì§¸ ì¼ìš”ì¼ 02:00 (ET).
    í•œêµ­ì€ ì„œë¨¸íƒ€ì„ì´ ì—†ìœ¼ë¯€ë¡œ ë‚ ì§œ ê¸°ì¤€ ê·¼ì‚¬ íŒì •."""
    now = datetime.now(KST)
    y = now.year
    # 3ì›” ë‘˜ì§¸ ì¼ìš”ì¼ (weekday: 0=Mon, 6=Sun)
    mar1_wd = datetime(y, 3, 1).weekday()
    second_sun_mar = 1 + (6 - mar1_wd) % 7 + 7
    # 11ì›” ì²«ì§¸ ì¼ìš”ì¼
    nov1_wd = datetime(y, 11, 1).weekday()
    first_sun_nov = 1 + (6 - nov1_wd) % 7
    mar_date = datetime(y, 3, second_sun_mar, tzinfo=KST)
    nov_date = datetime(y, 11, first_sun_nov, tzinfo=KST)
    return mar_date <= now < nov_date


def _us_market_hours_kst() -> tuple[str, str]:
    """ë¯¸êµ­ ì •ê·œì¥ KST ì‹œì‘/ì¢…ë£Œ ì‹œê° (ì„œë¨¸íƒ€ì„ ìë™ ë°˜ì˜).
    EST(ê²¨ìš¸): 23:30~06:00 KST | EDT(ì—¬ë¦„): 22:30~05:00 KST"""
    if _is_us_dst():
        return "22:30", "05:00"
    return "23:30", "06:00"


def _is_market_open(settings: dict) -> tuple[bool, str]:
    """í•œêµ­/ë¯¸êµ­ ì¥ ì‹œê°„ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤. (ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì—´ë ¤ìˆìœ¼ë©´ True)
    ì£¼ë§(í† /ì¼)ì—ëŠ” ë¬´ì¡°ê±´ False. ë¯¸êµ­ ì¥ ì‹œê°„ì€ ì„œë¨¸íƒ€ì„(DST) ìë™ ë°˜ì˜."""
    now = datetime.now(KST)

    # ì£¼ë§ ì²´í¬ (ì›”=0 ~ ê¸ˆ=4 í‰ì¼, í† =5 ì¼=6 ì£¼ë§)
    if now.weekday() >= 5:
        return False, ""

    now_min = now.hour * 60 + now.minute

    # í•œêµ­ ì¥ (09:00 ~ 15:20 KST, í‰ì¼ë§Œ)
    kr = settings.get("trading_hours_kr", settings.get("trading_hours", {}))
    kr_start = sum(int(x) * m for x, m in zip(kr.get("start", "09:00").split(":"), [60, 1]))
    kr_end = sum(int(x) * m for x, m in zip(kr.get("end", "15:20").split(":"), [60, 1]))
    if kr_start <= now_min < kr_end:
        return True, "KR"

    # ë¯¸êµ­ ì¥ (ì„œë¨¸íƒ€ì„ ìë™ ë°˜ì˜, í‰ì¼ë§Œ)
    # ê¸ˆìš”ì¼ ë°¤~í† ìš”ì¼ ìƒˆë²½ì€ ë¯¸êµ­ì¥ ì˜¤í”ˆì´ì§€ë§Œ, í† ìš”ì¼ ìƒˆë²½(weekday=5)ì€ ìœ„ì—ì„œ ì´ë¯¸ ì°¨ë‹¨ë¨
    us_default_start, us_default_end = _us_market_hours_kst()
    us = settings.get("trading_hours_us", {})
    us_start = sum(int(x) * m for x, m in zip(us.get("start", us_default_start).split(":"), [60, 1]))
    us_end = sum(int(x) * m for x, m in zip(us.get("end", us_default_end).split(":"), [60, 1]))
    if us_start <= now_min or now_min < us_end:  # ìì • ë„˜ê¹€ ì²˜ë¦¬
        return True, "US"

    return False, ""


def _us_analysis_time_kst() -> tuple[int, int]:
    """ë¯¸êµ­ì¥ ë¶„ì„ ì‹¤í–‰ ì‹œê° (KST, ì¥ ì˜¤í”ˆ 10ë¶„ í›„).
    EST(ê²¨ìš¸): 23:40 KST | EDT(ì—¬ë¦„): 22:40 KST"""
    return (22, 40) if _is_us_dst() else (23, 40)


def _next_trading_run_time():
    """ë‹¤ìŒ ì‹¤í–‰ ì‹œê° ê³„ì‚° (09:10 KST í•œêµ­ì¥ / 23:40 ë˜ëŠ” 22:40 KST ë¯¸êµ­ì¥).

    ë¯¸êµ­ì¥ ì‹œê°„ì€ ì„œë¨¸íƒ€ì„(DST) ìë™ ë°˜ì˜.
    ì£¼ë§(í† /ì¼)ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ í‰ì¼(ì›”ìš”ì¼)ë¡œ ì´ë™.
    """
    now = datetime.now(KST)
    us_h, us_m = _us_analysis_time_kst()

    # ì˜¤ëŠ˜ë¶€í„° ìµœëŒ€ 7ì¼ íƒìƒ‰ (ì£¼ë§ ê±´ë„ˆë›°ê¸°)
    for offset in range(7):
        day = now.date() + timedelta(days=offset)
        # ì£¼ë§ ê±´ë„ˆë›°ê¸° (í† =5, ì¼=6)
        if day.weekday() >= 5:
            continue
        run_times = [
            datetime(day.year, day.month, day.day, 9, 10, tzinfo=KST),
            datetime(day.year, day.month, day.day, us_h, us_m, tzinfo=KST),
        ]
        for t in run_times:
            if t > now:
                return t

    # í´ë°± (ë„ë‹¬í•˜ë©´ ì•ˆ ë˜ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
    tomorrow = now.date() + timedelta(days=1)
    return datetime(tomorrow.year, tomorrow.month, tomorrow.day, 9, 10, tzinfo=KST)


async def _trading_bot_loop():
    """ìë™ë§¤ë§¤ ë´‡ ë£¨í”„ â€” íˆ¬ìíŒ€ì¥ì´ ë¶„ì„ â†’ ìë™ ë§¤ë§¤.

    íë¦„:
    1. í•˜ë£¨ 2íšŒ ì •í•´ì§„ ì‹œê°ì— ì‹¤í–‰ (09:10 KST, 14:50 KST)
    2. ê´€ì‹¬ì¢…ëª©ì´ ìˆìœ¼ë©´ CIO íŒ€ì—ê²Œ ë¶„ì„ ìœ„ì„
    3. CIOê°€ 4ëª… ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ì·¨í•©í•˜ì—¬ ë§¤ìˆ˜/ë§¤ë„/ê´€ë§ íŒë‹¨
    4. ì‹ ë¢°ë„ 70% ì´ìƒ ì‹œê·¸ë„ë§Œ ìë™ ì£¼ë¬¸ ì‹¤í–‰ (auto_execute=Trueì¼ ë•Œë§Œ)
    5. ëª¨ì˜íˆ¬ì ëª¨ë“œ(paper_trading=True)ì—ì„œëŠ” ê°€ìƒ í¬íŠ¸í´ë¦¬ì˜¤ë§Œ ì—…ë°ì´íŠ¸
    """
    logger = logging.getLogger("corthex.trading")
    us_h, us_m = _us_analysis_time_kst()
    logger.info("ìë™ë§¤ë§¤ ë´‡ ë£¨í”„ ì‹œì‘ (CIO ì—°ë™ â€” í•˜ë£¨ 2íšŒ: 09:10 í•œêµ­ì¥ + %02d:%02d ë¯¸êµ­ì¥ KST)", us_h, us_m)

    while app_state.trading_bot_active:
        try:
            next_run = _next_trading_run_time()
            now = datetime.now(KST)
            sleep_seconds = (next_run - now).total_seconds()
            logger.info("[TRADING BOT] ë‹¤ìŒ ì‹¤í–‰ ì˜ˆì•½: %s (ì•½ %.0fì´ˆ í›„)",
                        next_run.strftime("%Y-%m-%d %H:%M KST"), sleep_seconds)
            if sleep_seconds > 0:
                await asyncio.sleep(sleep_seconds)
            if not app_state.trading_bot_active:
                break

            settings = _load_data("trading_settings", _default_trading_settings())
            is_open, market = _is_market_open(settings)

            if not is_open:
                continue

            # ê´€ì‹¬ì¢…ëª© í™•ì¸
            watchlist = _load_data("trading_watchlist", [])
            if not watchlist:
                continue

            # í•´ë‹¹ ì‹œì¥ì˜ ê´€ì‹¬ì¢…ëª©ë§Œ í•„í„° (í•œêµ­ ì¥ì´ë©´ í•œêµ­ ì¢…ëª©, ë¯¸êµ­ ì¥ì´ë©´ ë¯¸êµ­ ì¢…ëª©)
            market_watchlist = [w for w in watchlist if w.get("market", "KR") == market]
            if not market_watchlist:
                continue

            market_name = "í•œêµ­" if market == "KR" else "ë¯¸êµ­"
            logger.info("[TRADING BOT] %sì¥ ì˜¤í”ˆ â€” %dê°œ ì¢…ëª© CIO ë¶„ì„ ì‹œì‘", market_name, len(market_watchlist))
            save_activity_log("cio_manager",
                f"ğŸ¤– ìë™ë§¤ë§¤ ë´‡: {market_name}ì¥ {len(market_watchlist)}ê°œ ì¢…ëª© CIO ë¶„ì„ ì‹œì‘",
                "info")

            # CIO + ì „ë¬¸ê°€ íŒ€ì—ê²Œ ë¶„ì„ ìœ„ì„
            tickers_info = ", ".join([f"{w['name']}({w['ticker']})" for w in market_watchlist])
            strategies = _load_data("trading_strategies", [])
            active = [s for s in strategies if s.get("active")]
            strats_info = ", ".join([s["name"] for s in active[:5]]) or "ê¸°ë³¸ ì „ëµ"

            # ìê¸°í•™ìŠµ ë³´ì • ì„¹ì…˜ (ë² ì´ì§€ì•ˆ + ELO + ì˜¤ë‹µíŒ¨í„´ + Platt Scaling í†µí•©)
            cal_section = _build_calibration_prompt_section(settings)

            prompt = f"""[ìë™ë§¤ë§¤ ë´‡ â€” {market_name}ì¥ ì •ê¸° ë¶„ì„]

## ë¶„ì„ ëŒ€ìƒ ({len(market_watchlist)}ê°œ ì¢…ëª©)
{tickers_info}

## í™œì„± ì „ëµ: {strats_info}{cal_section}

## ë¶„ì„ ìš”ì²­
ë„êµ¬(API)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ ì•„ë˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
- **ì‹œí™©ë¶„ì„**: {'ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì§€ìˆ˜ íë¦„, ì™¸êµ­ì¸/ê¸°ê´€ ë™í–¥, ê¸ˆë¦¬/í™˜ìœ¨' if market == 'KR' else 'S&P500/ë‚˜ìŠ¤ë‹¥ ì§€ìˆ˜, ë¯¸êµ­ ê¸ˆë¦¬/ê³ ìš©ì§€í‘œ, ë‹¬ëŸ¬ ê°•ì„¸'}
- **ì¢…ëª©ë¶„ì„**: ê° ì¢…ëª© ì¬ë¬´ ê±´ì „ì„±, PER/PBR, ìµœê·¼ ì‹¤ì 
- **ê¸°ìˆ ì ë¶„ì„**: RSI, MACD, ì´ë™í‰ê· ì„ , ë³¼ë¦°ì €ë°´ë“œ
- **ë¦¬ìŠ¤í¬ê´€ë¦¬**: ì†ì ˆê°€, ì ì • í¬ì§€ì…˜ í¬ê¸°, ì „ì²´ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬

## ìµœì¢… ì‚°ì¶œë¬¼ (ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ ê·¸ëŒ€ë¡œ â€” ì˜ˆì‹œì²˜ëŸ¼ ì •í™•íˆ)
[ì‹œê·¸ë„] ì‚¼ì„±ì „ì (005930) | ë§¤ìˆ˜ | ì‹ ë¢°ë„ 72% | ë¹„ì¤‘ 15% | ëª©í‘œê°€ 78000 | ë°˜ë„ì²´ ìˆ˜ìš” íšŒë³µ + RSI ê³¼ë§¤ë„ êµ¬ê°„
[ì‹œê·¸ë„] ì¹´ì¹´ì˜¤ (035720) | ë§¤ë„ | ì‹ ë¢°ë„ 61% | ë¹„ì¤‘ 10% | ëª©í‘œê°€ 0 | PER ê³¼ëŒ€í‰ê°€, ê¸ˆë¦¬ ë¯¼ê° ì„¹í„° ì•½ì„¸
[ì‹œê·¸ë„] LGì—ë„ˆì§€ì†”ë£¨ì…˜ (373220) | ê´€ë§ | ì‹ ë¢°ë„ 45% | ë¹„ì¤‘ 0% | ëª©í‘œê°€ 390000 | í˜¼ì¡°ì„¸, ì´ ê°€ê²© ë„ë‹¬ ì‹œ ì§„ì… ê²€í† 

â€» ì£¼ì˜:
- ì‹ ë¢°ë„ëŠ” ì¢…ëª©ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°, 0~100 ìˆ«ì + % ê¸°í˜¸ë¡œ í‘œê¸°
- ëª©í‘œê°€(ê¶Œì¥ ë§¤ìˆ˜ ì§„ì…ê°€): ë§¤ìˆ˜/ê´€ë§ ì¢…ëª©ì€ ë°˜ë“œì‹œ ì…ë ¥. í˜„ì¬ê°€ë³´ë‹¤ ë‚®ì€ ëª©í‘œ ì§„ì…ê°€ ì„¤ì •. ë¯¸êµ­ ì£¼ì‹ì€ USD ë‹¨ìœ„. ë§¤ë„ ì¢…ëª©ì€ 0
- ëª©í‘œê°€ ë„ë‹¬ ì‹œ ì„œë²„ê°€ ìë™ìœ¼ë¡œ ë§¤ìˆ˜ ì‹¤í–‰ â€” ì‹ ì¤‘í•˜ê²Œ ì„¤ì •í•  ê²ƒ"""

            cio_result = await _call_agent("cio_manager", prompt)
            content = cio_result.get("content", "")
            cost = cio_result.get("cost_usd", 0)

            # â”€â”€ ë¹„ì„œì‹¤ì¥ QA: íŒ€ì¥ ë³´ê³ ì„œ ê²€ìˆ˜ â”€â”€
            qa_passed, qa_reason = await _chief_qa_review(content, "ê¸ˆìœµë¶„ì„íŒ€ì¥")
            save_activity_log("chief_of_staff",
                f"ğŸ“‹ ìë™ë¶„ì„ QA: {'âœ… ìŠ¹ì¸' if qa_passed else 'âŒ ë°˜ë ¤'} â€” {qa_reason[:80]}",
                "info" if qa_passed else "warning")

            # ì‹œê·¸ë„ íŒŒì‹±
            parsed_signals = _parse_cio_signals(content, market_watchlist)

            # ì‹œê·¸ë„ ì €ì¥ (QA ê²°ê³¼ í¬í•¨)
            signals = _load_data("trading_signals", [])
            new_signal = {
                "id": f"sig_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}",
                "date": datetime.now(KST).isoformat(),
                "market": market,
                "analysis": content,
                "tickers": [w["ticker"] for w in market_watchlist[:10]],
                "parsed_signals": parsed_signals,
                "strategy": "cio_bot_analysis",
                "analyzed_by": "ê¸ˆìœµë¶„ì„íŒ€ì¥ ë‹¨ë… ë¶„ì„",
                "cost_usd": cost,
                "auto_bot": True,
                "qa_passed": qa_passed,
                "qa_reason": qa_reason[:200],
            }
            signals.insert(0, new_signal)
            if len(signals) > 200:
                signals = signals[:200]
            _save_data("trading_signals", signals)

            # QA ë°˜ë ¤ ì‹œ ë§¤ë§¤ ì•ˆ í•¨
            if not qa_passed:
                save_activity_log("chief_of_staff",
                    f"ğŸš« ìë™ë¶„ì„ QA ë°˜ë ¤ â€” ë§¤ë§¤ ì¤‘ë‹¨: {qa_reason[:100]}", "warning")
                continue

            # ë§¤ë§¤ ê²°ì • ì¼ì§€ ì €ì¥ (P2-1: ìë™ë´‡ì—ì„œë„ decisions ì €ì¥)
            _save_decisions(parsed_signals)

            # ìë™ ì£¼ë¬¸ ì‹¤í–‰ (auto_execute=True + ì‹ ë¢°ë„ ì¶©ì¡± ì‹œ)
            auto_execute = settings.get("auto_execute", False)
            min_confidence = settings.get("min_confidence", 70)
            order_size = settings.get("order_size", 0)  # 0 = CIO ë¹„ì¤‘ ììœ¨

            if auto_execute:
                enable_real = settings.get("enable_real", True)
                enable_mock = settings.get("enable_mock", False)
                paper_mode = settings.get("paper_trading", True)
                use_kis = enable_real and _KIS_AVAILABLE and not paper_mode and _kis_configured()
                use_mock_kis = (not use_kis) and enable_mock and _KIS_AVAILABLE and _kis_mock_configured()

                # CIO ë¹„ì¤‘ ê¸°ë°˜ ë§¤ìˆ˜(Bì•ˆ): order_size=0ì´ë©´ ì”ê³ Ã—ë¹„ì¤‘ìœ¼ë¡œ ìë™ ì‚°ì¶œ
                account_balance = 0
                if order_size == 0:
                    try:
                        if use_kis:
                            _bal = await _kis_balance()
                            account_balance = _bal.get("cash", 0) if _bal.get("success") else 0
                        elif use_mock_kis:
                            _bal = await _kis_mock_balance()
                            account_balance = _bal.get("cash", 0) if _bal.get("success") else 0
                        else:
                            _port = _load_data("trading_portfolio", _default_portfolio())
                            account_balance = _port.get("cash", 0)
                    except Exception as e:
                        logger.debug("ë´‡ ì”ê³  ì¡°íšŒ ì‹¤íŒ¨: %s", e)
                    if account_balance <= 0:
                        account_balance = 1_000_000
                        save_activity_log("cio_manager", "CIO ë¹„ì¤‘ ëª¨ë“œ: ì”ê³  ì¡°íšŒ ì‹¤íŒ¨, ê¸°ë³¸ 100ë§Œì› ì‚¬ìš©", "warning")

                for sig in parsed_signals:
                    if sig["action"] not in ("buy", "sell"):
                        continue
                    # ìê¸°ë³´ì • ì ìš©: ìœ íš¨ ì‹ ë¢°ë„ = raw Ã— calibration_factor
                    # factor < 1 (AI ê³¼ì‹ ) â†’ ìœ íš¨ ì‹ ë¢°ë„ í•˜ë½ â†’ ë” ì—„ê²©í•œ í•„í„°
                    effective_conf = sig.get("confidence", 0) * calibration_factor
                    if effective_conf < min_confidence:
                        continue

                    ticker = sig["ticker"]
                    # í•œêµ­/ë¯¸êµ­ ì‹œì¥ ìë™ íŒë³„: tickerê°€ ì˜ë¬¸ì´ë©´ US, ìˆ«ìë©´ KR
                    sig_market = sig.get("market", market)
                    is_us = sig_market.upper() in ("US", "USA", "OVERSEAS") or (ticker.isalpha() and len(ticker) <= 5)

                    try:
                        if is_us:
                            # â”€â”€ ë¯¸êµ­ì£¼ì‹ í˜„ì¬ê°€ ì¡°íšŒ + ì§€ì •ê°€ ì£¼ë¬¸ â”€â”€
                            if _KIS_AVAILABLE and _kis_configured():
                                us_price_data = await _kis_us_price(ticker)
                                price = us_price_data.get("price", 0) if us_price_data.get("success") else 0
                            else:
                                target_w = next((w for w in market_watchlist if w.get("ticker", "").upper() == ticker.upper()), None)
                                price = float(target_w.get("target_price", 0)) if target_w else 0
                            if price <= 0:
                                save_activity_log("cio_manager", f"[US] {ticker} í˜„ì¬ê°€ ì¡°íšŒ ì‹¤íŒ¨ â€” ì£¼ë¬¸ ê±´ë„ˆëœ€", "warning")
                                continue
                            # ë¯¸êµ­ì£¼ì‹: order_size(ì›) Ã· (ê°€ê²©Ã—í™˜ìœ¨) = ì£¼ìˆ˜
                            _fx = _get_fx_rate()
                            _order_amt = order_size if order_size > 0 else int(account_balance * _get_signal_weight(sig, effective_conf))
                            qty = max(1, int(_order_amt / (price * _fx)))
                        else:
                            # â”€â”€ í•œêµ­ì£¼ì‹ í˜„ì¬ê°€ ì¡°íšŒ â”€â”€
                            if _KIS_AVAILABLE and _kis_configured():
                                price = await _kis_price(ticker)
                            else:
                                target_w = next((w for w in market_watchlist if w["ticker"] == ticker), None)
                                price = target_w.get("target_price", 0) if target_w else 0
                            if price <= 0:
                                price = 50000  # ê°€ê²© ë¯¸ì„¤ì • ì‹œ ê¸°ë³¸ê°’
                            _order_amt = order_size if order_size > 0 else int(account_balance * _get_signal_weight(sig, effective_conf))
                            qty = max(1, int(_order_amt / price))

                        if use_kis:
                            mode_str = "ì‹¤ê±°ë˜" if not KIS_IS_MOCK else "ëª¨ì˜íˆ¬ì(KIS)"
                            action_kr = "ë§¤ìˆ˜" if sig["action"] == "buy" else "ë§¤ë„"

                            if is_us:
                                order_result = await _kis_us_order(ticker, sig["action"], qty, price=price)
                                order_total = qty * price
                            else:
                                order_result = await _kis_order(ticker, sig["action"], qty, price=0)
                                order_total = qty * price

                            if order_result["success"]:
                                order_msg = f"[{mode_str}] {action_kr} ì£¼ë¬¸ ì™„ë£Œ: {sig.get('name', ticker)} {qty}ì£¼ ${price:.2f}" if is_us else \
                                            f"[{mode_str}] {action_kr} ì£¼ë¬¸ ì™„ë£Œ: {sig.get('name', ticker)} {qty}ì£¼ (ì£¼ë¬¸ë²ˆí˜¸: {order_result['order_no']})"
                                save_activity_log("cio_manager", order_msg, "info")
                                history = _load_data("trading_history", [])
                                _auto_h_id = f"kis_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}"
                                history.insert(0, {
                                    "id": _auto_h_id,
                                    "date": datetime.now(KST).isoformat(),
                                    "ticker": ticker, "name": sig.get("name", ticker),
                                    "action": sig["action"], "qty": qty, "price": price,
                                    "total": order_total, "pnl": 0,
                                    "strategy": f"CIO ìë™ë§¤ë§¤ ({mode_str}, ì‹ ë¢°ë„ {sig['confidence']}%)",
                                    "status": "executed", "market": "US" if is_us else "KR",
                                    "order_no": order_result["order_no"],
                                    "currency": "USD" if is_us else "KRW",
                                })
                                _save_data("trading_history", history)
                                if sig["action"] == "buy":
                                    _register_position_triggers(ticker, sig.get("name", ticker), price, qty,
                                                                "US" if is_us else "KR", settings, source_id=_auto_h_id)
                            else:
                                order_msg = f"[{mode_str}] ì£¼ë¬¸ ì‹¤íŒ¨: {sig.get('name', ticker)} â€” {order_result['message']}"
                                save_activity_log("cio_manager", order_msg, "warning")

                        elif use_mock_kis:
                            # â”€â”€ KIS ëª¨ì˜íˆ¬ì ê³„ì¢Œë¡œ ì‹¤ì œ ì£¼ë¬¸ â”€â”€
                            action_kr = "ë§¤ìˆ˜" if sig["action"] == "buy" else "ë§¤ë„"

                            if is_us:
                                order_result = await _kis_mock_us_order(ticker, sig["action"], qty, price=price)
                                order_total = qty * price
                            else:
                                order_result = await _kis_mock_order(ticker, sig["action"], qty, price=0)
                                order_total = qty * price

                            if order_result["success"]:
                                order_msg = f"[ëª¨ì˜íˆ¬ì] {action_kr} ì£¼ë¬¸ ì™„ë£Œ: {sig.get('name', ticker)} {qty}ì£¼" + \
                                            (f" ${price:.2f}" if is_us else f" (ì£¼ë¬¸ë²ˆí˜¸: {order_result['order_no']})")
                                save_activity_log("cio_manager", order_msg, "info")
                                history = _load_data("trading_history", [])
                                _auto_mock_id = f"mock_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}"
                                history.insert(0, {
                                    "id": _auto_mock_id,
                                    "date": datetime.now(KST).isoformat(),
                                    "ticker": ticker, "name": sig.get("name", ticker),
                                    "action": sig["action"], "qty": qty, "price": price,
                                    "total": order_total, "pnl": 0,
                                    "strategy": f"CIO ìë™ë§¤ë§¤ (ëª¨ì˜íˆ¬ì, ì‹ ë¢°ë„ {sig['confidence']}%)",
                                    "status": "mock_executed", "market": "US" if is_us else "KR",
                                    "order_no": order_result["order_no"],
                                    "currency": "USD" if is_us else "KRW",
                                })
                                _save_data("trading_history", history)
                                if sig["action"] == "buy":
                                    _register_position_triggers(ticker, sig.get("name", ticker), price, qty,
                                                                "US" if is_us else "KR", settings, source_id=_auto_mock_id)
                            else:
                                order_msg = f"[ëª¨ì˜íˆ¬ì] ì£¼ë¬¸ ì‹¤íŒ¨: {sig.get('name', ticker)} â€” {order_result['message']}"
                                save_activity_log("cio_manager", order_msg, "warning")

                        else:
                            # ê°€ìƒ í¬íŠ¸í´ë¦¬ì˜¤ ì—…ë°ì´íŠ¸ (paper_trading ëª¨ë“œ)
                            portfolio = _load_data("trading_portfolio", _default_portfolio())
                            if sig["action"] == "buy" and portfolio["cash"] >= price * qty:
                                holding = next((h for h in portfolio["holdings"] if h["ticker"] == ticker), None)
                                total_amount = qty * price
                                if holding:
                                    old_total = holding["avg_price"] * holding["qty"]
                                    new_total = old_total + total_amount
                                    holding["qty"] += qty
                                    holding["avg_price"] = int(new_total / holding["qty"])
                                    holding["current_price"] = price
                                else:
                                    portfolio["holdings"].append({
                                        "ticker": ticker, "name": sig.get("name", ticker),
                                        "qty": qty, "avg_price": price, "current_price": price,
                                        "market": sig.get("market", market),
                                    })
                                portfolio["cash"] -= total_amount
                                portfolio["updated_at"] = datetime.now(KST).isoformat()
                                _save_data("trading_portfolio", portfolio)

                                history = _load_data("trading_history", [])
                                history.insert(0, {
                                    "id": f"auto_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}",
                                    "date": datetime.now(KST).isoformat(),
                                    "ticker": ticker, "name": sig.get("name", ticker),
                                    "action": "buy", "qty": qty, "price": price,
                                    "total": total_amount, "pnl": 0,
                                    "strategy": f"CIO ìë™ë§¤ë§¤ (ê°€ìƒ, ì‹ ë¢°ë„ {sig['confidence']}%)",
                                    "status": "executed", "market": sig.get("market", market),
                                })
                                _save_data("trading_history", history)

                                save_activity_log("cio_manager",
                                    f"[ê°€ìƒ] ë§¤ìˆ˜: {sig.get('name', ticker)} {qty}ì£¼ x {price:,.0f}ì› (ì‹ ë¢°ë„ {sig['confidence']}%)",
                                    "info")

                            elif sig["action"] == "sell":
                                holding = next((h for h in portfolio["holdings"] if h["ticker"] == ticker), None)
                                if holding and holding["qty"] > 0:
                                    sell_qty = min(qty, holding["qty"])
                                    total_amount = sell_qty * price
                                    pnl = (price - holding["avg_price"]) * sell_qty
                                    holding["qty"] -= sell_qty
                                    if holding["qty"] == 0:
                                        portfolio["holdings"] = [h for h in portfolio["holdings"] if h["ticker"] != ticker]
                                    portfolio["cash"] += total_amount
                                    portfolio["updated_at"] = datetime.now(KST).isoformat()
                                    _save_data("trading_portfolio", portfolio)

                                    history = _load_data("trading_history", [])
                                    history.insert(0, {
                                        "id": f"auto_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}_{ticker}",
                                        "date": datetime.now(KST).isoformat(),
                                        "ticker": ticker, "name": sig.get("name", ticker),
                                        "action": "sell", "qty": sell_qty, "price": price,
                                        "total": total_amount, "pnl": pnl,
                                        "strategy": f"CIO ìë™ë§¤ë§¤ (ê°€ìƒ, ì‹ ë¢°ë„ {sig['confidence']}%)",
                                        "status": "executed", "market": sig.get("market", market),
                                    })
                                    _save_data("trading_history", history)

                                    pnl_str = f"{'+'if pnl>=0 else ''}{pnl:,.0f}ì›"
                                    save_activity_log("cio_manager",
                                        f"[ê°€ìƒ] ë§¤ë„: {sig.get('name', ticker)} {sell_qty}ì£¼ x {price:,.0f}ì› (ì†ìµ {pnl_str})",
                                        "info")
                    except Exception as order_err:
                        logger.error("[TRADING BOT] ìë™ì£¼ë¬¸ ì˜¤ë¥˜ (%s): %s", ticker, order_err)

                # â”€â”€ CIO ëª©í‘œê°€ ê¸°ë°˜ buy_limit íŠ¸ë¦¬ê±° ìë™ ë“±ë¡ â”€â”€
                # ë§¤ìˆ˜/ê´€ë§ ì‹œê·¸ë„ì— ëª©í‘œê°€ê°€ ìˆìœ¼ë©´, ê°€ê²© ë„ë‹¬ ì‹œ ì„œë²„ê°€ ìë™ ë§¤ìˆ˜ ì‹¤í–‰
                _today_str = datetime.now(KST).strftime("%Y%m%d")
                for sig in parsed_signals:
                    target_price = sig.get("target_price", 0)
                    if target_price <= 0:
                        continue
                    if sig["action"] not in ("buy", "hold"):
                        continue
                    _bl_ticker = sig["ticker"]
                    _bl_name = sig.get("name", _bl_ticker)
                    _bl_market = sig.get("market", market)
                    _bl_is_us = _bl_market.upper() in ("US", "USA", "OVERSEAS") or (
                        _bl_ticker.isalpha() and len(_bl_ticker) <= 5
                    )
                    # ì˜¤ëŠ˜ ì´ë¯¸ ë“±ë¡ëœ ê°™ì€ ì¢…ëª©ì˜ buy_limitì€ ê°±ì‹ (ì œê±° í›„ ì¬ë“±ë¡)
                    _all_triggers = _load_data("price_triggers", [])
                    _all_triggers = [
                        t for t in _all_triggers
                        if not (
                            t.get("type") == "buy_limit"
                            and t.get("ticker") == _bl_ticker
                            and t.get("created_at", "").startswith(_today_str)
                        )
                    ]
                    # ìˆ˜ëŸ‰: ë¹„ì¤‘ ê¸°ë°˜ ê³„ì‚°
                    _bl_weight = _get_signal_weight(sig, sig.get("confidence", 50))
                    _bl_amt = int(account_balance * _bl_weight) if account_balance > 0 else 500_000
                    _bl_fx = _get_fx_rate()
                    _bl_qty = max(1, int(_bl_amt / (target_price * _bl_fx))) if _bl_is_us else max(1, int(_bl_amt / target_price))
                    _bl_trigger = {
                        "id": f"bl_{_bl_ticker}_{datetime.now(KST).strftime('%Y%m%d%H%M%S')}",
                        "ticker": _bl_ticker, "name": _bl_name,
                        "type": "buy_limit", "trigger_price": target_price, "qty": _bl_qty,
                        "market": _bl_market, "active": True,
                        "created_at": datetime.now(KST).isoformat(),
                        "source": "cio_auto", "source_id": new_signal["id"],
                        "note": f"CIO ëª©í‘œë§¤ìˆ˜: {target_price:,.0f} ({sig.get('confidence', 0)}% ì‹ ë¢°ë„) â€” {sig.get('reason', '')[:60]}",
                    }
                    _all_triggers.insert(0, _bl_trigger)
                    if len(_all_triggers) > 500:
                        _all_triggers = _all_triggers[:500]
                    _save_data("price_triggers", _all_triggers)
                    save_activity_log(
                        "cio_manager",
                        f"ğŸ¯ ëª©í‘œë§¤ìˆ˜ ìë™ë“±ë¡: {_bl_name}({_bl_ticker}) ëª©í‘œê°€ {target_price:,.0f} Ã— {_bl_qty}ì£¼",
                        "info",
                    )

            buy_count = len([s for s in parsed_signals if s.get("action") == "buy"])
            sell_count = len([s for s in parsed_signals if s.get("action") == "sell"])
            logger.info("[TRADING BOT] CIO ë¶„ì„ ì™„ë£Œ: ë§¤ìˆ˜ %d, ë§¤ë„ %d (ë¹„ìš© $%.4f)", buy_count, sell_count, cost)

        except Exception as e:
            logger.error("[TRADING BOT] ì—ëŸ¬: %s", e)

    logger.info("ìë™ë§¤ë§¤ ë´‡ ë£¨í”„ ì¢…ë£Œ")


# â”€â”€ kis/balance, kis/status â†’ handlers/trading_handler.pyë¡œ ë¶„ë¦¬ â”€â”€

@app.get("/api/trading/kis/debug")
async def kis_debug():
    """KIS API ì›ë³¸ ì‘ë‹µ í™•ì¸ (ë””ë²„ê·¸ìš©)."""
    if not _KIS_AVAILABLE or not _kis_configured():
        return {"error": "KIS ë¯¸ì„¤ì •", "available": False}
    try:
        from kis_client import (
            _get_token, KIS_BASE, KIS_APP_KEY, KIS_APP_SECRET,
            KIS_ACCOUNT_NO, KIS_ACCOUNT_CODE, _TR, KIS_IS_MOCK as _mock,
        )
        import httpx as _hx

        token = await _get_token()
        async with _hx.AsyncClient(timeout=15) as client:
            resp = await client.get(
                f"{KIS_BASE}/uapi/domestic-stock/v1/trading/inquire-balance",
                headers={
                    "authorization": f"Bearer {token}",
                    "appkey": KIS_APP_KEY,
                    "appsecret": KIS_APP_SECRET,
                    "tr_id": _TR["balance"],
                },
                params={
                    "CANO": KIS_ACCOUNT_NO,
                    "ACNT_PRDT_CD": KIS_ACCOUNT_CODE,
                    "AFHR_FLPR_YN": "N", "OFL_YN": "",
                    "INQR_DVSN": "02", "UNPR_DVSN": "01",
                    "FUND_STTL_ICLD_YN": "N", "FNCG_AMT_AUTO_RDPT_YN": "N",
                    "PRCS_DVSN": "01", "CTX_AREA_FK100": "", "CTX_AREA_NK100": "",
                },
            )
            data = resp.json()
        return {
            "mode": "ëª¨ì˜íˆ¬ì" if _mock else "ì‹¤ê±°ë˜",
            "base_url": KIS_BASE,
            "account": f"****{KIS_ACCOUNT_NO[-4:]}",
            "tr_id": _TR["balance"],
            "http_status": resp.status_code,
            "rt_cd": data.get("rt_cd"),
            "msg_cd": data.get("msg_cd"),
            "msg1": data.get("msg1"),
            "output1_count": len(data.get("output1", [])),
            "output2_sample": data.get("output2", [{}])[:1],
        }
    except Exception as e:
        return {"error": str(e)}


def _enrich_overseas_balance_with_krw(result: dict) -> dict:
    """í•´ì™¸ ì”ê³  ê²°ê³¼ì— KRW í™˜ì‚° í•„ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    if not result.get("success"):
        return result
    fx = _get_fx_rate()
    result["fx_rate"] = fx
    for h in result.get("holdings", []):
        eval_usd = h.get("eval_amt", h.get("qty", 0) * h.get("current_price", 0))
        h["eval_amt_krw"] = round(eval_usd * fx)
        h["eval_profit_krw"] = round(h.get("eval_profit", 0) * fx)
    total_usd = result.get("total_eval_usd", 0)
    result["total_eval_krw"] = round(total_usd * fx)
    cash_usd = result.get("cash_usd", 0)
    result["cash_krw"] = round(cash_usd * fx)
    return result


@app.get("/api/trading/kis/debug-us")
async def kis_debug_us():
    """í•´ì™¸ì£¼ì‹ KIS API ì›ë³¸ ì‘ë‹µ í™•ì¸ (ë””ë²„ê·¸ìš©)."""
    if not _KIS_AVAILABLE or not _kis_configured():
        return {"error": "KIS ë¯¸ì„¤ì •", "available": False}
    try:
        from kis_client import (
            get_overseas_balance, KIS_IS_MOCK as _mock, KIS_BASE,
        )
        result = await get_overseas_balance()
        result = _enrich_overseas_balance_with_krw(result)
        return {
            "mode": "ëª¨ì˜íˆ¬ì" if _mock else "ì‹¤ê±°ë˜",
            "base_url": KIS_BASE,
            "result": result,
        }
    except Exception as e:
        return {"error": str(e)}


@app.get("/api/trading/cio/debug")
async def cio_debug():
    """CIO ì „ë¬¸ê°€ ë„êµ¬ ìŠ¤í‚¤ë§ˆ + í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ë””ë²„ê·¸ìš©)."""
    try:
        from ai_handler import _load_tool_schemas, ask_ai
        # CIO ì „ë¬¸ê°€(ì‹œí™©ë¶„ì„) í—ˆìš© ë„êµ¬
        detail = _AGENTS_DETAIL.get("market_analyst", {})
        allowed = detail.get("allowed_tools", [])
        schemas = _load_tool_schemas(allowed_tools=allowed)
        openai_tools = schemas.get("openai", [])
        # ìŠ¤í‚¤ë§ˆ ìš”ì•½ + í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
        tool_names = [t["function"]["name"] for t in openai_tools]
        tool_count = len(openai_tools)
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ë„êµ¬ ì—†ì´)
        test_result = await ask_ai("ì•ˆë…•í•˜ì„¸ìš”, í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. í•œ ì¤„ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.",
                                   system_prompt="ê°„ë‹¨íˆ ì‘ë‹µ", model="gpt-5.2")
        return {
            "specialist": "market_analyst (ì‹œí™©ë¶„ì„)",
            "allowed_tools": allowed,
            "openai_tool_count": tool_count,
            "openai_tool_names": tool_names,
            "openai_first_tool_schema": openai_tools[0] if openai_tools else None,
            "test_call_result": test_result.get("content", "")[:200] if "error" not in test_result else test_result["error"],
            "test_call_error": test_result.get("error"),
        }
    except Exception as e:
        return {"error": str(e)[:500]}


@app.get("/api/trading/cio/debug-tools")
async def cio_debug_tools():
    """CIO ì „ë¬¸ê°€ì—ê²Œ ë„êµ¬ í¬í•¨ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ (ì‹¤ì œ 400 ì—ëŸ¬ ì¬í˜„)."""
    try:
        from ai_handler import _load_tool_schemas, ask_ai
        detail = _AGENTS_DETAIL.get("market_analyst", {})
        allowed = detail.get("allowed_tools", [])
        schemas = _load_tool_schemas(allowed_tools=allowed)
        anthropic_tools = schemas.get("anthropic", [])
        # ë„êµ¬ í¬í•¨ í…ŒìŠ¤íŠ¸ â€” ì‹¤ì œ _call_agentì™€ ë™ì¼ ê²½ë¡œ
        async def _dummy_executor(name, args):
            return f"[í…ŒìŠ¤íŠ¸] {name} í˜¸ì¶œë¨: {args}"
        result = await ask_ai(
            "í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. global_market_tool action=index ë¡œ í˜„ì¬ ì‹œì¥ ì§€ìˆ˜ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
            system_prompt="ê°„ë‹¨íˆ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ì‘ë‹µí•˜ì„¸ìš”.",
            model="gpt-5.2",
            tools=anthropic_tools,
            tool_executor=_dummy_executor,
        )
        return {
            "success": "error" not in result,
            "content": result.get("content", "")[:300],
            "error": result.get("error"),
            "tools_count": len(anthropic_tools),
        }
    except Exception as e:
        return {"error": str(e)[:500], "type": type(e).__name__}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë²”ìš© ë””ë²„ê·¸ ì—”ë“œí¬ì¸íŠ¸ â€” ë²„ê·¸ ë°œìƒ ì‹œ CEOì—ê²Œ URL ì œê³µìš©
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/debug/ai-providers")
async def debug_ai_providers():
    """AI í”„ë¡œë°”ì´ë” ì—°ê²° ìƒíƒœ ì§„ë‹¨ â€” GPT/Claude/Gemini ì¤‘ ë­ê°€ ì¼œì ¸ìˆëŠ”ì§€ í™•ì¸."""
    import ai_handler as _ah
    providers = _ah.get_available_providers()
    env_keys = {
        "ANTHROPIC_API_KEY": bool(os.getenv("ANTHROPIC_API_KEY", "")),
        "OPENAI_API_KEY": bool(os.getenv("OPENAI_API_KEY", "")),
        "GOOGLE_API_KEY": bool(os.getenv("GOOGLE_API_KEY", "")),
    }
    client_info = {
        "anthropic": type(_ah._anthropic_client).__name__ if _ah._anthropic_client else None,
        "openai": type(_ah._openai_client).__name__ if _ah._openai_client else None,
        "google": type(_ah._google_client).__name__ if _ah._google_client else None,
    }
    env_key_map = {"anthropic": "ANTHROPIC_API_KEY", "openai": "OPENAI_API_KEY", "google": "GOOGLE_API_KEY"}
    exhausted = list(_ah._exhausted_providers)
    return {
        "providers_available": providers,
        "env_keys_present": env_keys,
        "client_types": client_info,
        "exhausted_providers": exhausted,
        "diagnosis": {
            k: ("ğŸ”´ í¬ë ˆë”§ ì†Œì§„" if k in _ah._exhausted_providers else
                "ì •ìƒ" if providers.get(k) else
                ("API í‚¤ ì—†ìŒ" if not env_keys.get(env_key_map[k]) else
                 "í‚¤ ìˆìœ¼ë‚˜ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨"))
            for k in ["anthropic", "openai", "google"]
        },
    }


@app.post("/api/debug/reset-exhausted-providers")
async def reset_exhausted_providers():
    """í¬ë ˆë”§ ì¶©ì „ í›„ ì†Œì§„ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    import ai_handler as _ah
    prev = list(_ah._exhausted_providers)
    _ah.reset_exhausted_providers()
    return {"reset": prev, "message": f"{len(prev)}ê°œ í”„ë¡œë°”ì´ë” ì†Œì§„ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ"}


@app.get("/api/debug/agent-calls")
async def debug_agent_calls():
    """ìµœê·¼ AI í˜¸ì¶œ ê¸°ë¡ 10ê±´ â€” ì–´ë–¤ ëª¨ë¸/í”„ë¡œë°”ì´ë”ë¡œ í˜¸ì¶œëëŠ”ì§€ í™•ì¸."""
    try:
        conn = __import__("db").get_connection()
        rows = conn.execute(
            "SELECT agent_id, model, provider, cost_usd, input_tokens, output_tokens, "
            "time_seconds, success, created_at FROM agent_calls "
            "ORDER BY created_at DESC LIMIT 10"
        ).fetchall()
        conn.close()
        return {
            "recent_calls": [
                {
                    "agent_id": r[0], "model": r[1], "provider": r[2],
                    "cost_usd": round(r[3], 4) if r[3] else 0,
                    "tokens": f"{r[4] or 0}+{r[5] or 0}",
                    "time_sec": round(r[6], 1) if r[6] else 0,
                    "success": bool(r[7]), "created_at": r[8],
                }
                for r in rows
            ],
            "total_count": len(rows),
        }
    except Exception as e:
        return {"error": str(e)[:300]}


@app.get("/api/debug/cio-signals")
async def debug_cio_signals():
    """CIO ì‹œê·¸ë„ íŒŒì‹± ìƒíƒœ â€” ì‹œê·¸ë„ì´ ì™œ ì•ˆ ëœ¨ëŠ”ì§€ í™•ì¸."""
    signals_data = load_setting("trading_signals") or {}
    decisions = load_setting("trading_decisions") or []
    return {
        "saved_signals": signals_data,
        "saved_decisions_count": len(decisions),
        "latest_decisions": decisions[-3:] if decisions else [],
        "watchlist": load_setting("watchlist") or [],
    }


@app.get("/api/debug/trading-execution")
async def debug_trading_execution():
    """ë§¤ë§¤ ì‹¤í–‰ ë””ë²„ê·¸ â€” auto_execute, ì„¤ì •, ìµœê·¼ ì‹œê·¸ë„, ì£¼ë¬¸ ìƒíƒœ í™•ì¸."""
    settings = _load_data("trading_settings", _default_trading_settings())
    signals = _load_data("trading_signals", [])
    history = _load_data("trading_history", [])
    latest_signal = signals[0] if signals else {}
    recent_history = history[:5]

    return {
        "settings": {
            "auto_execute": settings.get("auto_execute", False),
            "paper_trading": settings.get("paper_trading", True),
            "min_confidence": settings.get("min_confidence", 65),
            "order_size": settings.get("order_size", 0),
        },
        "kis_status": {
            "available": _KIS_AVAILABLE,
            "configured": _kis_configured() if _KIS_AVAILABLE else False,
            "is_mock": KIS_IS_MOCK,
        },
        "latest_signal": {
            "id": latest_signal.get("id", ""),
            "date": latest_signal.get("date", ""),
            "parsed_count": len(latest_signal.get("parsed_signals", [])),
            "parsed_signals": latest_signal.get("parsed_signals", []),
            "manual_run": latest_signal.get("manual_run", False),
        },
        "recent_orders": [{
            "id": h.get("id"), "date": h.get("date"),
            "ticker": h.get("ticker"), "action": h.get("action"),
            "qty": h.get("qty"), "status": h.get("status"),
        } for h in recent_history],
        "note": "ìˆ˜ë™ ì¦‰ì‹œ ì‹¤í–‰ì€ auto_execute ë¬´ê´€í•˜ê²Œ í•­ìƒ ë§¤ë§¤ ì§„í–‰ (2026-02-21 ìˆ˜ì •)",
    }


@app.get("/api/debug/trading-holdings")
async def debug_trading_holdings():
    """ë§¤ë§¤ ë³´ìœ ì¢…ëª© ë””ë²„ê·¸ â€” KIS ì”ê³  vs ë‚´ë¶€ í¬íŠ¸í´ë¦¬ì˜¤ vs ê±°ë˜ë‚´ì—­ ë¹„êµ."""
    portfolio = _load_data("trading_portfolio", _default_portfolio())
    history = _load_data("trading_history", [])
    settings = _load_data("trading_settings", _default_trading_settings())

    # KIS ì‹¤ê±°ë˜ ì”ê³ 
    kis_bal = None
    if _KIS_AVAILABLE and _kis_configured():
        try:
            kis_bal = await _kis_balance()
        except Exception as e:
            kis_bal = {"error": str(e)}

    # KIS ëª¨ì˜ ì”ê³ 
    kis_mock = None
    try:
        from kis_client import get_mock_balance
        kis_mock = await get_mock_balance()
    except Exception as e:
        kis_mock = {"error": str(e)}

    # ìµœê·¼ ë§¤ìˆ˜ ê¸°ë¡
    recent_buys = [t for t in history[:30] if t.get("action") == "buy"]

    return {
        "kis_available": _KIS_AVAILABLE,
        "kis_configured": _kis_configured() if _KIS_AVAILABLE else False,
        "kis_is_mock": KIS_IS_MOCK,
        "paper_trading": settings.get("paper_trading", True),
        "kis_real_balance": kis_bal,
        "kis_mock_balance": kis_mock,
        "internal_portfolio": {
            "cash": portfolio.get("cash", 0),
            "holdings": portfolio.get("holdings", []),
            "updated_at": portfolio.get("updated_at"),
        },
        "recent_buys": recent_buys[:10],
    }


@app.get("/api/debug/kis-token")
async def debug_kis_token():
    """KIS í† í° ìƒíƒœ ë””ë²„ê·¸ â€” í† í° ìœ íš¨ì„±, ë§Œë£Œì‹œê°„, ìºì‹œ ìƒíƒœ, ì¿¨ë‹¤ìš´."""
    info = {"kis_available": _KIS_AVAILABLE, "configured": False}
    if not _KIS_AVAILABLE:
        info["error"] = "kis_client ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"
        return info
    try:
        from kis_client import (
            is_configured, KIS_IS_MOCK, KIS_BASE,
            _token_cache, _last_token_request, _TOKEN_COOLDOWN_SEC,
            _last_token_request_domestic, _last_token_request_overseas,
            _last_balance_cache, _last_mock_balance_cache,
            KIS_ACCOUNT_NO, KIS_ACCOUNT_CODE,
        )
        info["configured"] = is_configured()
        info["is_mock"] = KIS_IS_MOCK
        info["base_url"] = KIS_BASE
        info["account"] = f"{KIS_ACCOUNT_NO[:4]}****-{KIS_ACCOUNT_CODE}" if KIS_ACCOUNT_NO else "ë¯¸ì„¤ì •"

        # í† í° ìƒíƒœ
        now = datetime.now()
        token = _token_cache.get("token")
        expires = _token_cache.get("expires")
        if token and expires:
            remaining = (expires - now).total_seconds()
            info["token"] = {
                "status": "ìœ íš¨" if remaining > 0 else "ë§Œë£Œë¨",
                "masked": f"{token[:8]}...{token[-4:]}" if token else None,
                "expires": expires.isoformat() if expires else None,
                "remaining_seconds": max(0, int(remaining)),
                "remaining_human": f"{int(remaining // 3600)}ì‹œê°„ {int((remaining % 3600) // 60)}ë¶„" if remaining > 0 else "ë§Œë£Œë¨",
            }
        else:
            info["token"] = {"status": "í† í° ì—†ìŒ (ì•„ì§ ë°œê¸‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì„œë²„ ì¬ì‹œì‘ë¨)"}

        # ì¿¨ë‹¤ìš´ ìƒíƒœ â€” êµ­ë‚´/í•´ì™¸ ë¶„ë¦¬
        def _cooldown_info(last_req, label):
            if last_req:
                elapsed = (now - last_req).total_seconds()
                cooldown_remaining = max(0, _TOKEN_COOLDOWN_SEC - elapsed)
                return {
                    "market": label,
                    "last_request": last_req.isoformat(),
                    "elapsed_seconds": int(elapsed),
                    "remaining_seconds": int(cooldown_remaining),
                    "can_request": cooldown_remaining <= 0,
                }
            return {"market": label, "last_request": None, "can_request": True}

        info["cooldown"] = {
            "domestic": _cooldown_info(_last_token_request_domestic, "êµ­ë‚´"),
            "overseas": _cooldown_info(_last_token_request_overseas, "í•´ì™¸"),
            "last_any": _last_token_request.isoformat() if _last_token_request else None,
        }

        # ì”ê³  ìºì‹œ ìƒíƒœ
        info["balance_cache"] = {
            "real_cached": bool(_last_balance_cache),
            "mock_cached": bool(_last_mock_balance_cache),
            "real_total_krw": _last_balance_cache.get("total_krw") if _last_balance_cache else None,
            "mock_total_krw": _last_mock_balance_cache.get("total_krw") if _last_mock_balance_cache else None,
        }
    except Exception as e:
        info["error"] = str(e)
    return info


@app.get("/api/debug/auto-trading-pipeline")
async def debug_auto_trading_pipeline():
    """ìë™ë§¤ë§¤ ì „ì²´ íŒŒì´í”„ë¼ì¸ ë””ë²„ê·¸ â€” KIS ì—°ê²°ë¶€í„° ì£¼ë¬¸ ì‹¤í–‰ê¹Œì§€ ì „ ë‹¨ê³„."""
    settings = _load_data("trading_settings", _default_trading_settings())
    signals = _load_data("trading_signals", [])
    watchlist = _load_data("trading_watchlist", [])
    history = _load_data("trading_history", [])

    # KIS ì—°ê²° ìƒíƒœ
    kis_ok = _KIS_AVAILABLE and _kis_configured()

    # AI ì—°ê²° ìƒíƒœ
    providers = get_available_providers()

    # ìµœê·¼ ì‹œê·¸ë„
    latest = signals[0] if signals else {}
    parsed = latest.get("parsed_signals", [])
    buy_signals = [s for s in parsed if s.get("action") == "buy"]
    sell_signals = [s for s in parsed if s.get("action") == "sell"]

    # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ìƒíƒœ
    pipeline = {
        "1_ai_connection": {
            "status": "OK" if any(providers.values()) else "FAIL",
            "providers": {k: "ì—°ê²°ë¨" if v else "ë¯¸ì—°ê²°" for k, v in providers.items()},
        },
        "2_watchlist": {
            "status": "OK" if watchlist else "FAIL",
            "count": len(watchlist),
            "tickers": [f"{w['name']}({w['ticker']})" for w in watchlist[:5]],
        },
        "3_signal_generation": {
            "status": "OK" if signals else "FAIL",
            "latest_date": latest.get("date", "ì—†ìŒ"),
            "analyzed_by": latest.get("analyzed_by", "ì—†ìŒ"),
            "buy_count": len(buy_signals),
            "sell_count": len(sell_signals),
            "hold_count": len([s for s in parsed if s.get("action") == "hold"]),
        },
        "4_kis_connection": {
            "status": "OK" if kis_ok else "FAIL",
            "kis_available": _KIS_AVAILABLE,
            "kis_configured": _kis_configured() if _KIS_AVAILABLE else False,
            "is_mock": KIS_IS_MOCK,
        },
        "5_order_execution": {
            "status": "OK" if kis_ok else "BLOCKED",
            "paper_trading": settings.get("paper_trading", True),
            "auto_execute": settings.get("auto_execute", False),
            "note": "ìˆ˜ë™ ì¦‰ì‹œì‹¤í–‰(ë²„íŠ¼)ì€ paper_trading ë¬´ì‹œí•˜ê³  KIS ì‹¤ì£¼ë¬¸ (2026-02-21 ìˆ˜ì •)",
            "min_confidence": settings.get("min_confidence", 65),
            "order_size": settings.get("order_size", 0),
        },
        "6_recent_orders": {
            "count": len(history),
            "last_5": [{
                "date": h.get("date", ""),
                "ticker": h.get("ticker", ""),
                "action": h.get("action", ""),
                "status": h.get("status", ""),
            } for h in history[:5]],
        },
    }

    # ì „ì²´ íŒì •
    all_ok = all(
        pipeline[k]["status"] == "OK"
        for k in ["1_ai_connection", "2_watchlist", "4_kis_connection"]
    )

    return {
        "overall": "READY" if all_ok else "NOT READY",
        "pipeline": pipeline,
        "quick_diagnosis": (
            "ëª¨ë“  ë‹¨ê³„ ì •ìƒ â€” ì¦‰ì‹œë¶„ì„ ë²„íŠ¼ìœ¼ë¡œ ë§¤ë§¤ ê°€ëŠ¥"
            if all_ok else
            " / ".join([
                f"[{k}] {pipeline[k]['status']}"
                for k in pipeline
                if pipeline[k]["status"] != "OK"
            ])
        ),
    }


@app.get("/api/debug/fx-rate")
async def debug_fx_rate():
    """í™˜ìœ¨ ìƒíƒœ ë””ë²„ê·¸ â€” í˜„ì¬ í™˜ìœ¨, ë§ˆì§€ë§‰ ê°±ì‹  ì‹œê°„, ìˆ˜ë™ ê°±ì‹ ."""
    current_rate = _get_fx_rate()
    last_update = app_state.last_fx_update
    since_update = time.time() - last_update if last_update > 0 else -1
    return {
        "current_rate": current_rate,
        "last_updated": datetime.fromtimestamp(last_update, tz=KST).isoformat() if last_update > 0 else "ê°±ì‹  ì•ˆë¨ (ê¸°ë³¸ê°’ ì‚¬ìš© ì¤‘)",
        "seconds_since_update": round(since_update) if since_update >= 0 else None,
        "next_update_in": max(0, round(_FX_UPDATE_INTERVAL - since_update)) if since_update >= 0 else "ë¯¸ì •",
        "source": "yfinance (USDKRW=X)",
    }


@app.post("/api/debug/fx-rate/refresh")
async def refresh_fx_rate():
    """í™˜ìœ¨ ì¦‰ì‹œ ê°±ì‹ ."""
    new_rate = await _update_fx_rate()
    if new_rate:
        return {"success": True, "rate": new_rate}
    return {"success": False, "rate": _get_fx_rate(), "message": "ê°±ì‹  ì‹¤íŒ¨ â€” ê¸°ì¡´ ê°’ ìœ ì§€"}


@app.get("/api/debug/server-logs")
async def debug_server_logs(lines: int = 50, service: str = "corthex"):
    """ì„œë²„ ë¡œê·¸ ë””ë²„ê·¸ â€” SSH í„°ë„ ë˜ëŠ” localhostì—ì„œë§Œ ì ‘ê·¼ ê°€ëŠ¥.
    Cloudflareë¥¼ ìš°íšŒí•˜ì—¬ ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    service: corthex(ì•± ë¡œê·¸), nginx-error, nginx-access
    """
    import subprocess
    # localhost ìš”ì²­ë§Œ í—ˆìš© (ë³´ì•ˆ)
    log_commands = {
        "corthex": f"journalctl -u corthex --no-pager -n {min(lines, 200)} --output=short-iso",
        "nginx-error": f"tail -n {min(lines, 200)} /var/log/nginx/error.log",
        "nginx-access": f"tail -n {min(lines, 200)} /var/log/nginx/access.log",
    }
    cmd = log_commands.get(service)
    if not cmd:
        return {"error": f"unknown service: {service}", "available": list(log_commands.keys())}
    try:
        result = subprocess.run(
            cmd, shell=True, capture_output=True, text=True, timeout=10
        )
        log_lines = result.stdout.strip().split("\n") if result.stdout else []
        return {
            "service": service,
            "lines": len(log_lines),
            "logs": log_lines[-min(lines, 200):],
            "stderr": result.stderr[:500] if result.stderr else None,
        }
    except subprocess.TimeoutExpired:
        return {"error": "timeout (10s)", "service": service}
    except Exception as e:
        return {"error": str(e), "service": service}


# â”€â”€ mock/balance, overseas/balance, overseas/mock-balance, portfolio/history,
#    portfolio/set-initial, portfolio/reset, mock/holdings, shadow/compare,
#    cio/predictions, cio/performance-summary â†’ handlers/trading_handler.pyë¡œ ë¶„ë¦¬ â”€â”€


# â”€â”€ ì§€ì‹íŒŒì¼ API â†’ handlers/knowledge_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.knowledge_handler import router as knowledge_router
app.include_router(knowledge_router)


# â”€â”€ ì—ì´ì „íŠ¸ ë©”ëª¨ë¦¬ API â†’ handlers/memory_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.memory_handler import router as memory_router
app.include_router(memory_router)


# â”€â”€ í”¼ë“œë°± API â†’ handlers/feedback_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.feedback_handler import router as feedback_router
app.include_router(feedback_router)


# â”€â”€ ëŒ€í™” API â†’ handlers/conversation_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.conversation_handler import router as conversation_router
app.include_router(conversation_router)


# â”€â”€ ì•„í‚¤í…ì²˜ ë§µ â†’ handlers/architecture_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.architecture_handler import router as architecture_router
app.include_router(architecture_router)

# â”€â”€ SNS ì—°ë™ â†’ handlers/sns_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.sns_handler import router as sns_router
app.include_router(sns_router)


# â”€â”€ ì¸ì¦ â†’ handlers/auth_handler.pyì—ì„œ ë¶„ë¦¬ë¨ (ìœ„ìª½ì—ì„œ include_router ì™„ë£Œ) â”€â”€

# â”€â”€ í—¬ìŠ¤ì²´í¬ â†’ handlers/health_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.health_handler import router as health_router
app.include_router(health_router)


# â”€â”€ í’ˆì§ˆê²€ìˆ˜(Quality) API â†’ handlers/quality_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.quality_handler import router as quality_router
app.include_router(quality_router)


# â”€â”€ íŠ¸ë ˆì´ë”© CRUD API â†’ handlers/trading_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.trading_handler import router as trading_router
app.include_router(trading_router)


# â”€â”€ ì—ì´ì „íŠ¸ ì„¤ì •(ì†Œìš¸/ëª¨ë¸/ì¶”ë¡ ), ì˜ˆì‚°, ëª¨ë¸ëª©ë¡ â†’ handlers/agent_handler.pyë¡œ ë¶„ë¦¬ â”€â”€

# â”€â”€ í™œë™ ë¡œê·¸ Â· ìœ„ì„ ë¡œê·¸ Â· ë‚´ë¶€í†µì‹ (Comms) API â†’ handlers/activity_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.activity_handler import router as activity_router
app.include_router(activity_router)


async def _broadcast_comms(msg_data: dict):
    """SSE í´ë¼ì´ì–¸íŠ¸ë“¤ì—ê²Œ ë‚´ë¶€í†µì‹  ë©”ì‹œì§€ broadcast."""
    await wm.broadcast_sse(msg_data)


# â”€â”€ íŒ€ì¥ ê°„ í˜‘ì˜(Consult) API â†’ handlers/consult_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.consult_handler import router as consult_router
app.include_router(consult_router)

# â”€â”€ ì•„ì¹´ì´ë¸Œ API â†’ handlers/archive_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.archive_handler import router as archive_router
app.include_router(archive_router)

# â”€â”€ í…”ë ˆê·¸ë¨ ìƒíƒœ/í…ŒìŠ¤íŠ¸ API â†’ handlers/telegram_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.telegram_handler import router as telegram_router
app.include_router(telegram_router)

# â”€â”€ Soul ìë™ ì§„í™” API â†’ handlers/soul_evolution_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.soul_evolution_handler import router as soul_evolution_router
app.include_router(soul_evolution_router)

# â”€â”€ Soul Gym ê²½ìŸ ì§„í™” API â†’ handlers/soul_gym_handler.py â”€â”€
from handlers.soul_gym_handler import router as soul_gym_router
app.include_router(soul_gym_router)

# â”€â”€ AGORA: AI ë²•í•™ í† ë¡  ì‹œìŠ¤í…œ â†’ handlers/agora_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.agora_handler import router as agora_router
app.include_router(agora_router)


# â”€â”€ í…”ë ˆê·¸ë¨ ë´‡ â”€â”€
# ì£¼ì˜: python-telegram-bot ë¯¸ì„¤ì¹˜ ì‹œì—ë„ ì„œë²„ê°€ ì •ìƒ ì‘ë™í•´ì•¼ í•¨
# ëª¨ë“  í…”ë ˆê·¸ë¨ ê´€ë ¨ ì½”ë“œëŠ” _telegram_available ì²´í¬ í›„ì—ë§Œ ì‹¤í–‰

# app_state.telegram_app â†’ app_state.telegram_app ì§ì ‘ ì‚¬ìš©


async def _start_telegram_bot() -> None:
    """í…”ë ˆê·¸ë¨ ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤ (FastAPI ì´ë²¤íŠ¸ ë£¨í”„ ì•ˆì—ì„œ ì‹¤í–‰)."""


    _log(f"[TG] ë´‡ ì‹œì‘ ì‹œë„ (_telegram_available={_telegram_available})")

    if not _telegram_available:
        _log("[TG] âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ â€” ê±´ë„ˆëœ€")
        return

    token = os.getenv("TELEGRAM_BOT_TOKEN", "")
    _log(f"[TG] í† í° ì¡´ì¬: {bool(token)} (ê¸¸ì´: {len(token)})")
    if not token:
        _log("[TG] âŒ í† í° ë¯¸ì„¤ì • â€” ê±´ë„ˆëœ€")
        _diag["tg_error"] = "TELEGRAM_BOT_TOKEN í™˜ê²½ë³€ìˆ˜ ì—†ìŒ"
        return

    try:
        _log("[TG] Application ë¹Œë“œ ì¤‘...")
        app_state.telegram_app = Application.builder().token(token).build()

        # â”€â”€ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë“¤ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ëœ ê²½ìš°ì—ë§Œ ì •ì˜) â”€â”€

        async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            chat_id = update.effective_chat.id
            ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
            if not ceo_id:
                logger.info("í…”ë ˆê·¸ë¨ chat_id ê°ì§€: %s", chat_id)
                await update.message.reply_text(
                    f"CORTHEX HQ í…”ë ˆê·¸ë¨ ë´‡ì…ë‹ˆë‹¤.\n\n"
                    f"ë‹¹ì‹ ì˜ chat_id: `{chat_id}`\n\n"
                    f"ì„œë²„ í™˜ê²½ë³€ìˆ˜ì— TELEGRAM_CEO_CHAT_ID={chat_id} ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.",
                    parse_mode="Markdown",
                )
                return
            if str(chat_id) != ceo_id:
                await update.message.reply_text("ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            await update.message.reply_text(
                "*CORTHEX HQ í…”ë ˆê·¸ë¨ ë´‡*\n\n"
                "CEO ì¸ì¦ ì™„ë£Œ.\n"
                "24ì‹œê°„ ì„œë²„ì—ì„œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.\n\n"
                "/help ë¡œ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.",
                parse_mode="Markdown",
            )

        async def cmd_help(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            await update.message.reply_text(
                "*CORTHEX HQ ì‚¬ìš©ë²•*\n\n"
                "*ì •ë³´*\n"
                "/agents â€” ì—ì´ì „íŠ¸ ëª©ë¡\n"
                "/health â€” ì„œë²„ ìƒíƒœ\n"
                "/status â€” ë°°ì¹˜ ì§„í–‰ í˜„í™©\n"
                "/budget â€” ì˜¤ëŠ˜ ë¹„ìš© / í•œë„ ë³€ê²½\n\n"
                "*AI ëª…ë ¹*\n"
                "/í† ë¡  \\[ì£¼ì œ\\] â€” ì„ì› í† ë¡  (2ë¼ìš´ë“œ)\n"
                "/ì‹¬ì¸µí† ë¡  \\[ì£¼ì œ\\] â€” ì‹¬ì¸µ ì„ì› í† ë¡  (3ë¼ìš´ë“œ)\n"
                "/ì „ì²´ \\[ë©”ì‹œì§€\\] â€” 29ëª… ë™ì‹œ ì§€ì‹œ\n"
                "/ìˆœì°¨ \\[ì‘ì—…\\] â€” ì—ì´ì „íŠ¸ ë¦´ë ˆì´ ìˆœì°¨ í˜‘ì—…\n"
                "@ì—ì´ì „íŠ¸ëª… \\[ì§€ì‹œ\\] â€” íŠ¹ì • ì—ì´ì „íŠ¸ ì§ì ‘ ì§€ì‹œ\n\n"
                "*ëª¨ë“œ ì „í™˜*\n"
                "/rt â€” ì‹¤ì‹œê°„ ëª¨ë“œ (AI ì¦‰ì‹œ ë‹µë³€)\n"
                "/batch â€” ë°°ì¹˜ ëª¨ë“œ\n\n"
                "*ì„¤ì •*\n"
                "/models â€” ì „ì› ëª¨ë¸ ë³€ê²½ (3ë‹¨ê³„ ë²„íŠ¼)\n"
                "/pause â€” AI ì²˜ë¦¬ ì¤‘ë‹¨\n"
                "/resume â€” AI ì²˜ë¦¬ ì¬ê°œ\n\n"
                "ì¼ë°˜ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ë©´ AIê°€ ìë™ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.",
                parse_mode="Markdown",
            )

        async def cmd_agents(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            divisions = {}
            for a in AGENTS:
                div = a.get("division", "ê¸°íƒ€")
                divisions.setdefault(div, []).append(a)
            lines = ["*CORTHEX HQ ì—ì´ì „íŠ¸ ëª©ë¡*\n"]
            div_labels = {
                "secretary": "ë¹„ì„œì‹¤",
                "leet_master.tech": "ê¸°ìˆ ê°œë°œì²˜ (CTO)",
                "leet_master.strategy": "ì‚¬ì—…ê¸°íšì²˜ (CSO)",
                "leet_master.legal": "ë²•ë¬´Â·IPì²˜ (CLO)",
                "leet_master.marketing": "ë§ˆì¼€íŒ…Â·ê³ ê°ì²˜ (CMO)",
                "finance.investment": "íˆ¬ìë¶„ì„ì²˜ (CIO)",
                "publishing": "ì¶œíŒÂ·ê¸°ë¡ì²˜ (CPO)",
            }
            for div, agents_list in divisions.items():
                label = div_labels.get(div, div)
                lines.append(f"\n*{label}* ({len(agents_list)}ëª…)")
                for a in agents_list:
                    icon = "ğŸ‘”" if a["role"] == "manager" else "ğŸ‘¤"
                    display = a.get("telegram_code", a["name_ko"])
                    lines.append(f"  {icon} {display}")
            lines.append(f"\nì´ {len(AGENTS)}ëª…")
            await update.message.reply_text("\n".join(lines), parse_mode="Markdown")

        async def cmd_health(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            now = datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S")
            await update.message.reply_text(
                f"*ì„œë²„ ìƒíƒœ*\n\n"
                f"ìƒíƒœ: ì •ìƒ ìš´ì˜ ì¤‘\n"
                f"ì„œë²„: Oracle Cloud (ì¶˜ì²œ)\n"
                f"ì—ì´ì „íŠ¸: {len(AGENTS)}ëª… ëŒ€ê¸° ì¤‘\n"
                f"ì‹œê°„: {now} KST",
                parse_mode="Markdown",
            )

        async def cmd_rt(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            """ì‹¤ì‹œê°„ ëª¨ë“œ ì „í™˜ (/rt)."""
            if not _is_tg_ceo(update):
                return
            save_setting("tg_mode", "realtime")
            await update.message.reply_text(
                "ğŸ”´ *ì‹¤ì‹œê°„ ëª¨ë“œ*ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.\n\n"
                "ì´ì œ ë³´ë‚´ì‹œëŠ” ë©”ì‹œì§€ì— AIê°€ ì¦‰ì‹œ ë‹µë³€í•©ë‹ˆë‹¤.",
                parse_mode="Markdown",
            )

        async def cmd_batch(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            """ë°°ì¹˜ ëª¨ë“œ ì „í™˜ (/batch)."""
            if not _is_tg_ceo(update):
                return
            save_setting("tg_mode", "batch")
            await update.message.reply_text(
                "ğŸ“¦ *ë°°ì¹˜ ëª¨ë“œ*ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.\n\n"
                "ë©”ì‹œì§€ë¥¼ ì ‘ìˆ˜ë§Œ í•˜ê³ , AI ì²˜ë¦¬ëŠ” í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                parse_mode="Markdown",
            )

        # â”€â”€ /status â€” ë°°ì¹˜ ì§„í–‰ ëª©ë¡ â”€â”€
        async def cmd_status(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            chains = load_setting("batch_chains") or []
            active = [c for c in chains if c.get("status") in ("running", "pending")]
            if not active:
                await update.message.reply_text("í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            lines = [f"*ì§„í–‰ ì¤‘ì¸ ë°°ì¹˜ ({len(active)}ê±´)*\n"]
            for c in active[:10]:
                step = c.get("step", "?")
                text_preview = c.get("text", "")[:40]
                chain_id = c.get("chain_id", "?")[:8]
                lines.append(f"â€¢ `{chain_id}` | {step} | {text_preview}")
            await update.message.reply_text("\n".join(lines), parse_mode="Markdown")

        # â”€â”€ /budget â€” ì˜¤ëŠ˜ ì§€ì¶œ í™•ì¸/ë³€ê²½ â”€â”€
        async def cmd_budget(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            args = (update.message.text or "").split()
            today_cost = get_today_cost()
            daily_limit = load_setting("daily_budget_usd") or 10
            if len(args) >= 2:
                try:
                    new_limit = float(args[1])
                    save_setting("daily_budget_usd", new_limit)
                    await update.message.reply_text(
                        f"ğŸ’° ì¼ì¼ ì˜ˆì‚°ì„ *${new_limit:.2f}*ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\nì˜¤ëŠ˜ ì‚¬ìš©: ${today_cost:.4f}",
                        parse_mode="Markdown",
                    )
                    return
                except ValueError:
                    pass
            pct = (today_cost / daily_limit * 100) if daily_limit > 0 else 0
            await update.message.reply_text(
                f"ğŸ’° *ì˜¤ëŠ˜ ë¹„ìš© í˜„í™©*\n\n"
                f"ì‚¬ìš©: ${today_cost:.4f}\n"
                f"í•œë„: ${daily_limit:.2f}\n"
                f"ì‚¬ìš©ë¥ : {pct:.1f}%\n\n"
                f"í•œë„ ë³€ê²½: `/budget 15` (15ë‹¬ëŸ¬ë¡œ ë³€ê²½)",
                parse_mode="Markdown",
            )

        # â”€â”€ /pause, /resume â€” AI ì²˜ë¦¬ ì¤‘ë‹¨/ì¬ê°œ â”€â”€
        async def cmd_pause(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            save_setting("ai_paused", True)
            await update.message.reply_text("â¸ *AI ì²˜ë¦¬ë¥¼ ì¼ì‹œ ì¤‘ë‹¨*í–ˆìŠµë‹ˆë‹¤.\n\n`/resume`ìœ¼ë¡œ ì¬ê°œí•˜ì„¸ìš”.", parse_mode="Markdown")

        async def cmd_resume(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            save_setting("ai_paused", False)
            await update.message.reply_text("â–¶ï¸ *AI ì²˜ë¦¬ë¥¼ ì¬ê°œ*í–ˆìŠµë‹ˆë‹¤.", parse_mode="Markdown")

        # â”€â”€ /models â€” 3ë‹¨ê³„ ì¸ë¼ì¸ ë²„íŠ¼ìœ¼ë¡œ ëª¨ë¸ ë³€ê²½ â”€â”€
        # í”„ë¡œë°”ì´ë”ë³„ ëª¨ë¸ ëª©ë¡ (ì½”ë“œ ë‚´ _MODEL_CATALOGê³¼ ë™ê¸°í™”)
        _TG_MODELS = {
            "Anthropic": [
                ("claude-opus-4-6", "Opus 4.6", ["xhigh", "high", "low", "ì—†ìŒ"]),
                ("claude-sonnet-4-6", "Sonnet 4.6", ["high", "medium", "low", "ì—†ìŒ"]),
                ("claude-haiku-4-5-20251001", "Claude Haiku 4.5", []),
            ],
            "OpenAI": [
                ("gpt-5.2-pro", "GPT-5.2 Pro", ["xhigh", "high", "medium", "ì—†ìŒ"]),
                ("gpt-5.2", "GPT-5.2", ["xhigh", "high", "medium", "low", "ì—†ìŒ"]),
                ("gpt-5", "GPT-5", ["xhigh", "high", "low", "ì—†ìŒ"]),
                ("gpt-5-mini", "GPT-5 Mini", []),
            ],
            "Google": [
                ("gemini-3.1-pro-preview", "Gemini 3.1 Pro Preview", ["high", "low", "ì—†ìŒ"]),
                ("gemini-2.5-pro", "Gemini 2.5 Pro", ["high", "low", "ì—†ìŒ"]),
                ("gemini-2.5-flash", "Gemini 2.5 Flash", []),
            ],
        }

        async def cmd_models(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            current = load_setting("global_model_override") or {}
            cur_model = current.get("model", "ì—†ìŒ")
            cur_reason = current.get("reasoning", "ì—†ìŒ")
            buttons = [
                [InlineKeyboardButton("ğŸŸ£ Anthropic", callback_data="mdl_p_Anthropic")],
                [InlineKeyboardButton("ğŸŸ¢ OpenAI", callback_data="mdl_p_OpenAI")],
                [InlineKeyboardButton("ğŸ”µ Google", callback_data="mdl_p_Google")],
            ]
            await update.message.reply_text(
                f"*ì „ì› ëª¨ë¸ ë³€ê²½*\n\ní˜„ì¬: `{cur_model}` (ì¶”ë¡ : {cur_reason})\n\ní”„ë¡œë°”ì´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
                parse_mode="Markdown",
                reply_markup=InlineKeyboardMarkup(buttons),
            )

        async def models_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            query = update.callback_query
            await query.answer()
            data = query.data

            # 1ë‹¨ê³„: í”„ë¡œë°”ì´ë” ì„ íƒ â†’ ëª¨ë¸ ëª©ë¡ í‘œì‹œ
            if data.startswith("mdl_p_"):
                provider = data[6:]
                models_list = _TG_MODELS.get(provider, [])
                buttons = []
                for model_id, label, _ in models_list:
                    buttons.append([InlineKeyboardButton(label, callback_data=f"mdl_m_{model_id}")])
                buttons.append([InlineKeyboardButton("Â« ë’¤ë¡œ", callback_data="mdl_back")])
                await query.edit_message_text(
                    f"*{provider}* ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
                    parse_mode="Markdown",
                    reply_markup=InlineKeyboardMarkup(buttons),
                )

            # 2ë‹¨ê³„: ëª¨ë¸ ì„ íƒ â†’ ì¶”ë¡  ê°•ë„ í‘œì‹œ (ë˜ëŠ” ë°”ë¡œ ì €ì¥)
            elif data.startswith("mdl_m_"):
                model_id = data[6:]
                # ëª¨ë¸ì˜ ì¶”ë¡  ë ˆë²¨ ì°¾ê¸°
                reasoning_levels = []
                for provider, models_list in _TG_MODELS.items():
                    for mid, label, levels in models_list:
                        if mid == model_id:
                            reasoning_levels = levels
                            break

                if not reasoning_levels:
                    # ì¶”ë¡  ì—†ìŒ â†’ ë°”ë¡œ ì €ì¥ (ì›¹ê³¼ ë™ì¼í•œ í‚¤ ì‚¬ìš© â†’ ë™ê¸°í™”)
                    save_setting("model_mode", "manual")
                    save_setting("model_override", model_id)
                    save_setting("global_model_override", {"model": model_id, "reasoning": "ì—†ìŒ"})
                    await query.edit_message_text(f"âœ… ì „ì› ëª¨ë¸ì„ `{model_id}` ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\n(ì¶”ë¡ : ì—†ìŒ)", parse_mode="Markdown")
                else:
                    # ì¶”ë¡  ë ˆë²¨ ì„ íƒ ë²„íŠ¼
                    context.user_data["pending_model"] = model_id
                    buttons = []
                    for level in reasoning_levels:
                        buttons.append([InlineKeyboardButton(level, callback_data=f"mdl_r_{level}")])
                    buttons.append([InlineKeyboardButton("Â« ë’¤ë¡œ", callback_data="mdl_back")])
                    await query.edit_message_text(
                        f"*{model_id}*\nì¶”ë¡  ê°•ë„ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
                        parse_mode="Markdown",
                        reply_markup=InlineKeyboardMarkup(buttons),
                    )

            # 3ë‹¨ê³„: ì¶”ë¡  ê°•ë„ ì„ íƒ â†’ ì €ì¥
            elif data.startswith("mdl_r_"):
                level = data[6:]
                model_id = context.user_data.get("pending_model", "")
                if model_id:
                    # ì›¹ê³¼ ë™ì¼í•œ í‚¤ ì‚¬ìš© â†’ ë™ê¸°í™”
                    save_setting("model_mode", "manual")
                    save_setting("model_override", model_id)
                    save_setting("global_model_override", {"model": model_id, "reasoning": level})
                    await query.edit_message_text(
                        f"âœ… ì „ì› ëª¨ë¸ì„ `{model_id}` ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.\n(ì¶”ë¡ : {level})",
                        parse_mode="Markdown",
                    )
                else:
                    await query.edit_message_text("âŒ ëª¨ë¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. /modelsë¥¼ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

            # ë’¤ë¡œê°€ê¸°
            elif data == "mdl_back":
                current = load_setting("global_model_override") or {}
                cur_model = current.get("model", "ì—†ìŒ")
                cur_reason = current.get("reasoning", "ì—†ìŒ")
                buttons = [
                    [InlineKeyboardButton("ğŸŸ£ Anthropic", callback_data="mdl_p_Anthropic")],
                    [InlineKeyboardButton("ğŸŸ¢ OpenAI", callback_data="mdl_p_OpenAI")],
                    [InlineKeyboardButton("ğŸ”µ Google", callback_data="mdl_p_Google")],
                ]
                await query.edit_message_text(
                    f"*ì „ì› ëª¨ë¸ ë³€ê²½*\n\ní˜„ì¬: `{cur_model}` (ì¶”ë¡ : {cur_reason})\n\ní”„ë¡œë°”ì´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
                    parse_mode="Markdown",
                    reply_markup=InlineKeyboardMarkup(buttons),
                )

        # â”€â”€ ì¥ê¸° ì‹¤í–‰ ëª…ë ¹ ê³µí†µ í—¬í¼ (í† ë¡ /ì „ì²´/ìˆœì°¨ ë“± 2~10ë¶„ ì†Œìš” ëª…ë ¹) â”€â”€
        async def _tg_long_command(update_obj, task_text, target_agent_id=None):
            """ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰í•˜ê³  ì™„ë£Œ ì‹œ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ê²°ê³¼ ì „ì†¡."""
            chat_id = str(update_obj.effective_chat.id)
            task = create_task(task_text, source="telegram")
            cmd_name = task_text.split()[0]
            await update_obj.message.reply_text(
                f"â³ *{cmd_name}* ì‹œì‘ (#{task['task_id']})\n"
                f"ì™„ë£Œ ì‹œ ê²°ê³¼ë¥¼ ë³´ë‚´ë“œë¦½ë‹ˆë‹¤. (2~10ë¶„ ì†Œìš”)",
                parse_mode="Markdown",
            )

            async def _bg(t, tid, cid):
                try:
                    update_task(tid, status="running")
                    result = await _process_ai_command(t, tid, target_agent_id=target_agent_id)
                    content = result.get("content", result.get("error", "ê²°ê³¼ ì—†ìŒ"))
                    cost = result.get("cost_usd", result.get("total_cost_usd", 0))
                    tg_agent_id = result.get("agent_id", "chief_of_staff")
                    if "error" in result:
                        update_task(tid, status="failed",
                                    result_summary=str(result.get("error", ""))[:200],
                                    success=0, agent_id=tg_agent_id)
                    else:
                        update_task(tid, status="completed",
                                    result_summary=_extract_title_summary(content or ""),
                                    success=1, cost_usd=cost, agent_id=tg_agent_id)
                    if len(content) > 3900:
                        content = content[:3900] + "\n\n... (ê²°ê³¼ê°€ ì˜ë ¸ìŠµë‹ˆë‹¤. ì›¹ì—ì„œ ì „ì²´ í™•ì¸)"
                    await app_state.telegram_app.bot.send_message(
                        chat_id=int(cid),
                        text=f"{content}\n\nâ”€â”€â”€â”€â”€\nğŸ’° ${cost:.4f}",
                    )
                except Exception as e:
                    update_task(tid, status="failed",
                                result_summary=str(e)[:200], success=0)
                    try:
                        await app_state.telegram_app.bot.send_message(chat_id=int(cid), text=f"âŒ ì˜¤ë¥˜: {e}")
                    except Exception as e2:
                        logger.debug("TG ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: %s", e2)

            asyncio.create_task(_bg(task_text, task["task_id"], chat_id))

        # â”€â”€ /í† ë¡  [ì£¼ì œ] â€” ì„ì› í† ë¡  (2ë¼ìš´ë“œ) â”€â”€
        async def cmd_debate(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            topic = " ".join(context.args) if context.args else ""
            if not topic:
                await update.message.reply_text(
                    "ì‚¬ìš©ë²•: `/í† ë¡  [ì£¼ì œ]`\nì˜ˆ: `/í† ë¡  AIê°€ ì¸ê°„ì˜ ì¼ìë¦¬ë¥¼ ëŒ€ì²´í• ê¹Œ?`",
                    parse_mode="Markdown",
                )
                return
            await _tg_long_command(update, f"/í† ë¡  {topic}")

        # â”€â”€ /ì‹¬ì¸µí† ë¡  [ì£¼ì œ] â€” ì‹¬ì¸µ ì„ì› í† ë¡  (3ë¼ìš´ë“œ) â”€â”€
        async def cmd_deep_debate(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            topic = " ".join(context.args) if context.args else ""
            if not topic:
                await update.message.reply_text(
                    "ì‚¬ìš©ë²•: `/ì‹¬ì¸µí† ë¡  [ì£¼ì œ]`\nì˜ˆ: `/ì‹¬ì¸µí† ë¡  CORTHEX 2026 ì „ëµ ë°©í–¥`",
                    parse_mode="Markdown",
                )
                return
            await _tg_long_command(update, f"/ì‹¬ì¸µí† ë¡  {topic}")

        # â”€â”€ /ì „ì²´ [ë©”ì‹œì§€] â€” 29ëª… ë™ì‹œ ë¸Œë¡œë“œìºìŠ¤íŠ¸ â”€â”€
        async def cmd_broadcast_all(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            message = " ".join(context.args) if context.args else "ì „ì²´ ì¶œì„ ë³´ê³ "
            await _tg_long_command(update, f"/ì „ì²´ {message}")

        # â”€â”€ /ìˆœì°¨ [ë©”ì‹œì§€] â€” ì—ì´ì „íŠ¸ ë¦´ë ˆì´ ìˆœì°¨ í˜‘ì—… â”€â”€
        async def cmd_sequential(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            message = " ".join(context.args) if context.args else ""
            if not message:
                await update.message.reply_text(
                    "ì‚¬ìš©ë²•: `/ìˆœì°¨ [ì‘ì—…]`\nì˜ˆ: `/ìˆœì°¨ CORTHEX ì›¹ì‚¬ì´íŠ¸ ê¸°ìˆ â†’ë³´ì•ˆâ†’ì‚¬ì—…ì„± ë¶„ì„`",
                    parse_mode="Markdown",
                )
                return
            await _tg_long_command(update, f"/ìˆœì°¨ {message}")

        async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            if not _is_tg_ceo(update):
                return
            text = update.message.text.strip()
            if not text:
                return

            # @ì—ì´ì „íŠ¸ëª… ì§ì ‘ ì§€ì‹œ íŒŒì‹± (ì˜ˆ: "@cto_manager ê¸°ìˆ  ë¶„ì„í•´ì¤˜")
            tg_target_agent_id = None
            if text.startswith("@"):
                parts = text.split(None, 1)
                if len(parts) >= 2:
                    mention = parts[0][1:]
                    mention_lower = mention.lower()
                    for a in AGENTS:
                        aid = a.get("agent_id", "").lower()
                        aname = a.get("name_ko", "")
                        tcode = a.get("telegram_code", "").lstrip("@")
                        if aid == mention_lower or aid.startswith(mention_lower) or mention_lower in aname.lower() or mention == tcode:
                            tg_target_agent_id = a["agent_id"]
                            text = parts[1]
                            break
                    if not tg_target_agent_id:
                        await update.message.reply_text(
                            f"âŒ `@{parts[0][1:]}` ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                            f"/agents ë¡œ ì—ì´ì „íŠ¸ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”.",
                            parse_mode="Markdown",
                        )
                        return

            # í•œêµ­ì–´ ëª…ë ¹ì–´ ì²˜ë¦¬ (í…”ë ˆê·¸ë¨ CommandHandlerëŠ” ì˜ì–´ë§Œ ì§€ì›í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬)
            if text in ("ì‹¤ì‹œê°„", "/ì‹¤ì‹œê°„"):
                save_setting("tg_mode", "realtime")
                await update.message.reply_text(
                    "ğŸ”´ *ì‹¤ì‹œê°„ ëª¨ë“œ*ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.\n\n"
                    "ì´ì œ ë³´ë‚´ì‹œëŠ” ë©”ì‹œì§€ì— AIê°€ ì¦‰ì‹œ ë‹µë³€í•©ë‹ˆë‹¤.",
                    parse_mode="Markdown",
                )
                return
            if text in ("ë°°ì¹˜", "/ë°°ì¹˜"):
                save_setting("tg_mode", "batch")
                await update.message.reply_text(
                    "ğŸ“¦ *ë°°ì¹˜ ëª¨ë“œ*ë¡œ ì „í™˜í–ˆìŠµë‹ˆë‹¤.\n\n"
                    "ë©”ì‹œì§€ë¥¼ ì ‘ìˆ˜ë§Œ í•˜ê³ , AI ì²˜ë¦¬ëŠ” í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                    parse_mode="Markdown",
                )
                return

            # í•œêµ­ì–´ AI ëª…ë ¹ì–´ (/í† ë¡ , /ì‹¬ì¸µí† ë¡ , /ì „ì²´, /ìˆœì°¨)
            if text.startswith("/í† ë¡  ") or text == "/í† ë¡ ":
                topic = text[len("/í† ë¡ "):].strip()
                if not topic:
                    await update.message.reply_text(
                        "ì‚¬ìš©ë²•: /í† ë¡  [ì£¼ì œ]\nì˜ˆ: /í† ë¡  AIê°€ ì¸ê°„ì˜ ì¼ìë¦¬ë¥¼ ëŒ€ì²´í• ê¹Œ?")
                    return
                await _tg_long_command(update, f"/í† ë¡  {topic}")
                return
            if text.startswith("/ì‹¬ì¸µí† ë¡  ") or text == "/ì‹¬ì¸µí† ë¡ ":
                topic = text[len("/ì‹¬ì¸µí† ë¡ "):].strip()
                if not topic:
                    await update.message.reply_text(
                        "ì‚¬ìš©ë²•: /ì‹¬ì¸µí† ë¡  [ì£¼ì œ]\nì˜ˆ: /ì‹¬ì¸µí† ë¡  CORTHEX 2026 ì „ëµ ë°©í–¥")
                    return
                await _tg_long_command(update, f"/ì‹¬ì¸µí† ë¡  {topic}")
                return
            if text.startswith("/ì „ì²´ ") or text == "/ì „ì²´":
                message_text = text[len("/ì „ì²´"):].strip() or "ì „ì²´ ì¶œì„ ë³´ê³ "
                await _tg_long_command(update, f"/ì „ì²´ {message_text}")
                return
            if text.startswith("/ìˆœì°¨ ") or text == "/ìˆœì°¨":
                message_text = text[len("/ìˆœì°¨"):].strip()
                if not message_text:
                    await update.message.reply_text(
                        "ì‚¬ìš©ë²•: /ìˆœì°¨ [ì‘ì—…]\nì˜ˆ: /ìˆœì°¨ CORTHEX ì›¹ì‚¬ì´íŠ¸ ê¸°ìˆ â†’ë³´ì•ˆâ†’ì‚¬ì—…ì„± ë¶„ì„")
                    return
                await _tg_long_command(update, f"/ìˆœì°¨ {message_text}")
                return

            chat_id = str(update.effective_chat.id)
            # DBì— ë©”ì‹œì§€ + ì‘ì—… ì €ì¥
            task = create_task(text, source="telegram")
            save_message(text, source="telegram", chat_id=chat_id,
                         task_id=task["task_id"])

            # AI ì¼ì‹œ ì¤‘ë‹¨ ì²´í¬
            if load_setting("ai_paused"):
                await update.message.reply_text("â¸ AI ì²˜ë¦¬ê°€ ì¼ì‹œ ì¤‘ë‹¨ëœ ìƒíƒœì…ë‹ˆë‹¤.\n`/resume`ìœ¼ë¡œ ì¬ê°œí•˜ì„¸ìš”.", parse_mode="Markdown")
                return

            # ëª¨ë“œ í™•ì¸
            mode = load_setting("tg_mode") or "realtime"
            now = datetime.now(KST).strftime("%H:%M")
            result = {}  # ì›¹ì†Œì¼“ ë¸Œë¡œë“œìºìŠ¤íŠ¸ìš©

            if mode == "realtime" and is_ai_ready():
                # ì‹¤ì‹œê°„ ëª¨ë“œ: AIê°€ ë‹µë³€
                update_task(task["task_id"], status="running")
                await update.message.reply_text(f"â³ ì²˜ë¦¬ ì¤‘... (#{task['task_id']})")

                result = await _process_ai_command(text, task["task_id"], target_agent_id=tg_target_agent_id)

                tg_rt_agent_id = result.get("agent_id", "chief_of_staff")
                if "error" in result:
                    update_task(task["task_id"], status="failed",
                                result_summary=str(result.get("error", ""))[:200],
                                success=0, agent_id=tg_rt_agent_id)
                    await update.message.reply_text(f"âŒ {result['error']}")
                else:
                    content = result.get("content", "")
                    cost = result.get("cost_usd", 0)
                    model = result.get("model", "")
                    # í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (4096ì)
                    if len(content) > 3900:
                        content = content[:3900] + "\n\n... (ê²°ê³¼ê°€ ì˜ë ¸ìŠµë‹ˆë‹¤. ì›¹ì—ì„œ ì „ì²´ í™•ì¸)"
                    delegation = result.get("delegation", "")
                    model_short = model.split("-")[1] if "-" in model else model
                    # ë‹´ë‹¹ì í‘œì‹œ: íŒ€ì¥ ì´ë¦„ ë˜ëŠ” ë¹„ì„œì‹¤ì¥
                    footer_who = delegation if delegation else "ë¹„ì„œì‹¤ì¥"
                    update_task(task["task_id"], status="completed",
                                result_summary=_extract_title_summary(content or ""),
                                success=1, cost_usd=cost,
                                time_seconds=result.get("time_seconds", 0),
                                agent_id=tg_rt_agent_id)
                    await update.message.reply_text(
                        f"{content}\n\n"
                        f"â”€â”€â”€â”€â”€\n"
                        f"ğŸ‘¤ {footer_who} | ğŸ’° ${cost:.4f} | ğŸ¤– {model_short}",
                        parse_mode=None,
                    )
            elif mode == "batch" and is_ai_ready():
                # ë°°ì¹˜ ëª¨ë“œ + AI ì—°ê²°ë¨ â†’ ì‹¤ì œ ë°°ì¹˜ ì²´ì¸ ì‹¤í–‰
                update_task(task["task_id"], status="pending",
                            result_summary="ğŸ“¦ [ë°°ì¹˜ ì²´ì¸] ì‹œì‘ ì¤‘...")
                await update.message.reply_text(
                    f"ğŸ“¦ ë°°ì¹˜ ì ‘ìˆ˜ ì™„ë£Œ (#{task['task_id']})\n"
                    f"ë°°ì¹˜ ì²´ì¸ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n"
                    f"ì™„ë£Œ ì‹œ ê²°ê³¼ë¥¼ ì—¬ê¸°ë¡œ ë³´ë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
                    parse_mode=None,
                )

                # ë°°ì¹˜ ì²´ì¸ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì‹¤í–‰
                async def _tg_run_batch(text_arg, task_id_arg, chat_id_arg):
                    try:
                        chain_result = await _start_batch_chain(text_arg, task_id_arg)
                        if "error" in chain_result and app_state.telegram_app:
                            try:
                                await app_state.telegram_app.bot.send_message(
                                    chat_id=int(chat_id_arg),
                                    text=f"âŒ ë°°ì¹˜ ì‹œì‘ ì‹¤íŒ¨: {chain_result['error']}",
                                )
                            except Exception as e2:
                                logger.debug("TG ë°°ì¹˜ ì‹¤íŒ¨ ì „ì†¡ ì‹¤íŒ¨: %s", e2)
                    except Exception as e:
                        _log(f"[TG] ë°°ì¹˜ ì²´ì¸ ì˜¤ë¥˜: {e}")

                asyncio.create_task(_tg_run_batch(text, task["task_id"], chat_id))
            else:
                # AI ë¯¸ì—°ê²° â†’ ì ‘ìˆ˜ë§Œ
                update_task(task["task_id"], status="completed",
                            result_summary="AI ë¯¸ì—°ê²° â€” ì ‘ìˆ˜ë§Œ ì™„ë£Œ",
                            success=1, time_seconds=0.1)
                await update.message.reply_text(
                    f"ğŸ“‹ ì ‘ìˆ˜í–ˆìŠµë‹ˆë‹¤. ({now})\n"
                    f"ì‘ì—… ID: `{task['task_id']}`\n"
                    f"ìƒíƒœ: AI ë¯¸ì—°ê²°",
                    parse_mode="Markdown",
                )

            # í™œë™ ë¡œê·¸ ì €ì¥ + ì›¹ì†Œì¼“ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ì›¹ ì±„íŒ…ì—ë„ ëŒ€í™” í‘œì‹œ)
            log_entry = save_activity_log(
                "chief_of_staff",
                f"[í…”ë ˆê·¸ë¨] CEO ì§€ì‹œ: {text[:50]}{'...' if len(text) > 50 else ''} (#{task['task_id']})",
            )
            await wm.broadcast_multi([
                ("task_accepted", task),
                ("activity_log", log_entry),
                ("telegram_message", {"type": "user", "text": text, "source": "telegram"}),
            ])
            if "error" not in result:
                await wm.broadcast("result", {
                    "content": result.get("content", ""),
                    "sender_id": result.get("agent_id", "chief_of_staff"),
                    "handled_by": result.get("handled_by", "ë¹„ì„œì‹¤ì¥"),
                    "delegation": result.get("delegation", ""),
                    "time_seconds": result.get("time_seconds", 0),
                    "cost": result.get("total_cost_usd", result.get("cost_usd", 0)),
                    "model": result.get("model", ""),
                    "routing_method": result.get("routing_method", ""),
                    "source": "telegram",
                })
            else:
                await wm.broadcast("result", {
                    "content": f"âŒ {result['error']}",
                    "sender_id": "chief_of_staff",
                    "handled_by": "ë¹„ì„œì‹¤ì¥",
                    "time_seconds": 0, "cost": 0,
                    "source": "telegram",
                })

        def _is_tg_ceo(update: Update) -> bool:
            if not update.effective_chat or not update.message:
                return False
            ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
            if not ceo_id:
                return False
            if str(update.effective_chat.id) != ceo_id:
                asyncio.create_task(update.message.reply_text("ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤."))
                return False
            return True

        # â”€â”€ ê¸€ë¡œë²Œ ì—ëŸ¬ í•¸ë“¤ëŸ¬ (í•¸ë“¤ëŸ¬ ì˜ˆì™¸ ë¡œê¹…) â”€â”€
        async def _tg_error_handler(update, context):
            _log(f"[TG] âŒ í•¸ë“¤ëŸ¬ ì˜¤ë¥˜: {context.error}")
            import traceback
            _diag["tg_last_error"] = str(context.error)
            _diag["tg_error_time"] = datetime.now(KST).isoformat()
            traceback.print_exc()
        app_state.telegram_app.add_error_handler(_tg_error_handler)

        # í•¸ë“¤ëŸ¬ ë“±ë¡
        app_state.telegram_app.add_handler(CommandHandler("start", cmd_start))
        app_state.telegram_app.add_handler(CommandHandler("help", cmd_help))
        app_state.telegram_app.add_handler(CommandHandler("agents", cmd_agents))
        app_state.telegram_app.add_handler(CommandHandler("health", cmd_health))
        app_state.telegram_app.add_handler(CommandHandler("rt", cmd_rt))
        app_state.telegram_app.add_handler(CommandHandler("batch", cmd_batch))
        app_state.telegram_app.add_handler(CommandHandler("status", cmd_status))
        app_state.telegram_app.add_handler(CommandHandler("budget", cmd_budget))
        app_state.telegram_app.add_handler(CommandHandler("pause", cmd_pause))
        app_state.telegram_app.add_handler(CommandHandler("resume", cmd_resume))
        app_state.telegram_app.add_handler(CommandHandler("models", cmd_models))
        app_state.telegram_app.add_handler(CallbackQueryHandler(models_callback, pattern=r"^mdl_"))
        # í•œêµ­ì–´ ëª…ë ¹(/í† ë¡ , /ì‹¬ì¸µí† ë¡ , /ì „ì²´, /ìˆœì°¨)ì€ handle_messageì—ì„œ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        # (Telegram CommandHandlerëŠ” ë¼í‹´ ì†Œë¬¸ì+ìˆ«ì+ë°‘ì¤„ë§Œ í—ˆìš©)
        app_state.telegram_app.add_handler(
            MessageHandler(filters.TEXT, handle_message)
        )

        _log("[TG] í•¸ë“¤ëŸ¬ ë“±ë¡ ì™„ë£Œ, initialize()...")
        await app_state.telegram_app.initialize()

        # í† í° ìœ íš¨ì„± ì‚¬ì „ í™•ì¸ (getMe)
        try:
            me = await app_state.telegram_app.bot.get_me()
            _diag["tg_bot_username"] = me.username
            _diag["tg_bot_id"] = me.id
            _log(f"[TG] âœ… ë´‡ ì¸ì¦ ì„±ê³µ: @{me.username} (ID: {me.id})")
        except Exception as me_err:
            _log(f"[TG] âŒ ë´‡ í† í° ë¬´íš¨ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {me_err}")
            _diag["tg_error"] = f"getMe ì‹¤íŒ¨: {me_err}"
            app_state.telegram_app = None
            return

        # webhook ì¶©ëŒ ë°©ì§€: polling ì‹œì‘ ì „ webhook ê°•ì œ ì‚­ì œ
        for attempt in range(3):
            try:
                await app_state.telegram_app.bot.delete_webhook(drop_pending_updates=False)
                _log("[TG] webhook ì‚­ì œ ì™„ë£Œ (polling ì¶©ëŒ ë°©ì§€)")
                break
            except Exception as we:
                _log(f"[TG] webhook ì‚­ì œ ì‹œë„ {attempt+1}/3 ì‹¤íŒ¨: {we}")
                if attempt < 2:
                    await asyncio.sleep(1)

        # ë´‡ ëª…ë ¹ì–´ ë©”ë‰´ ì„¤ì • (initialize ì´í›„ì— API í˜¸ì¶œ ê°€ëŠ¥)
        # NOTE: Telegram BotCommandëŠ” ë¼í‹´ ì†Œë¬¸ì+ìˆ«ì+ë°‘ì¤„ë§Œ í—ˆìš© (í•œêµ­ì–´ ë¶ˆê°€)
        # í•œêµ­ì–´ ëª…ë ¹(/í† ë¡ , /ì‹¬ì¸µí† ë¡ , /ì „ì²´, /ìˆœì°¨)ì€ CommandHandlerë¡œë§Œ ë™ì‘
        try:
            await app_state.telegram_app.bot.set_my_commands([
                BotCommand("start", "ë´‡ ì‹œì‘"),
                BotCommand("help", "ì‚¬ìš©ë²• (í•œêµ­ì–´ ëª…ë ¹ í¬í•¨)"),
                BotCommand("agents", "ì—ì´ì „íŠ¸ ëª©ë¡"),
                BotCommand("health", "ì„œë²„ ìƒíƒœ"),
                BotCommand("rt", "ì‹¤ì‹œê°„ ëª¨ë“œ"),
                BotCommand("batch", "ë°°ì¹˜ ëª¨ë“œ"),
                BotCommand("models", "ì „ì› ëª¨ë¸ ë³€ê²½"),
                BotCommand("status", "ë°°ì¹˜ ì§„í–‰ ìƒíƒœ"),
                BotCommand("budget", "ì˜¤ëŠ˜ ë¹„ìš© / í•œë„ ë³€ê²½"),
                BotCommand("pause", "AI ì²˜ë¦¬ ì¤‘ë‹¨"),
                BotCommand("resume", "AI ì²˜ë¦¬ ì¬ê°œ"),
            ])
        except Exception as cmd_err:
            _log(f"[TG] ëª…ë ¹ì–´ ë©”ë‰´ ì„¤ì • ê±´ë„ˆëœ€ (ë´‡ì€ ì •ìƒ ë™ì‘): {cmd_err}")

        _log("[TG] start()...")
        await app_state.telegram_app.start()
        _log("[TG] polling ì‹œì‘...")
        # drop_pending_updates=True: ì´ì „ ìŒ“ì¸ ë©”ì‹œì§€ ë¬´ì‹œí•˜ê³  ìƒˆ ë©”ì‹œì§€ë§Œ ì²˜ë¦¬
        await app_state.telegram_app.updater.start_polling(
            drop_pending_updates=True,
            allowed_updates=Update.ALL_TYPES,
        )

        ceo_id = os.getenv("TELEGRAM_CEO_CHAT_ID", "")
        _diag["tg_started"] = True
        _log(f"[TG] âœ… ë´‡ ì‹œì‘ ì™„ë£Œ! (CEO: {ceo_id or 'ë¯¸ì„¤ì •'})")
    except Exception as e:
        _diag["tg_error"] = str(e)
        _log(f"[TG] âŒ ë´‡ ì‹œì‘ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        app_state.telegram_app = None


async def _stop_telegram_bot() -> None:
    """í…”ë ˆê·¸ë¨ ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."""

    if app_state.telegram_app:
        try:
            await app_state.telegram_app.updater.stop()
            await app_state.telegram_app.stop()
            await app_state.telegram_app.shutdown()
            logger.info("í…”ë ˆê·¸ë¨ ë´‡ ì¢…ë£Œ ì™„ë£Œ")
        except Exception as e:
            logger.warning("í…”ë ˆê·¸ë¨ ë´‡ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: %s", e)
        app_state.telegram_app = None


# â”€â”€ AI ì—ì´ì „íŠ¸ ìœ„ì„ ì‹œìŠ¤í…œ (Phase 5) â”€â”€

# ë¶€ì„œë³„ í‚¤ì›Œë“œ ë¼ìš°íŒ… í…Œì´ë¸”
_ROUTING_KEYWORDS: dict[str, list[str]] = {
    "cso_manager": [
        "ì‹œì¥", "ê²½ìŸì‚¬", "ì‚¬ì—…ê³„íš", "ë§¤ì¶œ", "ì˜ˆì¸¡", "ì „ëµ",
        "ë¹„ì¦ˆë‹ˆìŠ¤", "BM", "ìˆ˜ìµ", "ì‚¬ì—…", "ê¸°íš", "ì„±ì¥",
    ],
    "clo_manager": [
        "ì €ì‘ê¶Œ", "íŠ¹í—ˆ", "ìƒí‘œ", "ì•½ê´€", "ê³„ì•½", "ë²•ë¥ ", "ì†Œì†¡", "IP",
        "ê·œì œ", "ë¼ì´ì„ ìŠ¤", "ë²•ì ", "ë²•ë¬´",
    ],
    "cmo_manager": [
        "ë§ˆì¼€íŒ…", "ê´‘ê³ ", "SNS", "ì¸ìŠ¤íƒ€", "ìœ íŠœë¸Œ", "ê³ ê°",
        "ì„¤ë¬¸", "ë¸Œëœë”©", "ì½˜í…ì¸ ", "í™ë³´", "í”„ë¡œëª¨ì…˜", "ìº í˜ì¸",
    ],
    "cio_manager": [
        "ì‚¼ì„±", "ì• í”Œ", "ì£¼ì‹", "íˆ¬ì", "ì¢…ëª©", "ì°¨íŠ¸", "ì‹œí™©",
        "ì½”ìŠ¤í”¼", "ë‚˜ìŠ¤ë‹¥", "í¬íŠ¸í´ë¦¬ì˜¤", "ê¸ˆë¦¬", "í™˜ìœ¨", "ì±„ê¶Œ",
        "ETF", "í€ë“œ", "ë°°ë‹¹", "í…ŒìŠ¬ë¼", "ì—”ë¹„ë””ì•„",
        "ë§¤ìˆ˜", "ë§¤ë„", "ìë™ë§¤ë§¤", "í‚¤ì›€", "ë°±í…ŒìŠ¤íŠ¸", "ì „ëµ",
        "ì†ì ˆ", "ìµì ˆ", "ì‹œê°€ì´ì•¡", "PER", "RSI", "MACD",
    ],
    "cpo_manager": [
        "ê¸°ë¡", "ë¹Œë”©ë¡œê·¸", "ì—°ëŒ€ê¸°", "ë¸”ë¡œê·¸", "ì¶œíŒ", "í¸ì§‘", "íšŒê³ ",
        "ì•„ì¹´ì´ë¸Œ", "ë¬¸ì„œí™”", "íšŒì˜ë¡",
    ],
}

# ì—ì´ì „íŠ¸ ID â†’ í•œêµ­ì–´ ì´ë¦„ ë§¤í•‘
_AGENT_NAMES: dict[str, str] = {
    "chief_of_staff": "ë¹„ì„œì‹¤ì¥",
    "cso_manager": "ì „ëµíŒ€ì¥",
    "clo_manager": "ë²•ë¬´íŒ€ì¥",
    "cmo_manager": "ë§ˆì¼€íŒ…íŒ€ì¥",
    "cio_manager": "ê¸ˆìœµë¶„ì„íŒ€ì¥",
    "cpo_manager": "ì½˜í…ì¸ íŒ€ì¥",
}

# â”€â”€ ë…¸ì…˜ API ì—°ë™ (ì—ì´ì „íŠ¸ ì‚°ì¶œë¬¼ ìë™ ì €ì¥) â”€â”€


_TITLE_SKIP_WORDS = {"ì£„ì†¡", "ì˜¤ë¥˜", "ì—ëŸ¬", "ì‹¤íŒ¨", "sorry", "error", "ì•ˆë…•í•˜ì„¸ìš”", "ë„¤,", "ë„¤!"}
# CEO ëª…ë ¹ë¬¸ íŒ¨í„´: ì œëª©ì—ì„œ ê±¸ëŸ¬ì•¼ í•  ë¬¸ì¥ ë íŒ¨í„´
_TITLE_CMD_ENDINGS = ("í•´ì¤˜", "í•´ì£¼ì„¸ìš”", "í•´ë´", "í•˜ì„¸ìš”", "í• ê¹Œìš”", "ì•Œë ¤ì¤˜", "ì•Œë ¤ì£¼ì„¸ìš”",
                      "ë³´ê³ í•´", "ë¶„ì„í•´", "ì¡°ì‚¬í•´", "ë§Œë“¤ì–´ì¤˜", "ì‘ì„±í•´", "ì •ë¦¬í•´")

def _extract_notion_title(content: str, fallback: str = "ë³´ê³ ì„œ",
                          user_query: str = "") -> str:
    """AI ì‘ë‹µ ë³¸ë¬¸ì—ì„œ ê¹”ë”í•œ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ê¸ˆì§€ì–´(ì‚¬ê³¼/ì—ëŸ¬ ë¬¸êµ¬), CEO ëª…ë ¹ë¬¸ íŒ¨í„´, user_query ë°˜ë³µ ì¤„ì€ ê±´ë„ˆëœë‹ˆë‹¤."""
    if not content:
        return fallback
    # user_query ìœ ì‚¬ë„ ì²´í¬ìš© (ì• 20ì ì •ê·œí™”)
    q_norm = user_query.strip().replace("**", "").replace("*", "")[:20] if user_query else ""
    for line in content.split("\n"):
        line = line.strip()
        if not line:
            continue
        line = line.lstrip("#").strip()
        line = line.replace("**", "").replace("*", "")
        if len(line) < 3 or line.startswith("---") or line.startswith("```"):
            continue
        # ê¸ˆì§€ì–´ í•„í„°: "ì£„ì†¡í•©ë‹ˆë‹¤", "ì˜¤ë¥˜ì…ë‹ˆë‹¤" ë“± ì œëª©ìœ¼ë¡œ ë¶€ì ì ˆí•œ ë¬¸êµ¬
        low = line[:10].lower()
        if any(low.startswith(w) for w in _TITLE_SKIP_WORDS):
            continue
        # CEO ëª…ë ¹ë¬¸ íŒ¨í„´ í•„í„°: "~í•´ì¤˜", "~ë¶„ì„í•´" ë“± ëª…ë ¹í˜• ë¬¸ì¥ ê±´ë„ˆë›°ê¸°
        if any(line.rstrip(".,!? ").endswith(e) for e in _TITLE_CMD_ENDINGS):
            continue
        # user_query ë°˜ë³µ í•„í„°: CEO ëª…ë ¹ì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ëŠ” ì¤„ ê±´ë„ˆë›°ê¸°
        if q_norm and len(q_norm) > 5 and line[:20].startswith(q_norm[:15]):
            continue
        return line[:100]
    return fallback


_NOTION_API_KEY = os.getenv("NOTION_API_KEY", "")
# ë¹„ì„œì‹¤ DB (ë¹„ì„œì‹¤ì¥â†’CEO ë³´ê³ ì„œë§Œ)
_NOTION_DB_SECRETARY = os.getenv("NOTION_DB_SECRETARY", "30a56b49-78dc-8153-bac1-dee5d04d6a74")
# ì—ì´ì „íŠ¸ ì‚°ì¶œë¬¼ DB (íŒ€ì¥ 6ëª… ì‘ì—…ë¬¼)
_NOTION_DB_OUTPUT = os.getenv("NOTION_DB_OUTPUT", "30a56b49-78dc-81ce-aaca-ef3fc90a6fba")
# ì•„ì¹´ì´ë¸Œ DB (v3 ë°ì´í„° + êµ¬ë²„ì „ ì´ê´€)
_NOTION_DB_ARCHIVE = os.getenv("NOTION_DB_ARCHIVE", "31256b49-78dc-81c9-9ad2-e31a076d0d97")
# í•˜ìœ„ í˜¸í™˜
_NOTION_DB_ID = os.getenv("NOTION_DEFAULT_DB_ID", _NOTION_DB_OUTPUT)

# ë…¸ì…˜ ë¡œê·¸ â†’ app_state ì‚¬ìš© (alias)
_notion_log = app_state.notion_log

def _add_notion_log(status: str, title: str, db: str = "", url: str = "", error: str = ""):
    """ë…¸ì…˜ ì‘ì—… ë¡œê·¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤ (ìµœê·¼ 20ê°œ)."""
    _notion_log.append({
        "time": datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S"),
        "status": status,
        "title": title[:60],
        "db": db,
        "url": url,
        "error": error[:200] if error else "",
    })
    # in-place íŠ¸ë¦¬ë° (alias ê¹¨ì§€ì§€ ì•Šê²Œ)
    if len(_notion_log) > 500:
        del _notion_log[:-500]

# ì—ì´ì „íŠ¸ ID â†’ ë¶€ì„œëª… ë§¤í•‘
_AGENT_DIVISION: dict[str, str] = {}
for _a in AGENTS:
    if _a.get("division"):
        _AGENT_DIVISION[_a["agent_id"]] = _a["division"]


async def _save_to_notion(agent_id: str, title: str, content: str,
                          report_type: str = "ë³´ê³ ì„œ",
                          db_target: str = "output") -> str | None:
    """ì—ì´ì „íŠ¸ ì‚°ì¶œë¬¼ì„ ë…¸ì…˜ DBì— ì €ì¥í•©ë‹ˆë‹¤.

    db_target: "output" = ì—ì´ì „íŠ¸ ì‚°ì¶œë¬¼ DB, "secretary" = ë¹„ì„œì‹¤ DB
    Python ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬(urllib)ë§Œ ì‚¬ìš© â€” ì¶”ê°€ íŒ¨í‚¤ì§€ ë¶ˆí•„ìš”.
    ì‹¤íŒ¨í•´ë„ ì—ëŸ¬ë§Œ ë¡œê¹…í•˜ê³  None ë°˜í™˜ (ì„œë²„ ë™ì‘ì— ì˜í–¥ ì—†ìŒ).
    """
    if not _NOTION_API_KEY:
        _add_notion_log("SKIP", title, error="API í‚¤ ì—†ìŒ")
        return None

    db_id = _NOTION_DB_SECRETARY if db_target == "secretary" else _NOTION_DB_OUTPUT
    db_name = "ë¹„ì„œì‹¤" if db_target == "secretary" else "ì‚°ì¶œë¬¼"

    division = _AGENT_DIVISION.get(agent_id, "")
    agent_name = _AGENT_NAMES.get(agent_id, _SPECIALIST_NAMES.get(agent_id, agent_id))
    now_str = datetime.now(KST).strftime("%Y-%m-%d")

    # ë…¸ì…˜ í˜ì´ì§€ í”„ë¡œí¼í‹° êµ¬ì„± â€” ë‘ DB ìŠ¤í‚¤ë§ˆê°€ ë‹¤ë¦„
    # ë¹„ì„œì‹¤ DB: Name, ë‹´ë‹¹ì(select), ì¹´í…Œê³ ë¦¬(select), ìƒíƒœ(select), ë‚ ì§œ(date), ë‚´ìš©(rich_text)
    # ì—ì´ì „íŠ¸ ì‚°ì¶œë¬¼ DB: Name, ì—ì´ì „íŠ¸(select), ë³´ê³ ìœ í˜•(select), ë¶€ì„œ(select), ìƒíƒœ(select), ë‚ ì§œ(date)
    properties: dict = {
        "Name": {"title": [{"text": {"content": title[:100]}}]},
    }
    if db_target == "secretary":
        # ë¹„ì„œì‹¤ DB: ë‹´ë‹¹ì + ì¹´í…Œê³ ë¦¬ + ë‚´ìš©
        if agent_name:
            properties["ë‹´ë‹¹ì"] = {"select": {"name": agent_name}}
        properties["ì¹´í…Œê³ ë¦¬"] = {"select": {"name": "ë³´ê³ ì„œ"}}
        if content:
            properties["ë‚´ìš©"] = {"rich_text": [{"text": {"content": content[:2000]}}]}
    else:
        # ì—ì´ì „íŠ¸ ì‚°ì¶œë¬¼ DB: ì—ì´ì „íŠ¸ + ë³´ê³ ìœ í˜• + ë¶€ì„œ
        if agent_name:
            properties["ì—ì´ì „íŠ¸"] = {"select": {"name": agent_name}}
        if report_type:
            properties["ë³´ê³ ìœ í˜•"] = {"select": {"name": report_type}}
        # ë¶€ì„œ ë§¤í•‘: division â†’ ë…¸ì…˜ ë¶€ì„œ select ì˜µì…˜
        _div_map = {
            "secretary": "ë¹„ì„œì‹¤",
            "leet_master.tech": "LEET MASTER",
            "leet_master.strategy": "LEET MASTER",
            "leet_master.legal": "LEET MASTER",
            "leet_master.marketing": "LEET MASTER",
            "finance.investment": "íˆ¬ìë¶„ì„",
            "publishing": "ì¶œíŒê¸°ë¡",
        }
        notion_div = _div_map.get(division, "")
        if notion_div:
            properties["ë¶€ì„œ"] = {"select": {"name": notion_div}}
    properties["ìƒíƒœ"] = {"select": {"name": "ì™„ë£Œ"}}
    properties["ë‚ ì§œ"] = {"date": {"start": now_str}}

    # ë³¸ë¬¸ â†’ ë…¸ì…˜ ë¸”ë¡ (ìµœëŒ€ 2000ì, ë…¸ì…˜ ë¸”ë¡ í¬ê¸° ì œí•œ)
    children = []
    text_chunks = [content[i:i+1900] for i in range(0, min(len(content), 8000), 1900)]
    for chunk in text_chunks:
        children.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"text": {"content": chunk}}]},
        })

    body = json.dumps({
        "parent": {"database_id": db_id},
        "properties": properties,
        "children": children,
    }).encode("utf-8")

    headers = {
        "Authorization": f"Bearer {_NOTION_API_KEY}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }

    def _do_request():
        req = urllib.request.Request(
            "https://api.notion.com/v1/pages",
            data=body, headers=headers, method="POST",
        )
        try:
            with urllib.request.urlopen(req, timeout=10) as resp:
                return json.loads(resp.read().decode("utf-8"))
        except urllib.error.HTTPError as e:
            err_body = e.read().decode("utf-8", errors="replace")[:300]
            _log(f"[Notion] HTTP {e.code} ì˜¤ë¥˜ ({db_name}): {err_body}")
            # ì˜¤ë¥˜ ì›ì¸ íŒíŠ¸: 400=ì†ì„±ëª… ë¶ˆì¼ì¹˜(Name vs ì œëª©), 401=APIí‚¤ ì˜¤ë¥˜, 404=DB ID ì˜¤ë¥˜
            return {"_error": f"HTTP {e.code}: {err_body}"}
        except Exception as e:
            _log(f"[Notion] ìš”ì²­ ì‹¤íŒ¨ ({db_name}): {e}")
            return {"_error": str(e)}

    try:
        result = await asyncio.to_thread(_do_request)
        # _error í‚¤ê°€ ìˆìœ¼ë©´ _do_request ë‚´ë¶€ì—ì„œ ì˜¤ë¥˜ ë°œìƒ â€” ì´ë¯¸ _logì— ê¸°ë¡ë¨
        if result and "_error" in result:
            _add_notion_log("FAIL", title, db=db_name, error=result["_error"])
            return None
        if result and result.get("url"):
            _log(f"[Notion] ì €ì¥ ì™„ë£Œ ({db_name}): {title[:50]} â†’ {result['url']}")
            _add_notion_log("OK", title, db=db_name, url=result["url"])
            return result["url"]
        elif result:
            # ì‘ë‹µì€ ì™”ì§€ë§Œ url í•„ë“œê°€ ì—†ëŠ” ê²½ìš° â€” ì‘ë‹µ ë‚´ìš© ë¡œê¹…í•´ì„œ ë””ë²„ê¹… ê°€ëŠ¥í•˜ê²Œ
            resp_snippet = str(result)[:200]
            _log(f"[Notion] ì‘ë‹µì— URL ì—†ìŒ ({db_name}): {resp_snippet}")
            _add_notion_log("FAIL", title, db=db_name, error=f"ì‘ë‹µì— URL ì—†ìŒ: {resp_snippet}")
        else:
            # resultê°€ None â€” _do_requestê°€ ì˜ˆì™¸ ì—†ì´ None ë°˜í™˜ (ì´ë¡ ìƒ ë°œìƒ ì•ˆ í•¨)
            _add_notion_log("FAIL", title, db=db_name, error="ì‘ë‹µ ì—†ìŒ(None)")
    except Exception as e:
        _log(f"[Notion] ë¹„ë™ê¸° ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        _add_notion_log("FAIL", title, db=db_name, error=str(e))

    return None


# â”€â”€ ë…¸ì…˜(Notion) ë¡œê·¸ API â†’ handlers/notion_handler.pyë¡œ ë¶„ë¦¬ â”€â”€
from handlers.notion_handler import router as notion_router
app.include_router(notion_router)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARGOS API â€” DB ìºì‹œ ì„œë¹™ (Phase 6-6) + ì‹ ë¢°ë„ ì„œë²„ ê³„ì‚° (Phase 6-7)
# + ì •ë³´êµ­ ìƒíƒœ API (Phase 6-8)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/api/argos/status")
async def argos_status():
    """ARGOS ìˆ˜ì§‘ ë ˆì´ì–´ í˜„í™© â€” ìˆ˜ì§‘ ì‹œê°, ì˜¤ë¥˜, ì´ ê±´ìˆ˜."""
    try:
        conn = get_connection()
        rows = conn.execute(
            "SELECT data_type, last_collected, last_error, total_count, updated_at "
            "FROM argos_collection_status"
        ).fetchall()
        # ì£¼ê°€ ë ˆì½”ë“œ ìˆ˜
        price_cnt     = conn.execute("SELECT COUNT(*) FROM argos_price_history").fetchone()[0]
        news_cnt      = conn.execute("SELECT COUNT(*) FROM argos_news_cache").fetchone()[0]
        dart_cnt      = conn.execute("SELECT COUNT(*) FROM argos_dart_filings").fetchone()[0]
        macro_cnt     = conn.execute("SELECT COUNT(*) FROM argos_macro_data").fetchone()[0]
        try:
            financial_cnt = conn.execute("SELECT COUNT(*) FROM argos_financial_data").fetchone()[0]
        except Exception:
            financial_cnt = 0
        try:
            sector_cnt = conn.execute("SELECT COUNT(*) FROM argos_sector_data").fetchone()[0]
        except Exception:
            sector_cnt = 0
        conn.close()

        status_map = {r[0]: {
            "last_collected": r[1], "last_error": r[2],
            "total_count": r[3], "updated_at": r[4]
        } for r in rows}

        return {"ok": True, "status": status_map, "db_counts": {
            "price": price_cnt, "news": news_cnt, "dart": dart_cnt,
            "macro": macro_cnt, "financial": financial_cnt, "sector": sector_cnt
        }}
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


@app.get("/api/argos/price/{ticker}")
async def argos_price(ticker: str, days: int = 30):
    """ARGOS DBì—ì„œ ì£¼ê°€ ì´ë ¥ ì„œë¹™ â€” AI ë„êµ¬ í˜¸ì¶œ ë¶ˆí•„ìš”."""
    try:
        conn = get_connection()
        cutoff = (datetime.now(KST) - timedelta(days=min(days, 90))).strftime("%Y-%m-%d")
        rows = conn.execute(
            """SELECT trade_date, open_price, high_price, low_price, close_price, volume, change_pct
               FROM argos_price_history
               WHERE ticker=? AND trade_date >= ?
               ORDER BY trade_date DESC LIMIT 90""",
            (ticker.upper(), cutoff)
        ).fetchall()
        conn.close()
        data = [{"date": r[0], "open": r[1], "high": r[2], "low": r[3],
                 "close": r[4], "volume": r[5], "change_pct": r[6]} for r in rows]
        return {"ok": True, "ticker": ticker, "count": len(data), "prices": data,
                "source": "ARGOS DB (ì„œë²„ ìºì‹œ)"}
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


@app.get("/api/argos/news/{keyword}")
async def argos_news(keyword: str, days: int = 7):
    """ARGOS DBì—ì„œ ë‰´ìŠ¤ ìºì‹œ ì„œë¹™."""
    try:
        conn = get_connection()
        cutoff = (datetime.now(KST) - timedelta(days=min(days, 30))).isoformat()
        rows = conn.execute(
            """SELECT title, description, link, pub_date, source
               FROM argos_news_cache
               WHERE keyword=? AND pub_date >= ?
               ORDER BY pub_date DESC LIMIT 50""",
            (keyword, cutoff)
        ).fetchall()
        conn.close()
        data = [{"title": r[0], "desc": r[1], "link": r[2],
                 "pub_date": r[3], "source": r[4]} for r in rows]
        return {"ok": True, "keyword": keyword, "count": len(data), "news": data,
                "source": "ARGOS DB (ì„œë²„ ìºì‹œ)"}
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


@app.get("/api/argos/dart/{ticker}")
async def argos_dart(ticker: str, days: int = 90):
    """ARGOS DBì—ì„œ DART ê³µì‹œ ì„œë¹™."""
    try:
        conn = get_connection()
        cutoff = (datetime.now(KST) - timedelta(days=min(days, 90))).strftime("%Y%m%d")
        rows = conn.execute(
            """SELECT corp_name, report_nm, rcept_no, flr_nm, rcept_dt
               FROM argos_dart_filings
               WHERE ticker=? AND rcept_dt >= ?
               ORDER BY rcept_dt DESC LIMIT 50""",
            (ticker.upper(), cutoff)
        ).fetchall()
        conn.close()
        data = [{"corp_name": r[0], "report_nm": r[1], "rcept_no": r[2],
                 "flr_nm": r[3], "rcept_dt": r[4]} for r in rows]
        return {"ok": True, "ticker": ticker, "count": len(data), "filings": data,
                "source": "ARGOS DB (ì„œë²„ ìºì‹œ)"}
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


@app.get("/api/argos/macro")
async def argos_macro(days: int = 30):
    """ARGOS DBì—ì„œ ë§¤í¬ë¡œ ì§€í‘œ ì„œë¹™."""
    try:
        conn = get_connection()
        cutoff = (datetime.now(KST) - timedelta(days=min(days, 365))).strftime("%Y-%m-%d")
        rows = conn.execute(
            """SELECT indicator, trade_date, value, source
               FROM argos_macro_data
               WHERE trade_date >= ?
               ORDER BY indicator, trade_date DESC""",
            (cutoff,)
        ).fetchall()
        conn.close()
        # indicatorë³„ ê·¸ë£¹í•‘
        from collections import defaultdict
        grouped = defaultdict(list)
        for r in rows:
            grouped[r[0]].append({"date": r[1], "value": r[2], "source": r[3]})
        return {"ok": True, "macro": dict(grouped), "source": "ARGOS DB (ì„œë²„ ìºì‹œ)"}
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


@app.get("/api/argos/confidence/{ticker}")
async def argos_confidence(ticker: str):
    """Phase 6-7: ì„œë²„ ê³„ì‚° ì‹ ë¢°ë„ â€” Quant + Calibration + Bayesian + ELO.
    AIëŠ” ì´ ê°’ì„ ë°›ì•„ ë‰´ìŠ¤ ë§¥ë½ìœ¼ë¡œ Â±20%p ì¡°ì •ë§Œ í•˜ë©´ ë¨.
    """
    try:
        conn = get_connection()

        # â‘  ìµœê·¼ ì£¼ê°€ ë°ì´í„° (90ì¼)
        price_rows = conn.execute(
            """SELECT close_price, volume, change_pct FROM argos_price_history
               WHERE ticker=? ORDER BY trade_date DESC LIMIT 90""",
            (ticker.upper(),)
        ).fetchall()

        quant_score = None
        if len(price_rows) >= 14:
            closes = [r[0] for r in reversed(price_rows)]
            volumes = [r[1] for r in reversed(price_rows)]

            # RSI(14)
            gains, losses = [], []
            for i in range(1, len(closes)):
                d = closes[i] - closes[i-1]
                (gains if d > 0 else losses).append(abs(d))
            avg_gain = sum(gains[-14:]) / 14 if gains else 0.001
            avg_loss = sum(losses[-14:]) / 14 if losses else 0.001
            rs = avg_gain / avg_loss
            rsi = 100 - (100 / (1 + rs))

            # 20ì¼ MA
            ma20 = sum(closes[-20:]) / min(20, len(closes))
            cur = closes[-1]
            above_ma = cur > ma20

            # ë³¼ë¦°ì €ë°´ë“œ
            if len(closes) >= 20:
                std20 = (sum((x - ma20)**2 for x in closes[-20:]) / 20) ** 0.5
                bb_upper = ma20 + 2 * std20
                bb_lower = ma20 - 2 * std20
                bb_pos = (cur - bb_lower) / (bb_upper - bb_lower) * 100 if bb_upper != bb_lower else 50
            else:
                bb_pos = 50

            # ê±°ë˜ëŸ‰ ë¹„ìœ¨ (ìµœê·¼ 5ì¼ / ì´ì „ 20ì¼ í‰ê· )
            if len(volumes) >= 25:
                vol_ratio = sum(volumes[-5:]) / 5 / (sum(volumes[-25:-5]) / 20 + 0.001)
            else:
                vol_ratio = 1.0

            # Quant Score ê³„ì‚° (0~99)
            rsi_score = max(0, min(100, (rsi - 30) / 40 * 100)) if rsi < 70 else max(0, (90 - rsi) * 3)
            ma_score = 60 if above_ma else 30
            bb_score = max(0, min(100, 100 - abs(bb_pos - 50) * 2))
            vol_score = min(100, vol_ratio * 50)
            trend_score = max(0, min(100, 50 + (cur - closes[-10]) / closes[-10] * 100)) if len(closes) >= 10 else 50
            quant_score = round((rsi_score * 0.25 + ma_score * 0.25 + bb_score * 0.2 + vol_score * 0.15 + trend_score * 0.15))

        # â‘¡ Calibration Factor
        calibration = _compute_calibration_factor(20)
        cal_factor = calibration.get("factor", 1.0)

        # â‘¢ Bayesian ë²„í‚· ë³´ì •
        bayesian_adj = 0
        try:
            buckets = conn.execute(
                """SELECT bucket_label, actual_win_rate, sample_count
                   FROM confidence_calibration ORDER BY created_at DESC LIMIT 10"""
            ).fetchall()
            if buckets and quant_score is not None:
                qs_norm = quant_score  # 0~99
                best = min(buckets, key=lambda b: abs(float(b[0].split("_")[0] if "_" in str(b[0]) else b[0]) - qs_norm), default=None)
                if best and best[2] >= 5:
                    actual_wr = float(best[1]) * 100  # ì‹¤ì œ ìŠ¹ë¥ (%)
                    bayesian_adj = round(actual_wr - 50, 1)  # 50% ê¸°ì¤€ í¸ì°¨
        except Exception:
            pass

        # â‘£ ELO ê°€ì¤‘ì¹˜ (ê¸ˆìœµë¶„ì„íŒ€ì¥ í‰ê·  ELO â†’ ì‹ ë¢°ë„ ê°€ì¤‘)
        elo_adj = 0
        try:
            from db import get_analyst_elo
            elos = [get_analyst_elo(aid)["elo_rating"] for aid in ["cio_manager"]]
            avg_elo = sum(elos) / len(elos)
            # ELO 1500 ê¸°ì¤€: 100ì  ì°¨ì´ = Â±3%p
            elo_adj = round((avg_elo - 1500) / 100 * 3, 1)
        except Exception:
            pass

        conn.close()

        # ìµœì¢… ì„œë²„ ì‹ ë¢°ë„
        base_conf = quant_score if quant_score is not None else 50
        server_conf = round(base_conf * cal_factor + bayesian_adj + elo_adj)
        server_conf = max(10, min(95, server_conf))

        return {
            "ok": True,
            "ticker": ticker,
            "server_confidence": server_conf,
            "components": {
                "quant_score": quant_score,
                "calibration_factor": round(cal_factor, 3),
                "bayesian_adj": bayesian_adj,
                "elo_adj": elo_adj,
            },
            "ai_instruction": f"ì„œë²„ ê³„ì‚° ì‹ ë¢°ë„ {server_conf}%. ë‰´ìŠ¤/ë§¥ë½ ë¶„ì„ í›„ Â±20%p ë²”ìœ„ ë‚´ì—ì„œ ì¡°ì • (ì´íƒˆ ì‹œ ì´ìœ  ëª…ì‹œ).",
            "price_bars_used": len(price_rows),
            "source": "ARGOS ì„œë²„ ê³„ì‚° (AI í˜¸ì¶œ ì—†ìŒ)"
        }
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


@app.get("/api/intelligence/status")
async def intelligence_status():
    """ì •ë³´êµ­ í†µí•© ìƒíƒœ â€” ìƒë‹¨ ìƒíƒœë°” + ì •ë³´êµ­ íƒ­ ë°ì´í„° ì†ŒìŠ¤ (Phase 6-8)."""
    try:
        conn = get_connection()
        now_kst = datetime.now(KST)

        # ARGOS ìˆ˜ì§‘ ìƒíƒœ
        argos_rows = conn.execute(
            "SELECT data_type, last_collected, last_error FROM argos_collection_status"
        ).fetchall()
        argos_map = {r[0]: {"last": r[1], "error": r[2]} for r in argos_rows}

        # í™œì„± ê°€ê²© íŠ¸ë¦¬ê±°
        triggers = _load_data("price_triggers", [])
        active_triggers = [t for t in triggers if t.get("active", True)]

        # ì˜¤ëŠ˜ AI ë¹„ìš©
        today_str = now_kst.strftime("%Y-%m-%d")
        cost_rows = conn.execute(
            """SELECT COALESCE(SUM(cost_usd), 0) FROM agent_calls
               WHERE created_at >= ?""",
            (today_str,)
        ).fetchone()
        today_cost = round(float(cost_rows[0] or 0), 4)

        week_ago = (now_kst - timedelta(days=7)).strftime("%Y-%m-%d")
        week_rows = conn.execute(
            "SELECT COALESCE(SUM(cost_usd), 0) FROM agent_calls WHERE created_at >= ?",
            (week_ago,)
        ).fetchone()
        week_cost = round(float(week_rows[0] or 0), 4)

        # ìµœê·¼ AI í™œë™ (êµì‹ ë¡œê·¸ í†µí•©)
        recent_logs = conn.execute(
            """SELECT agent_id, message, level, timestamp FROM activity_logs
               ORDER BY timestamp DESC LIMIT 20"""
        ).fetchall()
        activity = [{"agent": r[0], "msg": r[1][:100], "level": r[2], "ts": r[3]} for r in recent_logs]

        # ìµœê·¼ ì—ëŸ¬ (24ì‹œê°„)
        yesterday = (now_kst - timedelta(hours=24)).isoformat()
        error_logs = conn.execute(
            """SELECT agent_id, message, timestamp FROM activity_logs
               WHERE level='error' AND timestamp >= ?
               ORDER BY timestamp DESC LIMIT 10""",
            (yesterday,)
        ).fetchall()
        errors = [{"agent": r[0], "msg": r[1][:150], "ts": r[2]} for r in error_logs]

        # íŒ€ì¥ë³„ ë¹„ìš© (ì˜¤ëŠ˜)
        agent_costs = conn.execute(
            """SELECT agent_id, COALESCE(SUM(cost_usd), 0) as cost
               FROM agent_calls WHERE created_at >= ?
               GROUP BY agent_id ORDER BY cost DESC""",
            (today_str,)
        ).fetchall()
        per_agent = [{"agent": r[0], "cost": round(float(r[1]), 4)} for r in agent_costs]

        conn.close()

        # ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ íŒì •
        price_ok = bool(argos_map.get("price", {}).get("last"))
        news_ok  = bool(argos_map.get("news", {}).get("last"))
        has_error = bool(errors)

        return {
            "ok": True,
            "timestamp": now_kst.isoformat(),
            "status_bar": {
                "data_ok": price_ok,
                "data_last": argos_map.get("price", {}).get("last", ""),
                "ai_ok": len(activity) > 0,
                "ai_last": activity[0]["ts"] if activity else "",
                "trigger_count": len(active_triggers),
                "today_cost_usd": today_cost,
                "has_error": has_error,
            },
            "argos": argos_map,
            "triggers": active_triggers[:20],
            "activity": activity,
            "errors": errors,
            "costs": {
                "today_usd": today_cost,
                "week_usd": week_cost,
                "per_agent": per_agent,
            },
        }
    except Exception as e:
        return JSONResponse({"ok": False, "error": str(e)}, status_code=500)


@app.post("/api/argos/collect/now")
async def argos_collect_now(req: Request):
    """ìˆ˜ë™ìœ¼ë¡œ ARGOS ìˆ˜ì§‘ì„ ì¦‰ì‹œ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤."""
    body = await req.json() if req.headers.get("content-type", "").startswith("application/json") else {}
    data_type = body.get("type", "all")
    results = {}
    if data_type in ("all", "price"):
        results["price"] = await _argos_collect_prices_safe() or "ì‹¤í–‰ë¨"
    if data_type in ("all", "news"):
        results["news"] = await _argos_collect_news_safe() or "ì‹¤í–‰ë¨"
    if data_type in ("all", "dart"):
        results["dart"] = await _argos_collect_dart_safe() or "ì‹¤í–‰ë¨"
    if data_type in ("all", "macro"):
        results["macro"] = await _argos_collect_macro_safe() or "ì‹¤í–‰ë¨"
    if data_type in ("all", "financial"):
        results["financial"] = await _argos_collect_financial_safe() or "ì‹¤í–‰ë¨"
    if data_type in ("all", "sector"):
        results["sector"] = await _argos_collect_sector_safe() or "ì‹¤í–‰ë¨"
    return {"ok": True, "triggered": results}


@app.get("/api/debug/argos-diag")
async def argos_diagnostic():
    """ARGOS ìˆ˜ì§‘ ë¬¸ì œ ì§„ë‹¨ â€” ê° ë‹¨ê³„ë³„ ì„±ê³µ/ì‹¤íŒ¨ ë¦¬í¬íŠ¸. í•­ëª©ë‹¹ 15ì´ˆ íƒ€ì„ì•„ì›ƒ."""
    DIAG_TIMEOUT = 15
    diag = {}
    # 1) DB ì—°ê²°
    try:
        conn = get_connection()
        diag["db"] = "OK"
        conn.close()
    except Exception as e:
        diag["db"] = f"FAIL: {e}"
        return {"ok": False, "diag": diag}

    # 2) watchlist
    wl = _load_data("trading_watchlist", [])
    diag["watchlist"] = f"{len(wl)}ì¢…ëª©"
    kr = [w for w in wl if w.get("market", "KR") == "KR"]
    us = [w for w in wl if w.get("market") == "US"]
    diag["kr_tickers"] = [w["ticker"] for w in kr]
    diag["us_tickers"] = [w["ticker"] for w in us]

    # 3) pykrx í…ŒìŠ¤íŠ¸ (ì‚¼ì„±ì „ì 3ì¼)
    try:
        from pykrx import stock as _pk
        today = datetime.now(KST).strftime("%Y%m%d")
        start = (datetime.now(KST) - timedelta(days=3)).strftime("%Y%m%d")
        df = await asyncio.wait_for(
            asyncio.to_thread(_pk.get_market_ohlcv_by_date, start, today, "005930"),
            timeout=DIAG_TIMEOUT,
        )
        diag["pykrx"] = f"OK ({len(df)}í–‰)" if df is not None and not df.empty else "EMPTY"
    except asyncio.TimeoutError:
        diag["pykrx"] = f"TIMEOUT ({DIAG_TIMEOUT}s)"
    except Exception as e:
        diag["pykrx"] = f"FAIL: {e}"

    # 4) yfinance í…ŒìŠ¤íŠ¸ (NVDA)
    try:
        import yfinance as yf
        t = yf.Ticker("NVDA")
        h = await asyncio.wait_for(
            asyncio.to_thread(lambda: t.history(period="3d")),
            timeout=DIAG_TIMEOUT,
        )
        diag["yfinance"] = f"OK ({len(h)}í–‰)" if h is not None and not h.empty else "EMPTY"
    except asyncio.TimeoutError:
        diag["yfinance"] = f"TIMEOUT ({DIAG_TIMEOUT}s)"
    except Exception as e:
        diag["yfinance"] = f"FAIL: {e}"

    # 5) ARGOS í…Œì´ë¸” ë ˆì½”ë“œ ìˆ˜
    try:
        conn = get_connection()
        diag["price_rows"] = conn.execute("SELECT COUNT(*) FROM argos_price_history").fetchone()[0]
        diag["news_rows"] = conn.execute("SELECT COUNT(*) FROM argos_news_cache").fetchone()[0]
        diag["dart_rows"] = conn.execute("SELECT COUNT(*) FROM argos_dart_filings").fetchone()[0]
        diag["macro_rows"] = conn.execute("SELECT COUNT(*) FROM argos_macro_data").fetchone()[0]
        diag["status_rows"] = conn.execute("SELECT COUNT(*) FROM argos_collection_status").fetchone()[0]
        conn.close()
    except Exception as e:
        diag["db_check"] = f"FAIL: {e}"

    # 6) ë§¤í¬ë¡œ ìˆ˜ë™ í…ŒìŠ¤íŠ¸ (íƒ€ì„ì•„ì›ƒ ìˆìŒ)
    try:
        n = await asyncio.wait_for(_argos_collect_macro(), timeout=60)
        diag["macro_test"] = f"OK ({n}ê±´ ìˆ˜ì§‘)"
    except asyncio.TimeoutError:
        diag["macro_test"] = "TIMEOUT (60s)"
    except Exception as e:
        diag["macro_test"] = f"FAIL: {e}"

    return {"ok": True, "diag": diag}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# ë¸Œë¡œë“œìºìŠ¤íŠ¸ í‚¤ì›Œë“œ (ëª¨ë“  ë¶€ì„œì— ë™ì‹œ ì „ë‹¬í•˜ëŠ” ëª…ë ¹)
_BROADCAST_KEYWORDS = [
    "ì „ì²´", "ëª¨ë“  ë¶€ì„œ", "ì¶œì„", "íšŒì˜", "í˜„í™© ë³´ê³ ",
    "ì´ê´„", "ì „ì›", "ê° ë¶€ì„œ", "ì¶œì„ì²´í¬", "ë¸Œë¦¬í•‘",
]

# íŒ€ì¥/ë¹„ì„œì‹¤ì¥ â†’ ì†Œì† ì „ë¬¸ê°€ ë§¤í•‘
# 2026-02-25: ì „ë¬¸ê°€ ì „ì› ë™ë©´ â†’ íŒ€ì¥ ë‹¨ë… ë¶„ì„ ì²´ì œ.
# ì¬ë„ì… ì‹œì : íŒ€ì¥ í˜¼ì 30ë¶„+ & ë³‘ë ¬ì´ ì˜ë¯¸ ìˆì„ ë•Œ (CLAUDE.md ê·œì¹™)
_MANAGER_SPECIALISTS: dict[str, list[str]] = {
    "chief_of_staff": [],
    "cso_manager": [],
    "clo_manager": [],
    "cmo_manager": [],
    "cio_manager": [],
    "cpo_manager": [],
}

# ë§¤ë‹ˆì € â†’ ë¶€ì„œ ë§¤í•‘ (í’ˆì§ˆê²€ìˆ˜ ë£¨ë¸Œë¦­ ì¡°íšŒìš©)
_MANAGER_DIVISION: dict[str, str] = {
    "chief_of_staff": "secretary",
    "cso_manager": "leet_master.strategy",
    "clo_manager": "leet_master.legal",
    "cmo_manager": "leet_master.marketing",
    "cio_manager": "finance.investment",
    "cpo_manager": "publishing",
}
# ë™ë©´ ë¶€ì„œ (í’ˆì§ˆê²€ìˆ˜ ì œì™¸)
_DORMANT_MANAGERS: set[str] = set()

# app_state.quality_gate â†’ app_state.quality_gate ì§ì ‘ ì‚¬ìš©

def _init_quality_gate():
    """í’ˆì§ˆê²€ìˆ˜ ê²Œì´íŠ¸ ì´ˆê¸°í™”."""

    if not _QUALITY_GATE_AVAILABLE:
        _log("[QA] QualityGate ëª¨ë“ˆ ë¯¸ì„¤ì¹˜ â€” í’ˆì§ˆê²€ìˆ˜ ë¹„í™œì„±")
        return
    config_path = Path(__file__).parent.parent / "config" / "quality_rules.yaml"
    app_state.quality_gate = QualityGate(config_path)
    _log("[QA] í’ˆì§ˆê²€ìˆ˜ ê²Œì´íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")


class _QAModelRouter:
    """ask_ai()ë¥¼ ModelRouter.complete() ì¸í„°í˜ì´ìŠ¤ë¡œ ê°ì‹¸ëŠ” ì–´ëŒ‘í„° (í’ˆì§ˆê²€ìˆ˜ìš©)."""

    async def complete(self, model_name="", messages=None,
                       temperature=0.0, max_tokens=4096,
                       agent_id="", **kwargs):
        from src.llm.base import LLMResponse
        messages = messages or []
        system_prompt = ""
        user_message = ""
        for msg in messages:
            if msg.get("role") == "system":
                system_prompt = msg["content"]
            elif msg.get("role") == "user":
                user_message = msg["content"]
        result = await ask_ai(user_message, system_prompt, model_name)
        if "error" in result:
            return LLMResponse(
                content=f"[QA ì˜¤ë¥˜] {result['error']}",
                model=model_name,
                input_tokens=0, output_tokens=0,
                cost_usd=0.0, provider="unknown",
            )
        return LLMResponse(
            content=result["content"],
            model=result.get("model", model_name),
            input_tokens=result.get("input_tokens", 0),
            output_tokens=result.get("output_tokens", 0),
            cost_usd=result.get("cost_usd", 0.0),
            provider=result.get("provider", "unknown"),
        )

_qa_router = _QAModelRouter()


async def _quality_review_specialists(
    chain: dict,
    previous_reviews: dict | None = None,
) -> list[dict]:
    """ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ë§¤ë‹ˆì € ëª¨ë¸ë¡œ ê°œë³„ ê²€ìˆ˜. ë¶ˆí•©ê²© ëª©ë¡ ë°˜í™˜.

    previous_reviews: {agent_id: HybridReviewResult} â€” ì¬ì‘ì—… ì‹œ ì´ì „ ê²€ìˆ˜ ê²°ê³¼.
        ì œê³µë˜ë©´ ì‚¬ìœ  íŠ¹ì • ì¬ê²€ìˆ˜ (ë°˜ë ¤ í•­ëª©ë§Œ ì¬í‰ê°€, ë‚˜ë¨¸ì§€ëŠ” ì´ì „ ì ìˆ˜ ìœ ì§€).

    Returns: [{"agent_id": ..., "review": HybridReviewResult, "content": ...}, ...]
    """
    if not app_state.quality_gate or not _QUALITY_GATE_AVAILABLE:
        return []

    target_id = chain.get("target_id", "chief_of_staff")
    if target_id in _DORMANT_MANAGERS:
        return []

    division = _MANAGER_DIVISION.get(target_id, "default")
    reviewer_model = _get_model_override(target_id) or "claude-sonnet-4-6"
    task_desc = chain.get("original_command", "")[:500]
    failed = []

    # #2: ê²€ìˆ˜ ì‹œì‘ ë¡œê·¸ â€” ì „ ì§ì› ì‘ì—… ì‹œì‘ ì‹œ ë¡œê·¸ ê¸°ë¡
    _spec_ids = list(chain.get("results", {}).get("specialists", {}).keys())
    if _spec_ids:
        _spec_names = ", ".join(_AGENT_NAMES.get(s, _SPECIALIST_NAMES.get(s, s)) for s in _spec_ids[:4])
        _qa_start_log = save_activity_log(
            target_id, f"ğŸ” ê²€ìˆ˜ ì‹œì‘: {_spec_names} ({len(_spec_ids)}ëª…)", level="qa_start"
        )
        await wm.send_activity_log(_qa_start_log)

    for agent_id, result_data in chain.get("results", {}).get("specialists", {}).items():
        content = result_data.get("content", "")

        # â˜… ì‚¬ìœ  íŠ¹ì • ì¬ê²€ìˆ˜ ëª¨ë“œ: ì´ì „ì— í•©ê²©í•œ ì „ë¬¸ê°€ëŠ” ê±´ë„ˆëœ€ (LLM ë¹„ìš© ì ˆì•½)
        if previous_reviews and agent_id not in previous_reviews:
            continue  # ì´ ì „ë¬¸ê°€ëŠ” ì´ì „ ê²€ìˆ˜ì—ì„œ í•©ê²© â†’ ì¬ê²€ìˆ˜ ë¶ˆí•„ìš”

        if result_data.get("error"):
            # ì—ëŸ¬ ê²°ê³¼ëŠ” ìë™ ë¶ˆí•©ê²© ì²˜ë¦¬
            failed.append({
                "agent_id": agent_id,
                "review": None,
                "content": content,
                "reason": f"ì—ëŸ¬ ì‘ë‹µ: {result_data.get('error', '')[:100]}",
            })
            continue

        # QAì— ë„êµ¬ ì‚¬ìš© ê¸°ë¡ í¬í•¨ â€” D1 + Q1 íŒì •ì„ ìœ„í•´ í•„ìˆ˜
        _qa_content = content
        _spec_tools = result_data.get("tools_used", [])
        if _spec_tools:
            _unique_tools = list(dict.fromkeys(_spec_tools))
            # ë„êµ¬ë³„ í˜¸ì¶œ íšŸìˆ˜ ì§‘ê³„
            from collections import Counter as _Counter
            _tool_counts = _Counter(_spec_tools)
            _tool_detail = ", ".join(f"{t}({c}íšŒ)" for t, c in _tool_counts.most_common())
            _qa_content += (
                f"\n\n---\n## ì‚¬ìš©í•œ ë„êµ¬ (ì´ {len(_spec_tools)}íšŒ í˜¸ì¶œ, ê³ ìœ  {len(_unique_tools)}ì¢…)\n"
                f"{_tool_detail}\n"
                f"â€» ìœ„ ë„êµ¬ë“¤ì€ ì‹¤ì‹œê°„ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë¶„ì„ ë‹¹ì¼ì˜ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ ê²ƒì…ë‹ˆë‹¤.\n"
                f"â€» ë„êµ¬ê°€ ë°˜í™˜í•œ ìˆ˜ì¹˜(ì£¼ê°€, ì¬ë¬´ì œí‘œ, ê±°ì‹œì§€í‘œ ë“±)ëŠ” ì •í™•í•œ ì‹¤ì‹œê°„ ë°ì´í„°ì…ë‹ˆë‹¤."
            )

        try:
            # â˜… ì‚¬ìœ  íŠ¹ì • ì¬ê²€ìˆ˜: ì´ì „ ë¦¬ë·°ê°€ ìˆìœ¼ë©´ ë°˜ë ¤ í•­ëª©ë§Œ ì¬í‰ê°€
            _prev_review = (previous_reviews or {}).get(agent_id)
            if _prev_review is not None:
                review = await app_state.quality_gate.targeted_hybrid_review(
                    result_data=_qa_content,
                    task_description=task_desc,
                    model_router=_qa_router,
                    previous_review=_prev_review,
                    reviewer_id=target_id,
                    reviewer_model=reviewer_model,
                    division=division,
                    target_agent_id=agent_id,
                )
            else:
                review = await app_state.quality_gate.hybrid_review(
                    result_data=_qa_content,
                    task_description=task_desc,
                    model_router=_qa_router,
                    reviewer_id=target_id,
                    reviewer_model=reviewer_model,
                    division=division,
                    target_agent_id=agent_id,
                )
            # í†µê³„ ê¸°ë¡ (ë©”ëª¨ë¦¬)
            app_state.quality_gate.record_review(review, target_id, agent_id, task_desc)
            chain["total_cost_usd"] += getattr(review, "_cost", 0)

            # â˜… í’ˆì§ˆê²€ìˆ˜ í†µí•© ë¡œê·¸ â€” ì „ë¬¸ê°€ë‹¹ 1ê±´ (Phase 4: #10/#10-2)
            _spec_name = _SPECIALIST_NAMES.get(agent_id, agent_id)
            _qa_parts = []
            for ci in review.checklist_results:
                _ico = "âœ…" if ci.passed else "âŒ"
                _req = "[í•„]" if ci.required else ""
                _qa_parts.append(f"{ci.id}{_ico}{_req}")
            for si in review.score_results:
                _crit = "â¬‡" if si.critical and si.score == 1 else ""
                _qa_parts.append(f"{si.id}:{si.score}{_crit}")
            _pass_icon = "âœ…" if review.passed else "âŒ"
            _pass_text = "í•©ê²©" if review.passed else "ë¶€í•©ê²©"
            _qa_summary = f"{_pass_icon} {_spec_name} {_pass_text}({review.weighted_average:.1f}) {' '.join(_qa_parts)}"
            _qa_unified_log = save_activity_log(
                agent_id, _qa_summary, level="qa_detail"
            )
            await wm.send_activity_log(_qa_unified_log)

            # DBì— ê²€ìˆ˜ ê²°ê³¼ ì €ì¥
            import json as _json
            try:
                save_quality_review(
                    chain_id=chain.get("chain_id", ""),
                    reviewer_id=target_id,
                    target_id=agent_id,
                    division=division,
                    passed=review.passed,
                    weighted_score=review.weighted_average,
                    checklist_json=_json.dumps(
                        [{"id": c.id, "passed": c.passed, "required": c.required}
                         for c in review.checklist_results], ensure_ascii=False
                    ),
                    scores_json=_json.dumps(
                        [{"id": s.id, "score": s.score, "weight": s.weight}
                         for s in review.score_results], ensure_ascii=False
                    ),
                    feedback=review.feedback[:500],
                    rejection_reasons=" / ".join(review.rejection_reasons)[:500] if review.rejection_reasons else "",
                    review_model=review.review_model,
                )
            except Exception as e:
                logger.debug("ê²€ìˆ˜ ê²°ê³¼ DB ì €ì¥ ì‹¤íŒ¨: %s", e)

            # â˜… ê¸°ë°€ë¬¸ì„œìš©: ëª¨ë“  ë¦¬ë·° ê²°ê³¼ ìˆ˜ì§‘ (í•©ê²©/ë¶ˆí•©ê²© ë¬´ê´€)
            chain.setdefault("qa_reviews", []).append({
                "agent_id": agent_id,
                "passed": review.passed,
                "weighted_average": review.weighted_average,
                "review_dict": review.to_dict(),
            })

            if not review.passed:
                reason = " / ".join(review.rejection_reasons) if review.rejection_reasons else "í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬"
                failed.append({
                    "agent_id": agent_id,
                    "review": review,
                    "content": content,
                    "reason": reason,
                })
                _log(f"[QA] âŒ ë¶ˆí•©ê²©: {agent_id} (ì ìˆ˜={review.weighted_average:.1f}, ì‚¬ìœ ={reason[:80]})")
                # QA ë¶ˆí•©ê²© ì‹¤ì‹œê°„ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ê²€ìˆ˜ë¡œê·¸ íƒ­ì— í‘œì‹œ)
                qa_log = save_activity_log(
                    agent_id,
                    f"âŒ [{agent_id}] ë¶ˆí•©ê²© (ì ìˆ˜ {review.weighted_average:.1f}) â€” {reason[:60]}",
                    level="qa_fail"
                )
                await wm.send_activity_log(qa_log)

                # â”€â”€ Phase 3: ë°˜ë ¤ì‚¬ìœ  êµì‹ ë¡œê·¸ + ê¸°ë°€ë¬¸ì„œ + ë°˜ë ¤ í•™ìŠµ â”€â”€
                _spec_name_rej = _SPECIALIST_NAMES.get(agent_id, agent_id)
                # (A) êµì‹ ë¡œê·¸ì— ë°˜ë ¤ ë©”ì‹œì§€ broadcast
                _rej_comms = {
                    "id": f"rej_{chain.get('chain_id', '')[:6]}_{agent_id[:8]}",
                    "sender": target_id,
                    "receiver": agent_id,
                    "message": f"âŒ {_spec_name_rej} ë°˜ë ¤: {reason[:200]}",
                    "log_type": "delegation",
                    "source": "qa_rejection",
                    "status": "ë°˜ë ¤",
                    "created_at": datetime.now().isoformat(),
                }
                await _broadcast_comms(_rej_comms)

                # (B) ê¸°ë°€ë¬¸ì„œì— ë°˜ë ¤ì‚¬ìœ  ì €ì¥
                from datetime import datetime as _dt_rej
                _rej_date = _dt_rej.now().strftime("%Y%m%d_%H%M")
                _rej_filename = f"ë°˜ë ¤ì‚¬ìœ _{_spec_name_rej}_{_rej_date}.md"
                _rej_detail = []
                for ci in review.checklist_results:
                    if not ci.passed:
                        _rej_detail.append(f"- {ci.id} {ci.label}: âŒ ë¶ˆí†µê³¼{' [í•„ìˆ˜]' if ci.required else ''}")
                for si in review.score_results:
                    if si.score <= 3:
                        _fb = f" â€” {si.feedback}" if si.feedback else ""
                        _rej_detail.append(f"- {si.id} {si.label}: {si.score}ì /5{_fb}")
                _rej_content = (
                    f"# ë°˜ë ¤ì‚¬ìœ  â€” {_spec_name_rej}\n\n"
                    f"**ì ìˆ˜**: {review.weighted_average:.1f}/5.0\n"
                    f"**ì‚¬ìœ **: {reason}\n\n"
                    f"## í•­ëª©ë³„ ë¬¸ì œì \n" + "\n".join(_rej_detail) + "\n\n"
                    f"## í”¼ë“œë°±\n{review.feedback[:500]}\n"
                )
                try:
                    save_archive(division, _rej_filename, _rej_content,
                                 correlation_id=chain.get("chain_id", ""),
                                 agent_id=target_id)
                except Exception as _ae:
                    logger.debug("ë°˜ë ¤ì‚¬ìœ  ê¸°ë°€ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: %s", _ae)

                # (C) ë°˜ë ¤ í•™ìŠµ: warnings ì¹´í…Œê³ ë¦¬ì— êµí›ˆ ì €ì¥
                try:
                    _mem_key = f"memory_categorized_{agent_id}"
                    _existing_mem = load_setting(_mem_key, {})
                    _warning_lesson = f"{_dt_rej.now().strftime('%m/%d')}: {reason[:100]}"
                    _prev_warnings = _existing_mem.get("warnings", "")
                    _existing_mem["warnings"] = (
                        (_prev_warnings + " | " + _warning_lesson).strip(" |")
                        if _prev_warnings else _warning_lesson
                    )
                    save_setting(_mem_key, _existing_mem)
                    _log(f"[QA] ë°˜ë ¤ í•™ìŠµ ì €ì¥: {agent_id} â† {_warning_lesson[:60]}")
                except Exception as _me:
                    logger.debug("ë°˜ë ¤ í•™ìŠµ ì €ì¥ ì‹¤íŒ¨: %s", _me)
            else:
                _log(f"[QA] âœ… í•©ê²©: {agent_id} (ì ìˆ˜={review.weighted_average:.1f})")
                # QA í•©ê²© ì‹¤ì‹œê°„ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ê²€ìˆ˜ë¡œê·¸ íƒ­ì— í‘œì‹œ)
                qa_log = save_activity_log(
                    agent_id,
                    f"âœ… [{agent_id}] í•©ê²© (ì ìˆ˜ {review.weighted_average:.1f})",
                    level="qa_pass"
                )
                await wm.send_activity_log(qa_log)

        except Exception as e:
            _log(f"[QA] ê²€ìˆ˜ ì˜¤ë¥˜ ({agent_id}): {e}")
            # ê²€ìˆ˜ ì‹¤íŒ¨ ì‹œ í†µê³¼ ì²˜ë¦¬ (ì—…ë¬´ ì°¨ë‹¨ ë°©ì§€)

    return failed


async def _handle_specialist_rework(chain: dict, failed_specs: list[dict], attempt: int = 1):
    """ë¶ˆí•©ê²© ì „ë¬¸ê°€ì—ê²Œ ì¬ì‘ì—… ì§€ì‹œ â†’ ì¬ê²€ìˆ˜.

    attempt: í˜„ì¬ ì¬ì‹œë„ íšŸìˆ˜ (1 ë˜ëŠ” 2)
    max_retry: quality_rules.yamlì—ì„œ ì„¤ì • (ê¸°ë³¸ 2)
    """
    max_retry = app_state.quality_gate.max_retry if app_state.quality_gate else 2
    if attempt > max_retry:
        # ì¬ì‹œë„ ì´ˆê³¼ â†’ ê²½ê³  ë±ƒì§€ ë¶€ì°© í›„ ì¢…í•© ë‹¨ê³„ë¡œ ì§„í–‰
        for spec in failed_specs:
            agent_id = spec["agent_id"]
            _log(f"[QA] âš ï¸ ì¬ì‘ì—… {max_retry}íšŒ ì´ˆê³¼ â€” {agent_id} ê²°ê³¼ë¥¼ ê²½ê³  í¬í•¨ ì±„ ì¢…í•© ì§„í–‰")
            existing = chain["results"]["specialists"].get(agent_id, {})
            existing["quality_warning"] = spec.get("reason", "í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬")[:200]
            chain["results"]["specialists"][agent_id] = existing
        return

    target_id = chain.get("target_id", "chief_of_staff")
    target_name = _AGENT_NAMES.get(target_id, target_id)
    task_desc = chain.get("original_command", "")[:500]

    await _broadcast_chain_status(
        chain,
        f"ğŸ”„ í’ˆì§ˆê²€ìˆ˜ ë¶ˆí•©ê²© {len(failed_specs)}ê±´ â†’ ì¬ì‘ì—… ì§€ì‹œ (ì‹œë„ {attempt}/{max_retry})"
    )

    # â”€â”€ ê°œë³„ ì „ë¬¸ê°€ ì¬ì‘ì—… ì½”ë£¨í‹´ (ë³‘ë ¬ ì‹¤í–‰ìš©) â”€â”€
    async def _do_single_rework(spec: dict) -> None:
        agent_id = spec["agent_id"]
        reason = spec.get("reason", "í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬")
        original_content = spec.get("content", "")  # ì „ë¬¸ ì²¨ë¶€ (ë¶€ë¶„ìˆ˜ì •ì„ ìœ„í•´)

        # ì „ë¬¸ê°€ ì´ˆë¡ë¶ˆ ë‹¤ì‹œ ì¼œê¸°
        agent_name = _AGENT_NAMES.get(agent_id, agent_id)
        await _broadcast_status(agent_id, "working", 0.5, f"{agent_name} ì¬ì‘ì—… ì¤‘...")

        # â˜… QA í•­ëª©ë³„ êµ¬ì²´ì  ë¬¸ì œì  ìƒì„± (ì¬ì‘ì—… ì‹œ ë­˜ ê³ ì³ì•¼ í•˜ëŠ”ì§€ ëª…í™•íˆ)
        _review = spec.get("review")
        _detail_lines = []
        _failed_ids: list[str] = []  # â˜… ë°˜ë ¤ í•­ëª© ID ë¦¬ìŠ¤íŠ¸ (ì‚¬ìœ  íŠ¹ì • ì¬ê²€ìˆ˜ìš©)
        if _review:
            from src.core.quality_gate import QualityGate as _QG
            _failed_ids = _QG.get_failed_item_ids(_review)
            # ë¶ˆí•©ê²© í•­ëª©ë§Œ ìƒì„¸ í‘œì‹œ (í†µê³¼ í•­ëª©ì€ ê°„ëµíˆ)
            for ci in _review.checklist_results:
                if not ci.passed:
                    _rq = " [í•„ìˆ˜]" if ci.required else ""
                    _fb = f" â€” {ci.feedback}" if ci.feedback else ""
                    _detail_lines.append(f"- âŒ {ci.id} {ci.label}{_rq}{_fb}")
            for si in _review.score_results:
                if si.score <= 1:
                    _crit = " âš ï¸ì¹˜ëª…ì " if si.critical else ""
                    _fb = f" â€” {si.feedback}" if si.feedback else ""
                    _detail_lines.append(f"- âŒ {si.id} {si.label}: {si.score}ì /5{_crit}{_fb}")
        _detail_block = "\n".join(_detail_lines) if _detail_lines else "(ìƒì„¸ í•­ëª© ì—†ìŒ)"
        _failed_ids_str = ", ".join(_failed_ids) if _failed_ids else "(ì „ì²´)"

        rework_prompt = (
            f"[ì¬ì‘ì—… ìš”ì²­ #{attempt}] ë‹¹ì‹ ì˜ ë³´ê³ ì„œê°€ í’ˆì§ˆê²€ìˆ˜ì—ì„œ ë¶ˆí•©ê²©ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            f"## ë°˜ë ¤ í•­ëª© ID: {_failed_ids_str}\n"
            f"âš ï¸ ìœ„ í•­ëª©ë§Œ ìˆ˜ì •í•˜ì„¸ìš”. í†µê³¼í•œ í•­ëª©ì€ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”.\n"
            f"âš ï¸ ì¬ê²€ìˆ˜ ì‹œ ìœ„ í•­ëª©ë§Œ ì¬ì±„ì ë©ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ëŠ” ì´ì „ ì ìˆ˜ê°€ ìœ ì§€ë©ë‹ˆë‹¤.\n\n"
            f"## ì›ë˜ ì—…ë¬´ ì§€ì‹œ\n{task_desc}\n\n"
            f"## ë¶ˆí•©ê²© ì‚¬ìœ \n{reason}\n\n"
            f"## í•­ëª©ë³„ ê²€ìˆ˜ ê²°ê³¼\n{_detail_block}\n\n"
            f"## ë‹¹ì‹ ì˜ ì´ì „ ë³´ê³ ì„œ (ì „ë¬¸)\n{original_content}\n\n"
            f"## ì§€ì‹œì‚¬í•­\n"
            f"âš ï¸ ë°˜ë ¤ëœ í•­ëª©ë§Œ ìˆ˜ì •í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.\n"
            f"- ì •í™•í–ˆë˜ ìˆ˜ì¹˜(ë§¤ì¶œ, PER, ì£¼ê°€ ë“±)ë¥¼ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš”.\n"
            f"- ì§€ì ëœ ë¶€ë¶„ë§Œ ë³´ì™„í•˜ì„¸ìš” (ë„êµ¬ ì¬í˜¸ì¶œí•˜ì—¬ ìµœì‹  ë°ì´í„° í™•ì¸).\n"
            f"- ë³´ê³ ì„œ ì „ì²´ë¥¼ ë‹¤ì‹œ ì“°ì§€ ë§ê³ , ë¬¸ì œ í•­ëª©ì„ ì •í™•íˆ ìˆ˜ì •í•˜ì„¸ìš”."
        )

        try:
            # ì „ë¬¸ê°€ ëª¨ë¸ë¡œ ì¬ì‘ì—… ì‹¤í–‰ (â˜… ë„êµ¬ í¬í•¨! â€” ì¬ì‘ì—…ì—ì„œë„ API í˜¸ì¶œ ê°€ëŠ¥)
            spec_model = _get_model_override(agent_id) or "claude-sonnet-4-6"
            spec_soul = _load_agent_prompt(agent_id)
            rework_tool_schemas = None
            rework_tool_executor = None
            rework_tools_used: list[str] = []
            _rw_detail = _AGENTS_DETAIL.get(agent_id, {})
            _rw_allowed = _rw_detail.get("allowed_tools", [])
            if _rw_allowed:
                _rw_schemas = _load_tool_schemas(allowed_tools=_rw_allowed)
                if _rw_schemas.get("anthropic"):
                    rework_tool_schemas = _rw_schemas["anthropic"]
                    _rw_max = int(_rw_detail.get("max_tool_calls", 5))
                    # í´ë¡œì € ìº¡ì²˜: í•¨ìˆ˜ ì¸ìë¡œ ë°”ì¸ë”©í•˜ì—¬ ë³‘ë ¬ ì•ˆì „
                    _captured_id = agent_id
                    _captured_name = agent_name

                    async def _rework_executor(tool_name: str, tool_input: dict,
                                               _aid=_captured_id, _aname=_captured_name):
                        rework_tools_used.append(tool_name)
                        _cnt = len(rework_tools_used)
                        await _broadcast_status(
                            _aid, "working", 0.5 + min(_cnt / _rw_max, 1.0) * 0.3,
                            f"{tool_name} ì‹¤í–‰ ì¤‘... (ì¬ì‘ì—…)",
                        )
                        _rw_log = save_activity_log(
                            _aid,
                            f"ğŸ”§ [{_aname}] {tool_name} í˜¸ì¶œ ({_cnt}íšŒ) [ì¬ì‘ì—…#{attempt}]",
                            level="tool",
                        )
                        await wm.send_activity_log(_rw_log)
                        pool = _init_tool_pool()
                        if pool:
                            return await pool.invoke(tool_name, caller_id=_aid, **tool_input)
                        return f"ë„êµ¬ '{tool_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                    rework_tool_executor = _rework_executor

            result = await ask_ai(
                user_message=rework_prompt,
                system_prompt=spec_soul,
                model=spec_model,
                tools=rework_tool_schemas,
                tool_executor=rework_tool_executor,
                reasoning_effort=_get_agent_reasoning_effort(agent_id),
            )

            if "error" not in result:
                # ì¬ì‘ì—… ê²°ê³¼ë¡œ êµì²´ (ë„êµ¬ ì‚¬ìš© ê¸°ë¡ í¬í•¨)
                chain["results"]["specialists"][agent_id] = {
                    "content": result["content"],
                    "model": result.get("model", spec_model),
                    "cost_usd": result.get("cost_usd", 0),
                    "rework_attempt": attempt,
                    "tools_used": result.get("tools_used", []),
                }
                chain["total_cost_usd"] += result.get("cost_usd", 0)
                _log(f"[QA] ì¬ì‘ì—… ì™„ë£Œ: {agent_id} (ì‹œë„ {attempt})")

                # â”€â”€ Phase 3: ì¬ì‘ì—… ë³´ê³ ì„œ ê¸°ë°€ë¬¸ì„œ ì €ì¥ + í™œë™ë¡œê·¸ â”€â”€
                from datetime import datetime as _dt_rw
                _rw_date = _dt_rw.now().strftime("%Y%m%d_%H%M")
                _rw_div = _AGENT_DIVISION.get(agent_id, "default")
                _rw_filename = f"{agent_name}_ë³´ê³ ì„œ_ì¬ì‘ì—…v{attempt}_{_rw_date}.md"
                try:
                    save_archive(
                        _rw_div, _rw_filename, result["content"],
                        correlation_id=chain.get("chain_id", ""),
                        agent_id=agent_id,
                    )
                except Exception as _ae2:
                    logger.debug("ì¬ì‘ì—… ê¸°ë°€ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: %s", _ae2)
                _rw_log = save_activity_log(
                    agent_id,
                    f"ğŸ”„ [{agent_name}] ì¬ì‘ì—… ë³´ê³ ì„œ ì œì¶œ (v{attempt})",
                    level="info",
                )
                await wm.send_activity_log(_rw_log)
            else:
                _log(f"[QA] ì¬ì‘ì—… ì‹¤íŒ¨: {agent_id} â€” {result.get('error', '')[:100]}")

        except Exception as e:
            _log(f"[QA] ì¬ì‘ì—… ì˜¤ë¥˜ ({agent_id}): {e}")

        # ì „ë¬¸ê°€ ì´ˆë¡ë¶ˆ ë„ê¸°
        await _broadcast_status(agent_id, "done", 1.0, "ì¬ì‘ì—… ì™„ë£Œ")

    # â”€â”€ ë¶ˆí•©ê²© ì „ë¬¸ê°€ ì¬ì‘ì—… (ì „ì› ì¦‰ì‹œ ë³‘ë ¬) â”€â”€
    await asyncio.gather(*[_do_single_rework(spec) for spec in failed_specs])

    # â˜… ì‚¬ìœ  íŠ¹ì • ì¬ê²€ìˆ˜: ì´ì „ ê²€ìˆ˜ ê²°ê³¼ë¥¼ ì „ë‹¬í•˜ì—¬ ë°˜ë ¤ í•­ëª©ë§Œ ì¬í‰ê°€
    _prev_reviews = {}
    for spec in failed_specs:
        _rv = spec.get("review")
        if _rv is not None:
            _prev_reviews[spec["agent_id"]] = _rv

    _save_chain(chain)
    still_failed = await _quality_review_specialists(chain, previous_reviews=_prev_reviews)

    if still_failed:
        # ì•„ì§ ë¶ˆí•©ê²©ì¸ ê±´ â†’ ë‹¤ì‹œ ì¬ì‘ì—… (attempt+1)
        await _handle_specialist_rework(chain, still_failed, attempt + 1)
    else:
        _log(f"[QA] ì¬ì‘ì—… í›„ ì „ì› í•©ê²© (ì‹œë„ {attempt})")


# Bì•ˆ: ì „ë¬¸ê°€ë³„ ì—­í•  prefix â€” íŒ€ì¥ì´ ìœ„ì„í•  ë•Œ CEO ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ì§€ ì•Šê³ ,
# ê° ì „ë¬¸ê°€ì˜ ì—­í• ì— ë§ëŠ” ì§€ì‹œë¥¼ ì•ì— ë¶™ì—¬ì„œ ë³´ëƒ„
# ì „ë¬¸ê°€ ì „ì› ì œê±° (2026-02-26). ì¬ë„ì… ì‹œ ì—¬ê¸°ì— ì¶”ê°€.
_SPECIALIST_ROLE_PREFIX: dict[str, str] = {}

# ì „ë¬¸ê°€ ID â†’ í•œêµ­ì–´ ì´ë¦„ (AGENTS ë¦¬ìŠ¤íŠ¸ì—ì„œ ìë™ êµ¬ì¶•)
_SPECIALIST_NAMES: dict[str, str] = {}
for _a in AGENTS:
    if _a["role"] == "specialist":
        _SPECIALIST_NAMES[_a["agent_id"]] = _a["name_ko"]

# í…”ë ˆê·¸ë¨ ì§ì›ì½”ë“œ ë§¤í•‘ (agents.yamlì˜ telegram_code í•„ë“œì—ì„œ ìë™ êµ¬ì¶•)
_TELEGRAM_CODES: dict[str, str] = {}
for _a in AGENTS:
    if _a.get("telegram_code"):
        _TELEGRAM_CODES[_a["agent_id"]] = _a["telegram_code"]


def _tg_code(agent_id: str) -> str:
    """agent_id â†’ í…”ë ˆê·¸ë¨ ì½”ë“œëª… (ì—†ìœ¼ë©´ name_ko í´ë°±)."""
    return _TELEGRAM_CODES.get(
        agent_id,
        _AGENT_NAMES.get(agent_id, _SPECIALIST_NAMES.get(agent_id, agent_id))
    )


def _tg_convert_names(text: str) -> str:
    """í…ìŠ¤íŠ¸ ë‚´ ì—ì´ì „íŠ¸ ì´ë¦„ì„ í…”ë ˆê·¸ë¨ ì½”ë“œëª…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    for aid, code in _TELEGRAM_CODES.items():
        name = _AGENT_NAMES.get(aid, _SPECIALIST_NAMES.get(aid, ""))
        if name and name in text:
            text = text.replace(name, code)
    return text


def _is_broadcast_command(text: str) -> bool:
    """ë¸Œë¡œë“œìºìŠ¤íŠ¸ ëª…ë ¹ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return any(kw in text for kw in _BROADCAST_KEYWORDS)


async def _broadcast_status(agent_id: str, status: str, progress: float, detail: str = ""):
    """ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ëª¨ë“  WebSocket í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ì†¡í•©ë‹ˆë‹¤.

    í”„ë¡ íŠ¸ì—”ë“œì˜ ìƒíƒœ í‘œì‹œë“±(ì´ˆë¡ë¶ˆ ê¹œë¹¡ì„)ì„ ì œì–´í•©ë‹ˆë‹¤.
    status: 'working' | 'done' | 'idle'
    """
    await wm.send_agent_status(agent_id, status, progress, detail)


async def _extract_and_save_memory(agent_id: str, task: str, response: str):
    """ëŒ€í™” í›„ ê¸°ì–µí•  ì •ë³´ ì¶”ì¶œ â†’ save_settingì— ì €ì¥ (ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ)."""
    try:
        extraction_prompt = (
            "ì•„ë˜ ëŒ€í™”ì—ì„œ ì—ì´ì „íŠ¸ê°€ ê¸°ì–µí•´ì•¼ í•  ì •ë³´ê°€ ìˆìœ¼ë©´ JSONìœ¼ë¡œ ì¶”ì¶œí•´ë¼. "
            "ì—†ìœ¼ë©´ ë¹ˆ dict {} ë°˜í™˜.\n\n"
            f"[ëŒ€í™”]\nì‚¬ìš©ì: {task[:400]}\nì—ì´ì „íŠ¸: {response[:400]}\n\n"
            "[ì¶”ì¶œ í•­ëª©]\n"
            "- ceo_preferences: CEOê°€ ì„ í˜¸í•˜ê±°ë‚˜ ì‹«ì–´í•˜ëŠ” ê²ƒ (ìˆìœ¼ë©´)\n"
            "- decisions: '~í•˜ê¸°ë¡œ ê²°ì •', '~ë¡œ í™•ì •' ë“± ì¤‘ìš” ê²°ì • (ìˆìœ¼ë©´)\n"
            "- warnings: ì´ ë°©ë²•ì€ ì•ˆ ë¨, CEOê°€ ì‹«ë‹¤ê³  í•¨ ë“± ì£¼ì˜ì‚¬í•­ (ìˆìœ¼ë©´)\n"
            "- context: í”„ë¡œì íŠ¸ ìƒíƒœ, ê±°ë˜ì²˜, ì¼ì • ë“± ì¤‘ìš” ë§¥ë½ (ìˆìœ¼ë©´)\n\n"
            "JSONë§Œ ë°˜í™˜ (ì„¤ëª… ì—†ì´):"
        )

        # ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ë¡œ ë©”ëª¨ë¦¬ ì¶”ì¶œ (Gemini Flash â†’ GPT Mini â†’ Claude)
        _mem_providers = get_available_providers()
        if _mem_providers.get("google"):
            _mem_model = "gemini-2.5-flash"
        elif _mem_providers.get("openai"):
            _mem_model = "gpt-5-mini"
        else:
            _mem_model = "claude-sonnet-4-6"
        result = await ask_ai(
            user_message=extraction_prompt,
            model=_mem_model,
            max_tokens=400,
            system_prompt="JSONë§Œ ë°˜í™˜. ì„¤ëª… ì—†ì´."
        )

        text_resp = result.get("content", "") if isinstance(result, dict) else str(result)
        text_resp = text_resp.strip()
        # JSON ë¸”ë¡ ì¶”ì¶œ
        if "```" in text_resp:
            text_resp = text_resp.split("```")[1].strip()
            if text_resp.startswith("json"):
                text_resp = text_resp[4:].strip()

        new_facts = json.loads(text_resp)
        if new_facts and isinstance(new_facts, dict):
            existing = load_setting(f"memory_categorized_{agent_id}", {})
            for key, val in new_facts.items():
                if val and val not in ("null", "ì—†ìŒ", ""):
                    prev = existing.get(key, "")
                    existing[key] = (prev + " | " + str(val)).strip(" |") if prev else str(val)
            save_setting(f"memory_categorized_{agent_id}", existing)
    except Exception as e:
        logger.debug(f"ê¸°ì–µ ì¶”ì¶œ ê±´ë„ˆëœ€ ({agent_id}): {e}")


async def _call_agent(agent_id: str, text: str, conversation_id: str | None = None) -> dict:
    """ë‹¨ì¼ ì—ì´ì „íŠ¸ì—ê²Œ AI í˜¸ì¶œì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ìƒíƒœ ì´ë²¤íŠ¸ + í™œë™ ë¡œê·¸ + ë„êµ¬ ìë™í˜¸ì¶œ í¬í•¨)."""
    agent_name = _AGENT_NAMES.get(agent_id, _SPECIALIST_NAMES.get(agent_id, agent_id))
    await _broadcast_status(agent_id, "working", 0.1, f"{agent_name} ì‘ì—… ì¤€ë¹„ ì¤‘...")

    # í™œë™ ë¡œê·¸ â€” ëˆ„ê°€ ì¼í•˜ëŠ”ì§€ ê¸°ë¡
    log_entry = save_activity_log(agent_id, f"[{agent_name}] ì‘ì—… ì‹œì‘: {text[:40]}...")
    await wm.send_activity_log(log_entry)

    soul = _load_agent_prompt(agent_id)

    # â”€â”€ ì—ì´ì „íŠ¸ ê¸°ì–µ ì£¼ì… (ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì–µ â†’ system_prompt ì•ì— ì‚½ì…) â”€â”€
    mem = load_setting(f"memory_categorized_{agent_id}", {})
    if mem:
        mem_lines = []
        if mem.get("ceo_preferences"):
            mem_lines.append(f"- CEO ì·¨í–¥/ì„ í˜¸: {mem['ceo_preferences']}")
        if mem.get("decisions"):
            mem_lines.append(f"- ì£¼ìš” ê²°ì •: {mem['decisions']}")
        if mem.get("warnings"):
            mem_lines.append(f"- ì£¼ì˜ì‚¬í•­: {mem['warnings']}")
        if mem.get("context"):
            mem_lines.append(f"- ì¤‘ìš” ë§¥ë½: {mem['context']}")
        if mem_lines:
            memory_block = "[ì—ì´ì „íŠ¸ ê¸°ì–µ]\n" + "\n".join(mem_lines) + "\n\n"
            soul = memory_block + soul

    override = _get_model_override(agent_id)
    model = select_model(text, override=override)

    # â”€â”€ ë„êµ¬ ìë™í˜¸ì¶œ (Function Calling) â”€â”€
    # ì—ì´ì „íŠ¸ë³„ í—ˆìš© ë„êµ¬ ëª©ë¡ìœ¼ë¡œ ìŠ¤í‚¤ë§ˆë¥¼ ë¡œë“œí•˜ê³ , ë„êµ¬ ì‹¤í–‰ í•¨ìˆ˜ë¥¼ ì „ë‹¬
    tool_schemas = None
    tool_executor_fn = None
    tools_used: list[str] = []  # ì‚¬ìš©í•œ ë„êµ¬ ì´ë¦„ ì¶”ì 
    detail = _AGENTS_DETAIL.get(agent_id, {})
    allowed = detail.get("allowed_tools", [])
    if allowed:
        schemas = _load_tool_schemas(allowed_tools=allowed)
        if schemas.get("anthropic"):
            tool_schemas = schemas["anthropic"]  # ask_ai ë‚´ë¶€ì—ì„œ í”„ë¡œë°”ì´ë”ë³„ ë³€í™˜

            _MAX_TOOL_CALLS = int(detail.get("max_tool_calls", 5))  # agents.yamlì—ì„œ ì—ì´ì „íŠ¸ë³„ ì„¤ì •, ê¸°ë³¸ê°’ 5

            async def _tool_executor(tool_name: str, tool_input: dict):
                """ToolPoolì„ í†µí•´ ë„êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
                tools_used.append(tool_name)
                call_count = len(tools_used)
                # ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜ ê¸°ë°˜ ì§„í–‰ë¥  ê³„ì‚° (1íšŒ=20%, 2íšŒ=40%, ..., 5íšŒ=100%)
                tool_progress = 0.3 + min(call_count / _MAX_TOOL_CALLS, 1.0) * 0.35
                tool_progress_pct = int(tool_progress * 100)

                # ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜ í¬í•¨ agent_status ì´ë²¤íŠ¸ ë°œì†¡
                await wm.send_agent_status(
                    agent_id, "working", round(tool_progress, 2),
                    f"{tool_name} ì‹¤í–‰ ì¤‘...",
                    tool_calls=call_count, max_calls=_MAX_TOOL_CALLS, tool_name=tool_name,
                )

                # ë„êµ¬ ì‚¬ìš© ì‹¤ì‹œê°„ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ë„êµ¬ë¡œê·¸ íƒ­ì— í‘œì‹œ)
                tool_log = save_activity_log(
                    agent_id, f"ğŸ”§ [{agent_name}] {tool_name} í˜¸ì¶œ ({call_count}íšŒ)",
                    level="tool"
                )
                await wm.send_activity_log(tool_log)

                pool = _init_tool_pool()
                if pool:
                    try:
                        # pool.invoke()ë¡œ í˜¸ì¶œ â€” _caller_model ìë™ ì£¼ì… (ì—ì´ì „íŠ¸ ëª¨ë¸ ë”°ë¼ê°)
                        return await pool.invoke(tool_name, caller_id=agent_id, **tool_input)
                    except Exception as e:
                        if "ToolNotFoundError" in type(e).__name__ or tool_name in str(e):
                            return f"ë„êµ¬ '{tool_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        raise
                # ToolPool ë¯¸ì´ˆê¸°í™”
                return f"ë„êµ¬ '{tool_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            tool_executor_fn = _tool_executor

    # â”€â”€ ìµœê·¼ ëŒ€í™” ê¸°ë¡ ë¡œë“œ (ëŒ€í™” ë§¥ë½ ìœ ì§€) â”€â”€
    conv_history = _build_conv_history(conversation_id, text)

    await _broadcast_status(agent_id, "working", 0.3, "AI ì‘ë‹µ ìƒì„± ì¤‘...")
    result = await ask_ai(text, system_prompt=soul, model=model,
                          tools=tool_schemas, tool_executor=tool_executor_fn,
                          reasoning_effort=_get_agent_reasoning_effort(agent_id),
                          conversation_history=conv_history)
    await _broadcast_status(agent_id, "working", 0.7, "ì‘ë‹µ ì²˜ë¦¬ ì¤‘...")

    if "error" in result:
        # ì—ëŸ¬ ë°œìƒ ì‹œ (íƒ€ì„ì•„ì›ƒ ë“±) â€” ì—ëŸ¬ ë‚´ìš©ê³¼ í•¨ê»˜ ê¸°ë¡
        try:
            from db import save_agent_call
            save_agent_call(
                agent_id=agent_id, model=model or "error",
                provider="", cost_usd=0, input_tokens=0, output_tokens=0, time_seconds=0,
            )
        except Exception:
            pass
        await _broadcast_status(agent_id, "done", 1.0, "ì˜¤ë¥˜ ë°œìƒ")
        # ì—ëŸ¬ í™œë™ ë¡œê·¸
        log_err = save_activity_log(agent_id, f"[{agent_name}] âŒ ì˜¤ë¥˜: {result['error'][:80]}", "warning")
        await wm.send_activity_log(log_err)
        return {"agent_id": agent_id, "name": agent_name, "error": result["error"], "cost_usd": 0}

    # agent_calls í…Œì´ë¸”ì— AI í˜¸ì¶œ ê¸°ë¡ ì €ì¥ (ì„±ê³µ ì‹œ)
    try:
        from db import save_agent_call
        save_agent_call(
            agent_id=agent_id,
            model=result.get("model", model) if isinstance(result, dict) else model,
            provider=result.get("provider", "") if isinstance(result, dict) else "",
            cost_usd=result.get("cost_usd", 0) if isinstance(result, dict) else 0,
            input_tokens=result.get("input_tokens", 0) if isinstance(result, dict) else 0,
            output_tokens=result.get("output_tokens", 0) if isinstance(result, dict) else 0,
            time_seconds=result.get("time_seconds", 0) if isinstance(result, dict) else 0,
        )
    except Exception as e:
        _log(f"[AGENT_CALL] ê¸°ë¡ ì‹¤íŒ¨: {e}")

    await _broadcast_status(agent_id, "working", 0.9, "ì €ì¥ ì™„ë£Œ...")
    await _broadcast_status(agent_id, "done", 1.0, "ì™„ë£Œ")

    # ì™„ë£Œ ë¡œê·¸
    cost = result.get("cost_usd", 0)
    content = result.get("content", "")
    log_done = save_activity_log(agent_id, f"[{agent_name}] ì‘ì—… ì™„ë£Œ (${cost:.4f})")
    await wm.send_activity_log(log_done)

    # â”€â”€ ë¹„ìš© ì—…ë°ì´íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (í”„ë¡ íŠ¸ì—”ë“œ ìš°ì¸¡ ìƒë‹¨ ê¸ˆì•¡ ì‹¤ì‹œê°„ ë°˜ì˜) â”€â”€
    try:
        today_cost = get_today_cost()
    except Exception:
        today_cost = cost
    await wm.send_cost_update(today_cost)

    # â”€â”€ ê¸°ì–µ ìë™ ì¶”ì¶œ (ë¹„ë™ê¸° ë°±ê·¸ë¼ìš´ë“œ â€” ì‘ë‹µì—ì„œ ì¤‘ìš” ì •ë³´ ì €ì¥) â”€â”€
    if content and len(content) > 30:
        asyncio.create_task(_extract_and_save_memory(agent_id, text, content))

    # ì‚°ì¶œë¬¼ ì €ì¥ (ë…¸ì…˜ + ì•„ì¹´ì´ë¸Œ DB)
    if content and len(content) > 20:
        # ë…¸ì…˜ì— ì €ì¥ (ë¹„ë™ê¸°, ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ)
        asyncio.create_task(_save_to_notion(
            agent_id=agent_id,
            title=_extract_notion_title(content, f"[{agent_name}] ë³´ê³ ì„œ", user_query=text),
            content=content,
            db_target="secretary" if _AGENT_DIVISION.get(agent_id) == "secretary" else "output",
        ))
        # ì•„ì¹´ì´ë¸Œ DBì— ì €ì¥ (ì˜êµ¬ ë³´ê´€)
        division = _AGENT_DIVISION.get(agent_id, "secretary")
        now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
        # ì‚¬ìš©í•œ ë„êµ¬ ë©”íƒ€ë°ì´í„°ë¥¼ ì½˜í…ì¸  ë§¨ ì•„ë˜ì— ì¶”ê°€
        if tools_used:
            unique_tools = list(dict.fromkeys(tools_used))  # ì¤‘ë³µ ì œê±°, ìˆœì„œ ìœ ì§€
            content += f"\n\n---\nğŸ”§ **ì‚¬ìš©í•œ ë„êµ¬**: {', '.join(unique_tools)}"

        # ì œëª© ì¶”ì¶œ: AI ì‘ë‹µì—ì„œ ì˜ë¯¸ ìˆëŠ” ì œëª©ì„ ë½‘ì•„ì„œ íŒŒì¼ëª…ì— ì‚¬ìš©
        _title = _extract_notion_title(content, text[:40], user_query=text)
        _safe_title = re.sub(r'[\\/:*?"<>|\n\r]', '', _title)[:30].strip()
        archive_content = f"# [{agent_name}] {_safe_title}\n\n{content}"
        save_archive(
            division=division,
            filename=f"{agent_id}_{_safe_title}_{now_str}.md",
            content=archive_content,
            agent_id=agent_id,
        )

    return {
        "agent_id": agent_id,
        "name": agent_name,
        "content": content,
        "cost_usd": cost,
        "model": result.get("model", ""),
        "time_seconds": result.get("time_seconds", 0),
        "input_tokens": result.get("input_tokens", 0),
        "output_tokens": result.get("output_tokens", 0),
        "tools_used": tools_used,
    }


async def _chief_qa_review(report_content: str, team_leader_name: str) -> tuple[bool, str]:
    """ë¹„ì„œì‹¤ì¥ì´ íŒ€ì¥ ë³´ê³ ì„œë¥¼ QAí•©ë‹ˆë‹¤. (ìŠ¹ì¸/ë°˜ë ¤)

    ë¹„ìœ : ë¹„ì„œì‹¤ì¥ì´ íŒ€ì¥ ë³´ê³ ì„œë¥¼ ì½ê³  "ì´ê±° CEOí•œí…Œ ì˜¬ë ¤ë„ ë˜ë‚˜?" ê²€ìˆ˜.
    Returns: (passed: bool, reason: str)
    """
    if not report_content or len(report_content.strip()) < 50:
        return False, "ë³´ê³ ì„œ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (50ì ë¯¸ë§Œ)"

    qa_prompt = f"""ë‹¹ì‹ ì€ ë¹„ì„œì‹¤ì¥ì…ë‹ˆë‹¤. {team_leader_name}ì˜ ë³´ê³ ì„œë¥¼ ê²€ìˆ˜í•˜ì„¸ìš”.

## ë³´ê³ ì„œ
{report_content[:8000]}

## ê²€ìˆ˜ ê¸°ì¤€ (5í•­ëª©, ê° í†µê³¼/ë¯¸ë‹¬)
1. **ê²°ë¡  ì¡´ì¬**: ë§¤ìˆ˜/ë§¤ë„/ê´€ë§ ì‹œê·¸ë„ì´ ëª…í™•í•œê°€?
2. **ê·¼ê±° ì œì‹œ**: ì‹œê·¸ë„ì— ë°ì´í„° ê¸°ë°˜ ê·¼ê±°ê°€ ìˆëŠ”ê°€? (ìˆ«ì, ì§€í‘œ)
3. **ë¦¬ìŠ¤í¬ ì–¸ê¸‰**: ì†ì ˆê°€/ìµœëŒ€ì†ì‹¤/ì£¼ì˜ì‚¬í•­ì´ ìˆëŠ”ê°€?
4. **í˜•ì‹ ì¤€ìˆ˜**: [ì‹œê·¸ë„] í˜•ì‹ìœ¼ë¡œ ì¢…ëª©ë³„ ê²°ê³¼ê°€ ìˆëŠ”ê°€?
5. **ë…¼ë¦¬ ì¼ê´€ì„±**: ë¶„ì„ê³¼ ê²°ë¡ ì´ ëª¨ìˆœë˜ì§€ ì•ŠëŠ”ê°€?

## ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì²« ì¤„ì— ì´ í˜•ì‹ë§Œ)
íŒì •: PASS ë˜ëŠ” FAIL
ì‚¬ìœ : [1ì¤„ ìš”ì•½]"""

    try:
        soul = _load_agent_prompt("chief_of_staff")
        override = _get_model_override("chief_of_staff")
        model = select_model(qa_prompt, override=override)
        result = await ask_ai(
            qa_prompt,
            system_prompt=soul,
            model=model,
            reasoning_effort=_get_agent_reasoning_effort("chief_of_staff"),
        )
        qa_text = result.get("content", "")

        # íŒŒì‹±: "íŒì •: PASS" or "íŒì •: FAIL" (ì˜ì–´ í‚¤ì›Œë“œë¡œ ì˜¤íŒ ë°©ì§€)
        qa_upper = qa_text.upper()
        if "PASS" in qa_upper and "FAIL" not in qa_upper:
            passed = True
        elif "FAIL" in qa_upper:
            passed = False
        else:
            # í´ë°±: í•œêµ­ì–´ í‚¤ì›Œë“œ
            passed = "ìŠ¹ì¸" in qa_text and "ë°˜ë ¤" not in qa_text[:200]
        reason = ""
        for line in qa_text.split("\n"):
            if "ì‚¬ìœ " in line and ":" in line:
                reason = line.split(":", 1)[-1].strip()
                break
        if not reason:
            reason = "ìŠ¹ì¸" if passed else "ê¸°ì¤€ ë¯¸ë‹¬"

        return passed, reason
    except Exception as e:
        logger.warning("ë¹„ì„œì‹¤ì¥ QA ì‹¤íŒ¨ (ê¸°ë³¸ ìŠ¹ì¸): %s", e)
        return True, f"QA ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ê¸°ë³¸ ìŠ¹ì¸: {str(e)[:60]}"


async def _delegate_to_specialists(manager_id: str, text: str) -> list[dict]:
    """íŒ€ì¥ì´ ì†Œì† ì „ë¬¸ê°€ë“¤ì—ê²Œ ë³‘ë ¬ë¡œ ìœ„ì„í•©ë‹ˆë‹¤.

    asyncio.gatherë¡œ ì „ë¬¸ê°€ë“¤ì„ ë™ì‹œì— í˜¸ì¶œ â†’ ìƒíƒœ í‘œì‹œë“± ì „ë¶€ ê¹œë¹¡ì„.
    ìœ„ì„ ë°œìƒ ì‹œ delegation_logì— ìë™ ê¸°ë¡í•©ë‹ˆë‹¤.
    """
    specialists = _MANAGER_SPECIALISTS.get(manager_id, [])
    if not specialists:
        return []

    # ìœ„ì„ ë¡œê·¸ ìë™ ê¸°ë¡ + WebSocket + SSE broadcast
    try:
        from db import save_delegation_log
        import time as _time
        mgr_name = _AGENT_NAMES.get(manager_id, manager_id)
        # ìœ„ì„ ì œëª© ì¶”ì¶œ (CEO ì›ë¬¸ì—ì„œ ì§§ì€ ìš”ì•½)
        _deleg_title = _extract_notion_title(text, text[:30])[:40]
        for spec_id in specialists:
            spec_name = _SPECIALIST_NAMES.get(spec_id, spec_id)
            row_id = save_delegation_log(
                sender=mgr_name,
                receiver=spec_name,
                message=text[:500],
                log_type="delegation",
            )
            _log_data = {
                "id": row_id,
                "sender": mgr_name,
                "receiver": spec_name,
                "title": _deleg_title,
                "message": text[:300],
                "log_type": "delegation",
                "created_at": _time.time(),
            }
            await wm.send_delegation_log(_log_data)
    except Exception as e:
        logger.debug("ìœ„ì„ ë¡œê·¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨: %s", e)

    # ì „ë¬¸ê°€ ì „ì› ì¦‰ì‹œ ë³‘ë ¬ ì¶œë°œ (ì‹œì°¨ ì—†ìŒ)
    # Google í‚¤ 4ê°œ ë¡œí…Œì´ì…˜ + ì†ë„ ì œí•œê¸°ê°€ 429 ë°©ì§€ ë‹´ë‹¹
    tasks = [_call_agent(spec_id, _SPECIALIST_ROLE_PREFIX.get(spec_id, "") + text) for spec_id in specialists]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    processed = []
    for i, r in enumerate(results):
        spec_id = specialists[i]
        if isinstance(r, Exception):
            processed.append({"agent_id": spec_id, "name": _SPECIALIST_NAMES.get(spec_id, spec_id), "error": str(r)[:100], "cost_usd": 0})
        else:
            # ì „ë¬¸ê°€ ê²°ê³¼ ë³´ê³  ë¡œê·¸ ìë™ ê¸°ë¡ + WebSocket + SSE broadcast
            try:
                from db import save_delegation_log
                import time as _time
                spec_name = _SPECIALIST_NAMES.get(spec_id, spec_id)
                mgr_name = _AGENT_NAMES.get(manager_id, manager_id)
                content_preview = r.get("content", "")[:300] if isinstance(r, dict) else str(r)[:300]
                _tools = r.get("tools_used", []) if isinstance(r, dict) else []
                _tools_unique = list(dict.fromkeys(_tools))[:5]  # ì¤‘ë³µ ì œê±°, ìµœëŒ€ 5ê°œ
                _tools_str = ",".join(_tools_unique) if _tools_unique else ""
                # ë³´ê³  ì œëª© ì¶”ì¶œ (ì‘ë‹µ ë‚´ìš©ì—ì„œ ì§§ì€ ìš”ì•½)
                _rpt_title = _extract_notion_title(
                    r.get("content", "") if isinstance(r, dict) else str(r),
                    f"{spec_name} ë³´ê³ ", user_query=text
                )[:40]
                row_id = save_delegation_log(
                    sender=spec_name,
                    receiver=mgr_name,
                    message=content_preview,
                    log_type="report",
                    tools_used=_tools_str,
                )
                _log_data = {
                    "id": row_id,
                    "sender": spec_name,
                    "receiver": mgr_name,
                    "title": _rpt_title,
                    "message": content_preview,
                    "log_type": "report",
                    "tools_used": _tools_unique,
                    "created_at": _time.time(),
                }
                await wm.send_delegation_log(_log_data)
            except Exception as e:
                logger.debug("ë³´ê³  ë¡œê·¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹¤íŒ¨: %s", e)
            processed.append(r)
    return processed


async def _manager_with_delegation(manager_id: str, text: str, conversation_id: str | None = None) -> dict:
    """íŒ€ì¥ì´ ì „ë¬¸ê°€ì—ê²Œ ìœ„ì„ â†’ ê²°ê³¼ ì¢…í•©(ê²€ìˆ˜) â†’ ë³´ê³ ì„œ ì‘ì„±.

    íë¦„: íŒ€ì¥ ë¶„ì„ ì‹œì‘ â†’ ì „ë¬¸ê°€ ë³‘ë ¬ í˜¸ì¶œ â†’ íŒ€ì¥ì´ ê²°ê³¼ ì¢…í•© + ê²€ìˆ˜ â†’ ë³´ê³ ì„œ ë°˜í™˜
    ê²€ìˆ˜: íŒ€ì¥ì´ ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ ì½ê³  ì¢…í•©í•˜ëŠ” ê³¼ì • ìì²´ê°€ í’ˆì§ˆ ê²€ìˆ˜ ì—­í• ì„ í•©ë‹ˆë‹¤.
    """
    mgr_name = _AGENT_NAMES.get(manager_id, manager_id)
    specialists = _MANAGER_SPECIALISTS.get(manager_id, [])
    spec_names = [_SPECIALIST_NAMES.get(s, s) for s in specialists]

    # ì „ë¬¸ê°€ê°€ ì—†ìœ¼ë©´ íŒ€ì¥ì´ ì§ì ‘ ì²˜ë¦¬
    if not specialists:
        return await _call_agent(manager_id, text, conversation_id=conversation_id)

    # â”€â”€ íŒ€ì¥ ë…ì ë¶„ì„ í•¨ìˆ˜ (CEO ì•„ì´ë””ì–´: íŒ€ì¥ = 5ë²ˆì§¸ ë¶„ì„ê°€) â”€â”€
    # ì „ë¬¸ê°€ì™€ ë³‘ë ¬ë¡œ íŒ€ì¥ë„ ë…ìì ìœ¼ë¡œ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ ìˆ˜í–‰.
    # "ì¢…í•© ë•Œ ë„êµ¬ ì¨ë¼"(í”„ë¡¬í”„íŠ¸ ì˜ì¡´) â†’ "ë…ìë¶„ì„ ë”°ë¡œ ëŒë ¤"(êµ¬ì¡°ì  ê°•ì œ)
    async def _manager_self_analysis():
        """íŒ€ì¥ ë…ì ë¶„ì„ â€” ì „ë¬¸ê°€ì™€ ë™ì¼í•˜ê²Œ ë„êµ¬ ì‚¬ìš©. êµ¬ì¡°ì  ë„êµ¬ ì‚¬ìš© ë³´ì¥."""
        log_self = save_activity_log(manager_id,
            f"[{mgr_name}] ğŸ”§ ë…ì ë¶„ì„ ì‹œì‘ (5ë²ˆì§¸ ë¶„ì„ê°€)", "info")
        await wm.send_activity_log(log_self)
        self_prompt = (
            f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤. ì „ë¬¸ê°€ë“¤ê³¼ ë³„ê°œë¡œ ë…ìì  ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n"
            f"ë°˜ë“œì‹œ ë„êµ¬(API)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ì§ì ‘ ì¡°íšŒí•˜ê³  ë¶„ì„í•˜ì„¸ìš”.\n"
            f"ì „ë¬¸ê°€ ê²°ê³¼ëŠ” ë¬´ì‹œí•˜ì„¸ìš” â€” ë‹¹ì‹ ë§Œì˜ ë…ë¦½ì  ê´€ì ì„ ì œì‹œí•˜ì„¸ìš”.\n\n"
            f"## ë¶„ì„ ìš”ì²­\n{text}\n"
        )
        self_result = await _call_agent(manager_id, self_prompt, conversation_id=conversation_id)
        log_done = save_activity_log(manager_id,
            f"[{mgr_name}] âœ… ë…ì ë¶„ì„ ì™„ë£Œ", "info")
        await wm.send_activity_log(log_done)
        return self_result

    # íŒ€ì¥ ìƒíƒœ: ë…ì ë¶„ì„ + ì „ë¬¸ê°€ ìœ„ì„ ì‹œì‘
    await _broadcast_status(manager_id, "working", 0.1, "ë…ì ë¶„ì„ + ì „ë¬¸ê°€ ìœ„ì„ ì¤‘...")
    log_mgr = save_activity_log(manager_id,
        f"[{mgr_name}] ğŸ”§ ë…ì ë¶„ì„ + ì „ë¬¸ê°€ {len(specialists)}ëª… ìœ„ì„: {', '.join(spec_names)}")
    await wm.send_activity_log(log_mgr)

    # íŒ€ì¥ ë…ìë¶„ì„ + ì „ë¬¸ê°€ ë³‘ë ¬ ì‹¤í–‰ (5ë²ˆì§¸ ë¶„ì„ê°€ êµ¬ì¡°)
    _mgr_self_task = _manager_self_analysis()
    _spec_task = _delegate_to_specialists(manager_id, text)
    _parallel = await asyncio.gather(_mgr_self_task, _spec_task, return_exceptions=True)
    manager_self_result = _parallel[0] if not isinstance(_parallel[0], Exception) else {"error": str(_parallel[0])[:200]}
    spec_results = _parallel[1] if not isinstance(_parallel[1], Exception) else []
    if isinstance(_parallel[1], Exception):
        log_spec_err = save_activity_log(manager_id,
            f"[{mgr_name}] âš ï¸ ì „ë¬¸ê°€ ìœ„ì„ ì‹¤íŒ¨: {str(_parallel[1])[:100]}", "warning")
        await wm.send_activity_log(log_spec_err)

    # â”€â”€ Phase 8: CIO 7ë‹¨ê³„ â€” (1) ì„ íŒë‹¨+ë…ìë¶„ì„ ê¸°ë°€ë¬¸ì„œ ì €ì¥ â”€â”€
    _p8_div = _MANAGER_DIVISION.get(manager_id, "default")
    _p8_date = datetime.now(KST).strftime("%Y%m%d_%H%M")
    if isinstance(manager_self_result, dict) and "error" not in manager_self_result:
        try:
            save_archive(
                _p8_div,
                f"{mgr_name}_ë³´ê³ ì„œ1_ë…ìë¶„ì„_{_p8_date}.md",
                manager_self_result.get("content", ""),
                agent_id=manager_id,
            )
        except Exception as _ae_p8:
            logger.debug("Phase8 ë…ìë¶„ì„ ê¸°ë°€ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: %s", _ae_p8)

    # â”€â”€ Phase 8: CIO 7ë‹¨ê³„ â€” (2) ì „ë¬¸ê°€ ë³´ê³ ì„œ ê°ê° ê¸°ë°€ë¬¸ì„œ ì €ì¥ â”€â”€
    for _p8r in (spec_results or []):
        if isinstance(_p8r, dict) and "error" not in _p8r:
            _p8_spec_id = _p8r.get("agent_id", "unknown")
            _p8_spec_name = _SPECIALIST_NAMES.get(_p8_spec_id, _p8_spec_id)
            try:
                save_archive(
                    _p8_div,
                    f"{_p8_spec_name}_ë³´ê³ ì„œ1_{_p8_date}.md",
                    _p8r.get("content", ""),
                    agent_id=_p8_spec_id,
                )
            except Exception as _ae_p8s:
                logger.debug("Phase8 ì „ë¬¸ê°€ ê¸°ë°€ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: %s", _ae_p8s)

    # â”€â”€ í’ˆì§ˆê²€ìˆ˜ (Quality Gate) â”€â”€ ì „ë¬¸ê°€ ê²°ê³¼ë¥¼ íŒ€ì¥ì´ ì¢…í•©í•˜ê¸° ì „ì— ê²€ìˆ˜
    if app_state.quality_gate and _QUALITY_GATE_AVAILABLE and spec_results:
        await _broadcast_status(manager_id, "working", 0.45, "ì „ë¬¸ê°€ ê²°ê³¼ í’ˆì§ˆê²€ìˆ˜ ì¤‘...")

        # í’ˆì§ˆê²€ìˆ˜ìš© pseudo-chain êµ¬ì„±
        _qa_chain = {
            "chain_id": f"trading_{manager_id}_{int(time.time())}",
            "target_id": manager_id,
            "original_command": text[:500],
            "total_cost_usd": 0,
            "results": {"specialists": {}},
        }
        for r in spec_results:
            if "error" not in r:
                _qa_chain["results"]["specialists"][r.get("agent_id", "unknown")] = {
                    "content": r.get("content", ""),
                    "model": r.get("model", ""),
                    "cost_usd": r.get("cost_usd", 0),
                    "tools_used": r.get("tools_used", []),
                }

        # â˜… ë²„ê·¸#2 ìˆ˜ì •: ê²€ìˆ˜ ëŒ€ìƒ 0ëª…(ì „ë¬¸ê°€ ì „ì› ì—ëŸ¬) â†’ "í•©ê²©"ì´ ì•„ë‹ˆë¼ ì—ëŸ¬ ê²½ê³ !
        _qa_valid_count = len(_qa_chain["results"]["specialists"])
        _qa_error_count = len(spec_results) - _qa_valid_count

        if _qa_valid_count == 0:
            # ì „ë¬¸ê°€ ì „ì› ì—ëŸ¬ â€” QA ìŠ¤í‚µ, ì—ëŸ¬ ê²½ê³  ë¡œê·¸
            log_err = save_activity_log(manager_id,
                f"[{mgr_name}] âš ï¸ ì „ë¬¸ê°€ {_qa_error_count}ëª… ì „ì› ì—ëŸ¬ â€” í’ˆì§ˆê²€ìˆ˜ ë¶ˆê°€ (ìœ íš¨ ë³´ê³ ì„œ 0ê±´)", "warning")
            await wm.send_activity_log(log_err)
        else:
            _qa_note = f" (ì—ëŸ¬ {_qa_error_count}ëª… ì œì™¸)" if _qa_error_count else ""
            log_qa = save_activity_log(manager_id,
                f"[{mgr_name}] ì „ë¬¸ê°€ {_qa_valid_count}ëª… ê²°ê³¼ í’ˆì§ˆê²€ìˆ˜ ì‹œì‘{_qa_note}", "info")
            await wm.send_activity_log(log_qa)

        failed_specs = await _quality_review_specialists(_qa_chain) if _qa_valid_count > 0 else []

        if failed_specs:
            # ë¶ˆí•©ê²© ì „ë¬¸ê°€ í™œë™ë¡œê·¸
            for fs in failed_specs:
                _fs_name = _SPECIALIST_NAMES.get(fs["agent_id"], fs["agent_id"])
                log_reject = save_activity_log(manager_id,
                    f"[{mgr_name}] âŒ {_fs_name} ë³´ê³ ì„œ ë°˜ë ¤: {fs.get('reason', 'í’ˆì§ˆ ë¯¸ë‹¬')[:80]}", "warning")
                await wm.send_activity_log(log_reject)

            # ë°˜ë ¤ â†’ ì¬ì‘ì—… â†’ ì¬ê²€ìˆ˜
            await _handle_specialist_rework(_qa_chain, failed_specs)

            # ì¬ì‘ì—… ê²°ê³¼ë¥¼ spec_resultsì— ë°˜ì˜
            for r in spec_results:
                _aid = r.get("agent_id", "unknown")
                if _aid in _qa_chain["results"]["specialists"]:
                    updated = _qa_chain["results"]["specialists"][_aid]
                    r["content"] = updated.get("content", r.get("content", ""))
                    r["cost_usd"] = r.get("cost_usd", 0) + updated.get("cost_usd", 0)
                    if updated.get("rework_attempt"):
                        r["rework_attempt"] = updated["rework_attempt"]
                        log_rework = save_activity_log(_aid,
                            f"[{_SPECIALIST_NAMES.get(_aid, _aid)}] ì¬ì‘ì—… ì™„ë£Œ (ì‹œë„ {updated['rework_attempt']}íšŒ)")
                        await wm.send_activity_log(log_rework)
                    if updated.get("quality_warning"):
                        r["quality_warning"] = updated["quality_warning"]
                    if updated.get("tools_used"):
                        r["tools_used"] = r.get("tools_used", []) + updated["tools_used"]
        elif _qa_valid_count > 0:
            # ë¶ˆí•©ê²© 0ëª… + ê²€ìˆ˜ ëŒ€ìƒ 1ëª… ì´ìƒ â†’ ì§„ì§œ ì „ì› í•©ê²©
            log_pass = save_activity_log(manager_id,
                f"[{mgr_name}] âœ… ì „ë¬¸ê°€ {_qa_valid_count}ëª… í’ˆì§ˆê²€ìˆ˜ í•©ê²©", "info")
            await wm.send_activity_log(log_pass)

        # â˜… í’ˆì§ˆê²€ìˆ˜ ê²°ê³¼ë¥¼ ê¸°ë°€ë¬¸ì„œì— ì €ì¥
        _qa_reviews = _qa_chain.get("qa_reviews", [])
        if _qa_reviews:
            try:
                _now_str = datetime.now(KST).strftime("%Y-%m-%d %H:%M")
                _qa_lines = [f"# í’ˆì§ˆê²€ìˆ˜ ë³´ê³ ì„œ â€” {mgr_name} ({_now_str})\n"]
                _qa_lines.append(f"ê²€ìˆ˜ ëŒ€ìƒ: {_qa_valid_count}ëª… | ë¶ˆí•©ê²©: {len(failed_specs)}ëª…\n")
                for qr in _qa_reviews:
                    _qr_name = _SPECIALIST_NAMES.get(qr["agent_id"], qr["agent_id"])
                    _qr_pass = "âœ… í•©ê²©" if qr["passed"] else "âŒ ë¶ˆí•©ê²©"
                    _qa_lines.append(f"## {_qr_name} â€” {qr['weighted_average']:.1f}ì  {_qr_pass}\n")
                    _rd = qr.get("review_dict", {})
                    # ì²´í¬ë¦¬ìŠ¤íŠ¸
                    for ci in _rd.get("checklist", []):
                        _st = "âœ…" if ci["passed"] else "âŒ"
                        _rq = " [í•„ìˆ˜]" if ci.get("required") else ""
                        _fb = f" â€” {ci['feedback']}" if ci.get("feedback") and not ci["passed"] else ""
                        _qa_lines.append(f"- ğŸ“‹ {ci['id']} {ci.get('label','')}: {_st}{_rq}{_fb}")
                    # ì ìˆ˜
                    for si in _rd.get("scores", []):
                        _cr = " âš ï¸ì¹˜ëª…ì " if si.get("critical") and si["score"] == 1 else ""
                        _fb = f" â€” {si['feedback']}" if si.get("feedback") and si["score"] <= 3 else ""
                        _qa_lines.append(f"- ğŸ“Š {si['id']} {si.get('label','')}: {si['score']}ì /5 (ê°€ì¤‘ {si.get('weight',0)}%){_cr}{_fb}")
                    # ë°˜ë ¤ ì‚¬ìœ 
                    _rej = _rd.get("rejection_reasons", [])
                    if _rej:
                        _qa_lines.append(f"\n**ë°˜ë ¤ ì‚¬ìœ **: {' / '.join(_rej)}")
                    _qa_lines.append("")
                _qa_content = "\n".join(_qa_lines)
                _qa_filename = f"QA_{mgr_name}_{datetime.now(KST).strftime('%Y%m%d_%H%M')}.md"
                _division = _MANAGER_DIVISION.get(manager_id, "default")
                save_archive(
                    division=_division,
                    filename=_qa_filename,
                    content=_qa_content,
                    correlation_id=_qa_chain.get("chain_id", ""),
                    agent_id=manager_id,
                )
                _log(f"[QA] í’ˆì§ˆê²€ìˆ˜ ë³´ê³ ì„œ ê¸°ë°€ë¬¸ì„œ ì €ì¥: {_qa_filename}")
            except Exception as e:
                _log(f"[QA] ê¸°ë°€ë¬¸ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ì „ë¬¸ê°€ ê²°ê³¼ ì·¨í•©
    spec_parts = []
    spec_cost = 0.0
    spec_time = 0.0
    for r in spec_results:
        name = r.get("name", r.get("agent_id", "?"))
        if "error" in r:
            spec_parts.append(f"[{name}] ì˜¤ë¥˜: {r['error'][:80]}")
        else:
            quality_note = ""
            if r.get("rework_attempt"):
                quality_note = f"\nâš ï¸ ì¬ì‘ì—… {r['rework_attempt']}íšŒ í›„ ê²°ê³¼"
            if r.get("quality_warning"):
                quality_note = f"\nâš ï¸ í’ˆì§ˆ ê²½ê³ : {r['quality_warning'][:60]}"
            spec_parts.append(f"[{name}]{quality_note}\n{r.get('content', 'ì‘ë‹µ ì—†ìŒ')}")
            spec_cost += r.get("cost_usd", 0)
            spec_time = max(spec_time, r.get("time_seconds", 0))

    # íŒ€ì¥ ë…ìë¶„ì„ ê²°ê³¼ ì·¨í•©
    manager_self_content = ""
    mgr_self_tools: list[str] = []
    if isinstance(manager_self_result, dict) and "error" not in manager_self_result:
        manager_self_content = manager_self_result.get("content", "")
        mgr_self_tools = manager_self_result.get("tools_used", [])
        spec_cost += manager_self_result.get("cost_usd", 0)
        spec_time = max(spec_time, manager_self_result.get("time_seconds", 0))

    # ì „ë¬¸ê°€ ì„±ê³µ/ì‹¤íŒ¨ ì§‘ê³„
    _spec_ok_count = len([r for r in spec_results if "error" not in r])
    _spec_err_count = len(spec_results) - _spec_ok_count

    # íŒ€ì¥ ì¢…í•© í”„ë¡¬í”„íŠ¸ â€” ë…ìë¶„ì„ + ì „ë¬¸ê°€ ê²°ê³¼ ì·¨í•©ë§Œ (ë„êµ¬ ë¶ˆí•„ìš”)
    # CEO ì•„ì´ë””ì–´: íŒ€ì¥ ë…ìë¶„ì„ì—ì„œ ì´ë¯¸ ë„êµ¬ ì‚¬ìš© ì™„ë£Œ â†’ ì¢…í•©ì€ ë‹¨ìˆœ ì·¨í•©
    synthesis_prompt = (
        f"ë‹¹ì‹ ì€ {mgr_name}ì…ë‹ˆë‹¤.\n"
        f"ì•„ë˜ ë¶„ì„ ê²°ê³¼(ë‹¹ì‹ ì˜ ë…ì ë¶„ì„ + ì „ë¬¸ê°€)ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
        f"ë„êµ¬ë¥¼ ë‹¤ì‹œ ì‚¬ìš©í•  í•„ìš” ì—†ìŠµë‹ˆë‹¤ â€” ê²°ê³¼ë¥¼ ì·¨í•©ë§Œ í•˜ì„¸ìš”.\n\n"
        f"## CEO ì›ë³¸ ëª…ë ¹\n{text}\n\n"
        f"## íŒ€ì¥ ë…ì ë¶„ì„\n{manager_self_content or '(ë¶„ì„ ì‹¤íŒ¨)'}\n\n"
        f"## ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼\n" + "\n\n".join(spec_parts)
    )

    soul = _load_agent_prompt(manager_id)
    override = _get_model_override(manager_id)
    model = select_model(synthesis_prompt, override=override)

    await _broadcast_status(manager_id, "working", 0.7, "ë…ìë¶„ì„ + ì „ë¬¸ê°€ ê²°ê³¼ ì¢…í•© ì¤‘...")
    synthesis = await ask_ai(synthesis_prompt, system_prompt=soul, model=model,
                             tools=None, tool_executor=None,
                             reasoning_effort=_get_agent_reasoning_effort(manager_id))

    await _broadcast_status(manager_id, "done", 1.0, "ë³´ê³  ì™„ë£Œ")

    if "error" in synthesis:
        # ì¢…í•© ì‹¤íŒ¨ ì‹œ ë…ìë¶„ì„ + ì „ë¬¸ê°€ ê²°ê³¼ ë°˜í™˜
        _spec_ok = len([r for r in spec_results if "error" not in r])
        content = f"**{mgr_name} ë…ì ë¶„ì„**\n\n{manager_self_content or '(ë¶„ì„ ì‹¤íŒ¨)'}\n\n---\n\n**ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼**\n\n" + "\n\n---\n\n".join(spec_parts)
        _all_spec_tools = [t for r in spec_results if isinstance(r, dict) and "error" not in r for t in r.get("tools_used", [])]
        return {"agent_id": manager_id, "name": mgr_name, "content": content, "cost_usd": spec_cost, "specialists_used": _spec_ok, "tools_used": mgr_self_tools + _all_spec_tools}

    total_cost = spec_cost + synthesis.get("cost_usd", 0)
    specialists_used = len([r for r in spec_results if "error" not in r])
    synth_content = synthesis.get("content", "")

    # ì „ë¬¸ê°€ ê°œë³„ ì‚°ì¶œë¬¼ë„ ë…¸ì…˜ì— ì €ì¥ (spawnëœ ì „ë¬¸ê°€ ê²°ê³¼ ì „ë¶€ ê¸°ë¡)
    for r in spec_results:
        if "error" not in r and r.get("content") and len(r["content"]) > 20:
            _sid = r.get("agent_id", "unknown")
            _sname = r.get("name", _sid)
            asyncio.create_task(_save_to_notion(
                agent_id=_sid,
                title=_extract_notion_title(r["content"], f"[{_sname}] ë¶„ì„ë³´ê³ ", user_query=text),
                content=r["content"],
                report_type="ì „ë¬¸ê°€ë³´ê³ ì„œ",
                db_target="output",
            ))

    # ì¢…í•© ë³´ê³ ì„œ ì €ì¥ (ë…¸ì…˜ + ì•„ì¹´ì´ë¸Œ DB)
    if synth_content and len(synth_content) > 20:
        asyncio.create_task(_save_to_notion(
            agent_id=manager_id,
            title=_extract_notion_title(synth_content, f"[{mgr_name}] ì¢…í•©ë³´ê³ ", user_query=text),
            content=synth_content,
            report_type="ì¢…í•©ë³´ê³ ì„œ",
            db_target="secretary" if _AGENT_DIVISION.get(manager_id) == "secretary" else "output",
        ))
        # ì•„ì¹´ì´ë¸Œ DBì— ì €ì¥ (ì œëª© ì¶”ì¶œí•˜ì—¬ íŒŒì¼ëª…ì— í¬í•¨)
        division = _AGENT_DIVISION.get(manager_id, "secretary")
        now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
        _synth_title = _extract_notion_title(synth_content, text[:40], user_query=text)
        _safe_synth = re.sub(r'[\\/:*?"<>|\n\r]', '', _synth_title)[:30].strip()
        archive_content = f"# [{mgr_name}] ì¢…í•©ë³´ê³ : {_safe_synth}\n\n{synth_content}"
        save_archive(
            division=division,
            filename=f"{manager_id}_{_safe_synth}_{now_str}.md",
            content=archive_content,
            agent_id=manager_id,
        )

    # íŒ€ì¥ ë…ìë¶„ì„ ë„êµ¬ ì‚¬ìš© ê¸°ë¡ ë¡œê·¸
    if mgr_self_tools:
        _unique_self = list(dict.fromkeys(mgr_self_tools))
        log_tools = save_activity_log(manager_id,
            f"[{mgr_name}] ğŸ”§ ë…ì ë¶„ì„ ë„êµ¬ {len(mgr_self_tools)}ê±´ ì‚¬ìš© (ê³ ìœ  {len(_unique_self)}ê°œ): {', '.join(_unique_self[:5])}", "tool")
        await wm.send_activity_log(log_tools)

    return {
        "agent_id": manager_id,
        "name": mgr_name,
        "content": synth_content,
        "cost_usd": total_cost,
        "model": synthesis.get("model", ""),
        "time_seconds": round(spec_time + synthesis.get("time_seconds", 0), 2),
        "specialists_used": specialists_used,
        "tools_used": mgr_self_tools,
    }


def _determine_routing_level(text: str) -> tuple[int, str | None]:
    """ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ Level 1~4ì™€ ëŒ€ìƒ íŒ€ì¥ ID ë°˜í™˜.

    Returns: (level, manager_id_or_None)
    - Level 1: ê°„ë‹¨í•œ ì¸ì‚¬/ë‹¨ìˆœ ì§ˆë¬¸ â†’ ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬ (íŒ€ì¥ í˜¸ì¶œ ì—†ìŒ)
    - Level 2: íŠ¹ì • ë¶€ì„œ ì „ë¬¸ ì§ˆë¬¸ â†’ íŒ€ì¥ 1ëª…ë§Œ í˜¸ì¶œ
    - Level 3: íŠ¹ì • ë¶€ì„œ ì‹¬ì¸µ ë¶„ì„ â†’ íŒ€ì¥ 1ëª… + spawn_agent ììœ¨ ì „ë¬¸ê°€ ì„ íƒ
    - Level 4: ë³µí•©/ì „ì‚¬ ì§ˆë¬¸ â†’ ì „ì› ë³‘ë ¬ í˜¸ì¶œ (ê¸°ì¡´ ë¸Œë¡œë“œìºìŠ¤íŠ¸)
    """
    t = text.lower()

    # Level 1: ê°„ë‹¨í•œ ìš”ì²­ â€” ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬
    SIMPLE_KEYWORDS = ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "ê³ ë§ˆì›Œ", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì¼ì •", "ë­ì•¼",
                       "ì–¸ì œì•¼", "ë­”ê°€ìš”", "ì•Œë ¤ì¤˜", "ì°¾ì•„ì¤˜", "í™•ì¸í•´ì¤˜"]
    if len(text) < 50 and any(k in t for k in SIMPLE_KEYWORDS):
        return (1, None)

    # Level 2/3: íŠ¹ì • ë¶€ì„œ ì „ë¬¸ ì§ˆë¬¸
    MANAGER_KEYWORDS = {
        "cto_manager": ["ê¸°ìˆ ", "ê°œë°œ", "ì½”ë“œ", "api", "ì„œë²„", "ì•±", "ì›¹", "í”„ë¡ íŠ¸", "ë°±ì—”ë“œ", "ì¸í”„ë¼", "ai ëª¨ë¸", "ë°ì´í„°ë² ì´ìŠ¤"],
        "cso_manager": ["ì‚¬ì—…", "ì‹œì¥", "ì¬ë¬´", "ì „ëµ", "ë¹„ì¦ˆë‹ˆìŠ¤", "ê³„íš", "ìˆ˜ìµ", "ë§¤ì¶œ", "íˆ¬ì ê³„íš"],
        "clo_manager": ["ë²•", "ê³„ì•½", "ì €ì‘ê¶Œ", "íŠ¹í—ˆ", "ì•½ê´€", "ë²•ë¥ ", "ip"],
        "cmo_manager": ["ë§ˆì¼€íŒ…", "ê³ ê°", "ì½˜í…ì¸ ", "sns", "ê´‘ê³ ", "ì»¤ë®¤ë‹ˆí‹°", "ë¸Œëœë”©"],
        "cio_manager": ["íˆ¬ì", "ì£¼ì‹", "ì½”ìŠ¤í”¼", "ì‹œí™©", "ì¢…ëª©", "ë¦¬ìŠ¤í¬", "í¬íŠ¸í´ë¦¬ì˜¤", "etf", "ì±„ê¶Œ"],
        "cpo_manager": ["ê¸°ë¡", "ì¶œíŒ", "ë¸”ë¡œê·¸", "ì—°ëŒ€ê¸°", "íšŒê³ ", "í¸ì§‘", "ì•„ì¹´ì´ë¸Œ"],
    }

    matched_manager = None
    for mgr_id, keywords in MANAGER_KEYWORDS.items():
        if any(k in t for k in keywords):
            matched_manager = mgr_id
            break

    if matched_manager:
        DEEP_KEYWORDS = ["ë¶„ì„", "ë³´ê³ ì„œ", "ì „ëµ", "ê³„íšì„œ", "ê²€í† ", "í‰ê°€", "ë¹„êµ", "ì˜ˆì¸¡", "ì „ë§"]
        if any(k in t for k in DEEP_KEYWORDS):
            return (3, matched_manager)
        return (2, matched_manager)

    return (4, None)


async def _manager_with_delegation_autonomous(manager_id: str, text: str, conversation_id: str | None = None) -> dict:
    """íŒ€ì¥ì´ spawn_agent ë„êµ¬ë¡œ í•„ìš”í•œ ì „ë¬¸ê°€ë§Œ ììœ¨ ì„ íƒí•˜ì—¬ í˜¸ì¶œ (Level 3ìš©)."""
    agent_cfg = next((a for a in AGENTS if a.get("agent_id") == manager_id), None)
    if not agent_cfg:
        return {"content": f"ì—ì´ì „íŠ¸ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {manager_id}", "error": True}

    soul = _load_agent_prompt(manager_id)
    specialists_pool = _MANAGER_SPECIALISTS.get(manager_id, [])

    # spawn_agent ë„êµ¬ ìŠ¤í‚¤ë§ˆ
    spawn_tool = {
        "name": "spawn_agent",
        "description": (
            f"ì†Œì† ì „ë¬¸ê°€ë¥¼ í˜¸ì¶œí•˜ì—¬ íŠ¹ì • ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. "
            f"ì‚¬ìš© ê°€ëŠ¥í•œ ì „ë¬¸ê°€ ID: {', '.join(specialists_pool)}"
        ),
        "input_schema": {
            "type": "object",
            "properties": {
                "agent_id": {
                    "type": "string",
                    "description": "í˜¸ì¶œí•  ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ ID",
                    "enum": specialists_pool,
                },
                "task": {
                    "type": "string",
                    "description": "ì „ë¬¸ê°€ì—ê²Œ ì§€ì‹œí•  êµ¬ì²´ì ì¸ ì‘ì—… ë‚´ìš©",
                }
            },
            "required": ["agent_id", "task"]
        }
    }

    specialist_results: dict[str, str] = {}

    async def _spawn_executor(tool_name: str, tool_input: dict) -> str:
        if tool_name == "spawn_agent":
            sid = tool_input.get("agent_id", "")
            task = tool_input.get("task", "")
            if sid in specialists_pool:
                logger.info("spawn_agent: %s â†’ %s", manager_id, sid)
                await _broadcast_status(manager_id, "working", 0.5, f"ì „ë¬¸ê°€ {_SPECIALIST_NAMES.get(sid, sid)} í˜¸ì¶œ ì¤‘...")
                result = await _call_agent(sid, task, conversation_id=conversation_id)
                content = result.get("content", "")
                specialist_results[sid] = content
                return content
        return "ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ì…ë‹ˆë‹¤."

    mgr_name = _AGENT_NAMES.get(manager_id, manager_id)
    await _broadcast_status(manager_id, "working", 0.2, f"{mgr_name} ë¶„ì„ ì¤‘ (ììœ¨ ì „ë¬¸ê°€ ì„ íƒ)...")

    override = _get_model_override(manager_id)
    model = select_model(text, override=override)

    result = await ask_ai(
        text,
        system_prompt=soul,
        model=model,
        tools=[spawn_tool],
        tool_executor=_spawn_executor,
    )

    await _broadcast_status(manager_id, "done", 1.0, "ë³´ê³  ì™„ë£Œ")

    return {
        "content": result.get("content", ""),
        "specialist_results": specialist_results,
        "manager_id": manager_id,
        "cost_usd": result.get("cost_usd", 0),
        "time_seconds": result.get("time_seconds", 0),
    }


async def _chief_finalize(original_text: str, manager_results: dict) -> dict:
    """Level 2/3 ì™„ë£Œ í›„ ë¹„ì„œì‹¤ì¥ì´ ìµœì¢… ë³´ê³ ì„œ 1ê°œ ì‘ì„±."""
    chief_cfg = next((a for a in AGENTS if a.get("agent_id") == "chief_of_staff"), None)
    if not chief_cfg:
        # fallback: íŒ€ì¥ ê²°ê³¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
        combined = "\n\n".join(r.get("content", "") for r in manager_results.values())
        return {"content": combined}

    results_text = "\n\n".join(
        f"[{mgr_id} ë³´ê³ ]\n{res.get('content', '')}"
        for mgr_id, res in manager_results.items()
    )

    synthesis_prompt = (
        f"CEO ì§ˆë¬¸: {original_text}\n\n"
        f"íŒ€ì¥ ë³´ê³  ë‚´ìš©:\n{results_text}\n\n"
        "ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ CEOì—ê²Œ ë“œë¦´ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”. "
        "í•µì‹¬ ê²°ë¡ ì„ ë¨¼ì €, ì„¸ë¶€ ë‚´ìš©ì„ ë’¤ì— ì •ë¦¬í•˜ì„¸ìš”."
    )

    soul = _load_agent_prompt("chief_of_staff")
    override = _get_model_override("chief_of_staff")
    model = select_model(synthesis_prompt, override=override)

    result = await ask_ai(
        synthesis_prompt,
        system_prompt=soul,
        model=model,
    )

    return {"content": result.get("content", ""), "routing_level": "finalized", "cost_usd": result.get("cost_usd", 0)}


async def _broadcast_to_managers_all(text: str, task_id: str, conversation_id: str | None = None) -> dict:
    """Level 4: ê¸°ì¡´ ë°©ì‹ â€” í™œì„± íŒ€ì¥ ë³‘ë ¬ í˜¸ì¶œ (ë¸Œë¡œë“œìºìŠ¤íŠ¸)."""
    # dormant ì œì™¸í•œ í™œì„± íŒ€ì¥ë§Œ
    managers = [m for m in ["cso_manager", "clo_manager", "cmo_manager", "cio_manager", "cpo_manager"]
                if m not in _DORMANT_MANAGERS]
    staff_specialists = []  # ë¹„ì„œì‹¤ ë³´ì¢Œê´€ë„ ë™ë©´ (ì „ë¬¸ê°€ ì „ì› ë™ë©´ ì²´ì œ)

    # ë¹„ì„œì‹¤ì¥ ìƒíƒœ: ì „ë‹¬ ì¤‘
    await _broadcast_status("chief_of_staff", "working", 0.1, f"{len(managers)}ê°œ ë¶€ì„œ íŒ€ì¥ì—ê²Œ ëª…ë ¹ í•˜ë‹¬ ì¤‘...")

    # í™œë™ ë¡œê·¸
    log_entry = save_activity_log("chief_of_staff", f"[ë¹„ì„œì‹¤ì¥] {len(managers)}ê°œ íŒ€ì¥ì—ê²Œ ëª…ë ¹ ì „ë‹¬: {text[:40]}...")
    await wm.send_activity_log(log_entry)

    # â”€â”€ 1ë‹¨ê³„: 6ê°œ íŒ€ì¥ + ë¹„ì„œì‹¤ ë³´ì¢Œê´€ 3ëª… ë™ì‹œ í˜¸ì¶œ â”€â”€
    mgr_tasks = [_manager_with_delegation(mgr_id, text, conversation_id=conversation_id) for mgr_id in managers]
    staff_tasks = [_call_agent(spec_id, text, conversation_id=conversation_id) for spec_id in staff_specialists]
    all_results = await asyncio.gather(*(mgr_tasks + staff_tasks), return_exceptions=True)

    mgr_results = all_results[:6]
    staff_results = all_results[6:]

    # â”€â”€ 2ë‹¨ê³„: íŒ€ì¥ ê²°ê³¼ ì •ë¦¬ (ê¸°ë°€ë¬¸ì„œì—ëŠ” ì´ë¯¸ _manager_with_delegationì—ì„œ ì €ì¥ë¨) â”€â”€
    mgr_summaries = []  # ë¹„ì„œì‹¤ì¥ì—ê²Œ ì „ë‹¬í•  ìš”ì•½
    total_cost = 0.0
    total_time = 0.0
    success_count = 0
    total_specialists = 0

    for i, result in enumerate(mgr_results):
        mgr_id = managers[i]
        mgr_name = _AGENT_NAMES.get(mgr_id, mgr_id)

        if isinstance(result, Exception):
            mgr_summaries.append(f"[{mgr_name}] ì˜¤ë¥˜: {str(result)[:100]}")
        elif "error" in result:
            mgr_summaries.append(f"[{mgr_name}] ì˜¤ë¥˜: {result['error'][:200]}")
        else:
            specs = result.get("specialists_used", 0)
            total_specialists += specs
            mgr_summaries.append(f"[{mgr_name}] (ì „ë¬¸ê°€ {specs}ëª…)\n{result.get('content', 'ì‘ë‹µ ì—†ìŒ')}")
            total_cost += result.get("cost_usd", 0)
            total_time = max(total_time, result.get("time_seconds", 0))
            success_count += 1

    # ë³´ì¢Œê´€ ê²°ê³¼ ì •ë¦¬
    staff_summaries = []
    for i, result in enumerate(staff_results):
        spec_id = staff_specialists[i]
        spec_name = _SPECIALIST_NAMES.get(spec_id, spec_id)

        if isinstance(result, Exception):
            staff_summaries.append(f"[{spec_name}] ì˜¤ë¥˜: {str(result)[:100]}")
        elif "error" in result:
            staff_summaries.append(f"[{spec_name}] ì˜¤ë¥˜: {result['error'][:200]}")
        else:
            staff_summaries.append(f"[{spec_name}]\n{result.get('content', 'ì‘ë‹µ ì—†ìŒ')}")
            total_cost += result.get("cost_usd", 0)
            total_time = max(total_time, result.get("time_seconds", 0))

    # â”€â”€ 3ë‹¨ê³„: ë¹„ì„œì‹¤ì¥ì´ AIë¡œ ì¢…í•© ë³´ê³ ì„œ ì‘ì„± â”€â”€
    await _broadcast_status("chief_of_staff", "working", 0.8, "ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì¤‘...")

    synthesis_input = (
        f"CEO ì›ë³¸ ëª…ë ¹: {text}\n\n"
        f"## 6ê°œ ë¶€ì„œ íŒ€ì¥ ë³´ê³ ì„œ\n\n"
        + "\n\n---\n\n".join(mgr_summaries)
        + f"\n\n## ë¹„ì„œì‹¤ ë³´ì¢Œê´€ ë³´ê³ \n\n"
        + "\n\n".join(staff_summaries)
    )

    synthesis_system = (
        "ë‹¹ì‹ ì€ ë¹„ì„œì‹¤ì¥ì…ë‹ˆë‹¤. 6ê°œ ë¶€ì„œ íŒ€ì¥ê³¼ ë¹„ì„œì‹¤ ë³´ì¢Œê´€ 3ëª…ì˜ ë³´ê³ ë¥¼ ê²€í† í•˜ê³ , "
        "CEOì—ê²Œ ì¢…í•© ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "## ë°˜ë“œì‹œ ì•„ë˜ êµ¬ì¡°ë¥¼ ë”°ë¥¼ ê²ƒ\n\n"
        "### í•µì‹¬ ìš”ì•½\n"
        "(ì „ì²´ ìƒí™©ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)\n\n"
        "### ë¶€ì„œë³„ í•œì¤„ ìš”ì•½\n"
        "| ë¶€ì„œ | í•µì‹¬ ë‚´ìš© | ìƒíƒœ |\n"
        "|------|----------|------|\n"
        "| CTO (ê¸°ìˆ ê°œë°œ) | ... | ì •ìƒ/ì£¼ì˜/ìœ„í—˜ |\n"
        "(6ê°œ ë¶€ì„œ ì „ë¶€)\n\n"
        "### CEO ê²°ì¬/ê²°ì • í•„ìš” ì‚¬í•­\n"
        "(ê° íŒ€ì¥ ë³´ê³ ì„œì—ì„œ CEOê°€ ê²°ì •í•´ì•¼ í•  ê²ƒë§Œ ì¶”ì¶œ. ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)\n"
        "- [ ] ë¶€ì„œëª…: ê²°ì • ì‚¬í•­ â€” ë°°ê²½ ì„¤ëª…\n"
        "(ê²°ì¬í•  ê²ƒì´ ì—†ìœ¼ë©´ 'í˜„ì¬ ê²°ì¬ ëŒ€ê¸° ì‚¬í•­ ì—†ìŒ')\n\n"
        "### íŠ¹ì´ì‚¬í•­ / ë¦¬ìŠ¤í¬\n"
        "(ê° ë³´ê³ ì„œì—ì„œ ë¦¬ìŠ¤í¬ ìš”ì†Œë§Œ ì¶”ì¶œ. ì—†ìœ¼ë©´ 'íŠ¹ì´ì‚¬í•­ ì—†ìŒ')\n\n"
        "### ë¹„ì„œì‹¤ ë³´ì¢Œê´€ ë³´ê³ \n"
        "- ê¸°ë¡ ë³´ì¢Œê´€: (1ì¤„ ìš”ì•½)\n"
        "- ì¼ì • ë³´ì¢Œê´€: (1ì¤„ ìš”ì•½)\n"
        "- ì†Œí†µ ë³´ì¢Œê´€: (1ì¤„ ìš”ì•½)\n\n"
        "## ê·œì¹™\n"
        "- í•œêµ­ì–´ë¡œ ì‘ì„±\n"
        "- ê°„ê²°í•˜ê²Œ. CEOê°€ 30ì´ˆ ì•ˆì— í•µì‹¬ì„ íŒŒì•…í•  ìˆ˜ ìˆê²Œ\n"
        "- ì¤‘ìš”í•œ ìˆ«ì/ë°ì´í„°ëŠ” ë°˜ë“œì‹œ í¬í•¨\n"
        "- íŒ€ì¥ ë³´ê³ ì„œë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì§€ ë§ê³ , í•µì‹¬ë§Œ ì¶”ì¶œí•˜ì—¬ ì¬êµ¬ì„±\n"
    )

    soul = _load_agent_prompt("chief_of_staff")
    override = _get_model_override("chief_of_staff")
    model = select_model(synthesis_input, override=override)

    chief_synthesis = await ask_ai(
        synthesis_input,
        system_prompt=synthesis_system + "\n\n" + soul,
        model=model,
    )

    await _broadcast_status("chief_of_staff", "done", 1.0, "ì¢…í•© ë³´ê³  ì™„ë£Œ")

    # ì¢…í•© ë³´ê³ ì„œ ë¹„ìš© ì¶”ê°€
    if "error" not in chief_synthesis:
        total_cost += chief_synthesis.get("cost_usd", 0)

    # â”€â”€ 4ë‹¨ê³„: ìµœì¢… ì¶œë ¥ = ë¹„ì„œì‹¤ì¥ ì¢…í•© ë³´ê³ ì„œë§Œ â”€â”€
    if "error" in chief_synthesis:
        # ì¢…í•© ì‹¤íŒ¨ ì‹œ íŒ€ì¥ ìš”ì•½ë§Œ ê°„ë‹¨íˆ í‘œì‹œ
        chief_content = "âš ï¸ ë¹„ì„œì‹¤ì¥ ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì‹¤íŒ¨\n\n" + "\n\n---\n\n".join(
            f"**{_AGENT_NAMES.get(managers[i], managers[i])}**: "
            + (mgr_results[i].get("content", "")[:100] + "..." if not isinstance(mgr_results[i], Exception) else "ì˜¤ë¥˜")
            for i in range(6)
        )
    else:
        chief_content = chief_synthesis.get("content", "")

    # ë§¨ ì•„ë˜ ì•ˆë‚´ ì¶”ê°€
    final_content = (
        f"ğŸ“‹ **ë¹„ì„œì‹¤ì¥ ì¢…í•© ë³´ê³ ** "
        f"(6ê°œ íŒ€ì¥ + ì „ë¬¸ê°€ {total_specialists}ëª… + ë³´ì¢Œê´€ 3ëª… ë™ì›)\n\n"
        f"{chief_content}\n\n"
        f"---\n\n"
        f"ğŸ“‚ **ìƒì„¸ ë³´ê³ ì„œ {success_count}ê±´ì´ ê¸°ë°€ë¬¸ì„œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.** "
        f"ê¸°ë°€ë¬¸ì„œ íƒ­ì—ì„œ ë¶€ì„œë³„ í•„í„°ë¡œ ê° íŒ€ì¥ì˜ ì „ì²´ ë³´ê³ ì„œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    # ë¹„ì„œì‹¤ì¥ ì¢…í•© ë³´ê³ ì„œë„ ì•„ì¹´ì´ë¸Œì— ì €ì¥
    now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
    save_archive(
        division="secretary",
        filename=f"chief_of_staff_broadcast_{now_str}.md",
        content=f"# [ë¹„ì„œì‹¤ì¥] ì¢…í•© ë³´ê³ : {text[:50]}\n\n{chief_content}",
        agent_id="chief_of_staff",
    )

    # DB ì—…ë°ì´íŠ¸
    update_task(task_id, status="completed",
                result_summary=f"ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì™„ë£Œ ({success_count}/6 ë¶€ì„œ, ì „ë¬¸ê°€ {total_specialists}ëª…, ë³´ì¢Œê´€ 3ëª…)",
                result_data=final_content,
                success=1,
                cost_usd=total_cost,
                time_seconds=round(total_time, 2),
                agent_id="chief_of_staff")

    return {
        "content": final_content,
        "agent_id": "chief_of_staff",
        "handled_by": "ë¹„ì„œì‹¤ì¥ â†’ 6ê°œ íŒ€ì¥ + ë³´ì¢Œê´€ 3ëª…",
        "delegation": "ë¹„ì„œì‹¤ì¥ â†’ íŒ€ì¥ â†’ ì „ë¬¸ê°€",
        "total_cost_usd": round(total_cost, 6),
        "time_seconds": round(total_time, 2),
        "model": "multi-agent",
        "routing_method": "ë¸Œë¡œë“œìºìŠ¤íŠ¸",
    }


# â”€â”€ í† ë¡  ì‹œìŠ¤í…œ (ì„ì› íšŒì˜ ë°©ì‹ ë‹¤ë¼ìš´ë“œ í† ë¡ ) â”€â”€

DEBATE_ROTATION = [
    ["cio_manager", "cto_manager", "cso_manager", "cmo_manager", "clo_manager", "cpo_manager"],
    ["cto_manager", "cso_manager", "cio_manager", "clo_manager", "cmo_manager", "cpo_manager"],
    ["cso_manager", "cmo_manager", "cto_manager", "cio_manager", "cpo_manager", "clo_manager"],
]

# íŒ€ì¥ë³„ í† ë¡  ê´€ì  â€” 1ë¼ìš´ë“œì—ì„œ ê°ì ë¬´ì—‡ì„ ë¶„ì„í•´ì•¼ í•˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì‹œ
_DEBATE_LENSES: dict[str, str] = {
    "cio_manager": (
        "íˆ¬ì/ì¬ë¬´ ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”:\n"
        "- ì´ ì£¼ì œê°€ íšŒì‚¬ ì¬ë¬´ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ (ë§¤ì¶œ, ë¹„ìš©, ROI ìˆ˜ì¹˜ ì¶”ì •)\n"
        "- ì‹¤í–‰ ì‹œ ì¬ë¬´ ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒë¹„ìš©\n"
        "- ì‹œì¥/ê²½ìŸ í™˜ê²½ì—ì„œ íƒ€ì´ë°ì´ ì ì ˆí•œì§€ ê·¼ê±° ì œì‹œ"
    ),
    "cto_manager": (
        "ê¸°ìˆ  ì‹¤í˜„ ê°€ëŠ¥ì„± ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”:\n"
        "- í˜„ì¬ ê¸°ìˆ  ìŠ¤íƒìœ¼ë¡œ êµ¬í˜„ ê°€ëŠ¥í•œì§€, ì¶”ê°€ í•„ìš”í•œ ê¸°ìˆ ì€ ë¬´ì—‡ì¸ì§€\n"
        "- ê°œë°œ ë¦¬ì†ŒìŠ¤ (ì¸ë ¥, ì‹œê°„, ë¹„ìš©) í˜„ì‹¤ì  ì¶”ì •\n"
        "- ê¸°ìˆ ì  ë¦¬ìŠ¤í¬ (í™•ì¥ì„±, ìœ ì§€ë³´ìˆ˜, ë³´ì•ˆ) êµ¬ì²´ì ìœ¼ë¡œ"
    ),
    "cso_manager": (
        "ì‚¬ì—… ì „ëµ ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”:\n"
        "- ì‹œì¥ ê·œëª¨ì™€ ê²½ìŸ êµ¬ë„ (êµ¬ì²´ì  ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€ ì¸ìš©)\n"
        "- ìš°ë¦¬ì˜ ì°¨ë³„í™” í¬ì¸íŠ¸ê°€ ë¬´ì—‡ì´ê³  ê²½ìŸ ìš°ìœ„ê°€ ì§€ì† ê°€ëŠ¥í•œì§€\n"
        "- ì‹¤í–‰ ì „ëµì˜ ë‹¨ê³„ì™€ ìš°ì„ ìˆœìœ„"
    ),
    "cmo_manager": (
        "ë§ˆì¼€íŒ…/ê³ ê° ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”:\n"
        "- íƒ€ê²Ÿ ê³ ê°ì´ ì´ê²ƒì„ ì •ë§ ì›í•˜ëŠ”ì§€, ì–´ë–¤ ê·¼ê±°ê°€ ìˆëŠ”ì§€\n"
        "- ê³ ê° íšë“ ë¹„ìš©(CAC)ê³¼ ì±„ë„ ì „ëµì˜ í˜„ì‹¤ì„±\n"
        "- ë¸Œëœë“œ/í¬ì§€ì…”ë‹ì— ë¯¸ì¹˜ëŠ” ì˜í–¥"
    ),
    "clo_manager": (
        "ë²•ë¬´/ë¦¬ìŠ¤í¬ ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”:\n"
        "- ë²•ì  ë¦¬ìŠ¤í¬ì™€ ê·œì œ ì´ìŠˆ (êµ¬ì²´ì  ë²•ë ¹ì´ë‚˜ íŒë¡€ ì¸ìš©)\n"
        "- ì§€ì‹ì¬ì‚°ê¶Œ ë³´í˜¸ ë°©ì•ˆ ë˜ëŠ” ì¹¨í•´ ìœ„í—˜\n"
        "- ê³„ì•½/ì•½ê´€/ê°œì¸ì •ë³´ ê´€ë ¨ ì£¼ì˜ì‚¬í•­"
    ),
    "cpo_manager": (
        "ì œí’ˆ/ì½˜í…ì¸  ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”:\n"
        "- ì‚¬ìš©ì ê²½í—˜ê³¼ ì œí’ˆ ì™„ì„±ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥\n"
        "- ì½˜í…ì¸  ì „ëµ ë° ì§€ì‹ ìì‚°ìœ¼ë¡œì„œì˜ ê°€ì¹˜\n"
        "- ì‹¤í–‰ ì‹œ í’ˆì§ˆ ê¸°ì¤€ê³¼ ê¸°ë¡/ë¬¸ì„œí™” ë°©ì•ˆ"
    ),
}


async def _call_agent_debate(agent_id: str, topic: str, history: str, extra_instruction: str) -> str:
    """í† ë¡ ìš© ì—ì´ì „íŠ¸ í˜¸ì¶œ â€” ì£¼ì œ + ì´ì „ ë°œì–¸ + ì¶”ê°€ ì§€ì‹œë¥¼ ê²°í•©í•˜ì—¬ í˜¸ì¶œ."""
    prompt = (
        f"[ì„ì› í† ë¡  ëª¨ë“œ]\n"
        f"ì§€ê¸ˆì€ CEOê°€ ì†Œì§‘í•œ ì„ì› í† ë¡ ì…ë‹ˆë‹¤. ë³´ê³ ì„œê°€ ì•„ë‹ˆë¼ \"í† ë¡  ë°œì–¸\"ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n"
        f"í˜•ì‹ì ì¸ ë³´ê³ ì„œ í‹€(## íŒ€ì¥ ì˜ê²¬, ## íŒ€ì› ë³´ê³ ì„œ ìš”ì•½ ë“±)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
        f"ëŒ€ì‹  ë‹¹ì‹ ì˜ í•µì‹¬ ì£¼ì¥ì„ ëª…í™•íˆ ë°íˆê³ , ê·¼ê±°ë¥¼ ë“¤ì–´ ì„¤ë“í•˜ì„¸ìš”.\n\n"
        f"[í† ë¡  ì£¼ì œ]\n{topic}\n\n"
        f"[ì´ì „ ë°œì–¸ë“¤]\n{history if history else '(ì²« ë°œì–¸ì…ë‹ˆë‹¤. ë‹¤ë¥¸ íŒ€ì¥ì˜ ì˜ê²¬ ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ë°œì–¸í•˜ì„¸ìš”.)'}\n\n"
        f"{extra_instruction}"
    )
    result = await _call_agent(agent_id, prompt)
    return result.get("content", str(result)) if isinstance(result, dict) else str(result)


async def _broadcast_with_debate(ceo_message: str, rounds: int = 2) -> dict:
    """ì„ì› íšŒì˜ ë°©ì‹ í† ë¡  â€” CEO ë©”ì‹œì§€ë¥¼ íŒ€ì¥ë“¤ì´ ë‹¤ë‹¨ê³„ í† ë¡  í›„ ë¹„ì„œì‹¤ì¥ì´ ì¢…í•©."""
    debate_history = ""

    # ì°¸ê°€ íŒ€ì¥ ëª©ë¡ (ì„¤ì •ì— ì¡´ì¬í•˜ëŠ” íŒ€ì¥ë§Œ)
    all_managers = ["cio_manager", "cto_manager", "cso_manager", "cmo_manager", "clo_manager", "cpo_manager"]
    manager_ids = [m for m in all_managers if m in _AGENTS_DETAIL]

    for round_num in range(1, rounds + 1):
        rotation_idx = (round_num - 1) % len(DEBATE_ROTATION)
        ordered_managers = [m for m in DEBATE_ROTATION[rotation_idx] if m in manager_ids]

        if round_num == 1:
            # ë¼ìš´ë“œ 1: ë³‘ë ¬ â€” ì„œë¡œ ëª¨ë¥´ê³  ë…ë¦½ ì˜ê²¬ ì œì‹œ (íŒ€ì¥ë³„ ë§ì¶¤ ë¶„ì„ ê´€ì )
            tasks = []
            for mid in ordered_managers:
                lens = _DEBATE_LENSES.get(mid, "ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ ê´€ì ì—ì„œ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.")
                r1_instruction = (
                    f"\n\n[1ë¼ìš´ë“œ â€” ë…ë¦½ ì˜ê²¬ ì œì‹œ]\n"
                    f"{lens}\n\n"
                    f"[ë°œì–¸ ê·œì¹™]\n"
                    f"- ê²°ë¡ ì„ ë¨¼ì € í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•œ ë’¤ ê·¼ê±°ë¥¼ ëŒ€ì„¸ìš”\n"
                    f"- \"~í•  ìˆ˜ ìˆë‹¤\", \"~ì´ ì¢‹ì„ ê²ƒì´ë‹¤\" ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ê¸ˆì§€. êµ¬ì²´ì  ìˆ˜ì¹˜, ì‚¬ë¡€, ê¸°í•œì„ ë„£ìœ¼ì„¸ìš”\n"
                    f"- CEOê°€ ì˜ì‚¬ê²°ì •í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì£¼ì„¸ìš”. êµê³¼ì„œ ë‚´ìš© ë³µë¶™ì´ ì•„ë‹ˆë¼ ì´ ìƒí™©ì— ë§ëŠ” íŒë‹¨ì„ í•˜ì„¸ìš”\n"
                    f"- 300ì ì´ìƒ 800ì ì´í•˜ë¡œ í•µì‹¬ë§Œ"
                )
                tasks.append(_call_agent_debate(mid, ceo_message, "", r1_instruction))
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            for mid, resp in zip(ordered_managers, responses):
                if not isinstance(resp, Exception):
                    mgr_name = _AGENT_NAMES.get(mid, mid)
                    debate_history += f"\n[{mgr_name}ì˜ 1ë¼ìš´ë“œ ì˜ê²¬]\n{resp}\n"
        else:
            # ë¼ìš´ë“œ 2+: ìˆœì°¨ â€” ì´ì „ ë¼ìš´ë“œ ì „ì²´ë¥¼ ì½ê³  ë°˜ë°•/ë³´ê°•
            rebuttal_instruction = (
                f"\n\n[{round_num}ë¼ìš´ë“œ â€” ë°˜ë°• ë° ë³´ê°•]\n"
                "ìœ„ ë°œì–¸ë“¤ì„ ì½ê³  ì•„ë˜ 3ê°€ì§€ë¥¼ ë°˜ë“œì‹œ ìˆ˜í–‰í•˜ì„¸ìš”:\n\n"
                "1. **ë°˜ë°•**: ë‹¤ë¥¸ íŒ€ì¥ ì˜ê²¬ ì¤‘ ê°€ì¥ ì·¨ì•½í•œ ë…¼ë¦¬ë‚˜ ë¹ ì§„ ê´€ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì í•˜ì„¸ìš”.\n"
                "   - ëˆ„êµ¬ì˜ ì–´ë–¤ ì£¼ì¥ì´ ì™œ í‹€ë ¸ê±°ë‚˜ ë¶€ì¡±í•œì§€ ì´ë¦„ì„ ê±°ë¡ í•˜ì—¬ ëª…í™•íˆ ë°íˆì„¸ìš”.\n"
                "   - \"ì¼ë¦¬ ìˆì§€ë§Œ\"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ë¹ˆ ì–‘ë³´ í‘œí˜„ ê¸ˆì§€.\n\n"
                "2. **ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€**: 1ë¼ìš´ë“œì—ì„œ ì•„ë¬´ë„ ì–¸ê¸‰í•˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ê´€ì , ë°ì´í„°, ë¦¬ìŠ¤í¬ë¥¼ í•˜ë‚˜ ì´ìƒ ì œì‹œí•˜ì„¸ìš”.\n\n"
                "3. **ì…ì¥ í‘œëª…**: ì´ ì£¼ì œì— ëŒ€í•œ ë‹¹ì‹ ì˜ ìµœì¢… ì…ì¥ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ëª…í™•íˆ ë°íˆì„¸ìš”.\n"
                "   ì°¬ì„±/ë°˜ëŒ€/ì¡°ê±´ë¶€ ì°¬ì„± ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³  ê·¸ ì´ìœ ë¥¼ ëŒ€ì„¸ìš”.\n\n"
                "- 'ë™ì˜í•©ë‹ˆë‹¤', 'ì¢‹ì€ ì˜ê²¬ì…ë‹ˆë‹¤', 'ê° íŒ€ì¥ì˜ ì˜ê²¬ì„ ì¡´ì¤‘í•©ë‹ˆë‹¤' ê°™ì€ ë¹ˆ ë™ì˜/ì˜ˆì˜ í‘œí˜„ì€ ì ˆëŒ€ ê¸ˆì§€\n"
                "- 300ì ì´ìƒ 800ì ì´í•˜ë¡œ í•µì‹¬ë§Œ"
            )
            for mid in ordered_managers:
                mgr_name = _AGENT_NAMES.get(mid, mid)
                resp = await _call_agent_debate(mid, ceo_message, debate_history, rebuttal_instruction)
                debate_history += f"\n[{mgr_name}ì˜ {round_num}ë¼ìš´ë“œ ë°œì–¸]\n{resp}\n"

    # ë¹„ì„œì‹¤ì¥ì´ í† ë¡  ê²°ê³¼ ì¢…í•©
    synthesis_prompt = (
        f"[ì„ì› í† ë¡  ì¢…í•© ë³´ê³ ]\n\n"
        f"[í† ë¡  ì£¼ì œ]\n{ceo_message}\n\n"
        f"[íŒ€ì¥ë“¤ì˜ í† ë¡  ë‚´ìš©]\n{debate_history}\n\n"
        "ìœ„ í† ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ CEOì—ê²Œ ë³´ê³ í•˜ì„¸ìš”. ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:\n\n"
        "## í•œì¤„ ê²°ë¡ \n"
        "(ì´ í† ë¡ ì˜ ê²°ë¡ ì„ CEOê°€ ì¦‰ì‹œ ì´í•´í•  ìˆ˜ ìˆëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ)\n\n"
        "## í•µì‹¬ ìŸì  (íŒ€ì¥ ê°„ ì‹¤ì œë¡œ ëŒ€ë¦½í•œ ê²ƒë§Œ)\n"
        "| ìŸì  | ì°¬ì„± ì¸¡ | ë°˜ëŒ€ ì¸¡ | íŒì • |\n"
        "(í˜•ì‹ì ìœ¼ë¡œ ì´ê²¬ì´ ì—†ëŠ” í•­ëª©ì€ ì œì™¸. ì‹¤ì œ ì˜ê²¬ ì¶©ëŒë§Œ ê¸°ë¡)\n\n"
        "## ì „ì› í•©ì˜ ì‚¬í•­\n"
        "(íŒ€ì¥ë“¤ì´ ì‹¤ì œë¡œ ê³µí†µ ë™ì˜í•œ í•µì‹¬ í¬ì¸íŠ¸ë§Œ. ì—†ìœ¼ë©´ 'ì—†ìŒ')\n\n"
        "## CEO ê²°ì • í•„ìš” ì‚¬í•­\n"
        "(CEOê°€ ê²°ì •í•´ì•¼ í•  êµ¬ì²´ì  ì„ íƒì§€ë¥¼ A/B í˜•íƒœë¡œ ì œì‹œ. ê° ì„ íƒì§€ì˜ ì¥ë‹¨ì  1ì¤„ì”©)\n\n"
        "## ë¹„ì„œì‹¤ì¥ ê¶Œê³ \n"
        "(ë‹¹ì‹ ì˜ íŒë‹¨ìœ¼ë¡œ ì–´ë–¤ ë°©í–¥ì´ ë‚˜ì€ì§€, ê·¸ ì´ìœ ì™€ í•¨ê»˜)"
    )

    final_result = await _call_agent("chief_of_staff", synthesis_prompt)
    final_content = final_result.get("content", str(final_result)) if isinstance(final_result, dict) else str(final_result)

    return {
        "content": (
            f"## ì„ì› í† ë¡  ê²°ê³¼ ({rounds}ë¼ìš´ë“œ)\n\n"
            f"{final_content}\n\n"
            f"---\n\n"
            f"<details><summary>ì „ì²´ í† ë¡  ë‚´ì—­ ë³´ê¸°</summary>\n\n{debate_history}\n</details>"
        ),
        "debate_rounds": rounds,
        "participants": manager_ids,
        "agent_id": "chief_of_staff",
        "handled_by": f"ì„ì› í† ë¡  ({rounds}ë¼ìš´ë“œ, {len(manager_ids)}ëª… ì°¸ì—¬)",
    }


async def _broadcast_to_managers(text: str, task_id: str, target_agent_id: str | None = None, conversation_id: str | None = None) -> dict:
    """ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ…: Levelì— ë”°ë¼ ì ì ˆí•œ ì—ì´ì „íŠ¸ë§Œ í˜¸ì¶œ.

    Level 1: ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬ (íŒ€ì¥ í˜¸ì¶œ ì—†ìŒ)
    Level 2: íŒ€ì¥ 1ëª…ë§Œ í˜¸ì¶œ (ì „ë¬¸ê°€ ìœ„ì„ ì—†ìŒ)
    Level 3: íŒ€ì¥ 1ëª… + spawn_agent ììœ¨ ì „ë¬¸ê°€ ì„ íƒ
    Level 4: ì „ì› ë³‘ë ¬ í˜¸ì¶œ (ê¸°ì¡´ ë¸Œë¡œë“œìºìŠ¤íŠ¸)
    """
    # CEO ì§ì ‘ ê°œì…: íŠ¹ì • ì—ì´ì „íŠ¸ì—ê²Œ ì§ì ‘ ì „ë‹¬
    if target_agent_id:
        logger.info("CEO ì§ì ‘ ê°œì…: â†’ %s", target_agent_id)
        return await _call_agent(target_agent_id, text, conversation_id=conversation_id)

    level, manager_id = _determine_routing_level(text)
    logger.info("ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ… Level %d, íŒ€ì¥: %s", level, manager_id)

    if level == 1:
        # ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬
        return await _call_agent("chief_of_staff", text, conversation_id=conversation_id)

    elif level == 2:
        # íŒ€ì¥ 1ëª…ë§Œ í˜¸ì¶œ (ì „ë¬¸ê°€ ìœ„ì„ ì—†ìŒ)
        mgr_result = await _call_agent(manager_id, text, conversation_id=conversation_id)
        return await _chief_finalize(text, {manager_id: mgr_result})

    elif level == 3:
        # íŒ€ì¥ + spawn_agent ììœ¨ ì „ë¬¸ê°€ ì„ íƒ
        mgr_result = await _manager_with_delegation_autonomous(manager_id, text, conversation_id=conversation_id)
        return await _chief_finalize(text, {manager_id: mgr_result})

    else:  # level == 4
        return await _broadcast_to_managers_all(text, task_id, conversation_id=conversation_id)


async def _sequential_collaboration(text: str, task_id: str, agent_order: list[str] | None = None) -> dict:
    """ì—ì´ì „íŠ¸ ê°„ ìˆœì°¨ í˜‘ì—… â€” ë¹„ì„œì‹¤ì¥ì´ í—ˆë¸Œë¡œ ë¶€ì„œ ê°„ ìˆœì°¨ ì‘ì—…ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.

    íë¦„:
    1) ë¹„ì„œì‹¤ì¥ì´ AIë¡œ ì‘ì—… ìˆœì„œ ê²°ì • (ë˜ëŠ” CEOê°€ ì§ì ‘ ì§€ì •)
    2) ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸ì—ê²Œ ì›ë³¸ ëª…ë ¹ ì „ë‹¬
    3) ì´ì „ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì—ì´ì „íŠ¸ì—ê²Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬
    4) ëª¨ë“  ì—ì´ì „íŠ¸ ì™„ë£Œ í›„ ë¹„ì„œì‹¤ì¥ì´ ì¢…í•© ë³´ê³ 

    ì˜ˆ: "CPOê°€ ë°ì´í„° ìˆ˜ì§‘ â†’ CMOê°€ ë§ˆì¼€íŒ… ì½˜í…ì¸  ì‘ì„±" ê°™ì€ ìˆœì°¨ ì‘ì—…
    """
    await _broadcast_status("chief_of_staff", "working", 0.1, "ìˆœì°¨ í˜‘ì—… ê³„íš ìˆ˜ë¦½ ì¤‘...")

    # ì—ì´ì „íŠ¸ ìˆœì„œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìœ¼ë©´ AIê°€ ê²°ì •
    if not agent_order:
        order_prompt = (
            f"CEO ëª…ë ¹: {text}\n\n"
            "ì´ ì‘ì—…ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì–´ë–¤ ë¶€ì„œê°€ ì–´ë–¤ ìˆœì„œë¡œ ì‘ì—…í•´ì•¼ í•˜ëŠ”ì§€ ê²°ì •í•˜ì„¸ìš”.\n"
            "ê°€ëŠ¥í•œ ë¶€ì„œ: cto_manager(ê¸°ìˆ ), cso_manager(ì‚¬ì—…), clo_manager(ë²•ë¬´), "
            "cmo_manager(ë§ˆì¼€íŒ…), cio_manager(íˆ¬ì), cpo_manager(ê¸°íš)\n\n"
            "JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:\n"
            '{"order": ["ì²«ë²ˆì§¸_agent_id", "ë‘ë²ˆì§¸_agent_id"], "reason": "ì´ìœ "}\n'
            "ìµœì†Œ 2ê°œ, ìµœëŒ€ 4ê°œ ë¶€ì„œë§Œ ì„ íƒí•˜ì„¸ìš”. ê´€ë ¨ ì—†ëŠ” ë¶€ì„œëŠ” ì œì™¸."
        )
        soul = _load_agent_prompt("chief_of_staff")
        override = _get_model_override("chief_of_staff")
        model = select_model(order_prompt, override=override)
        plan_result = await ask_ai(order_prompt, system_prompt=soul, model=model)

        if "error" not in plan_result:
            try:
                raw = plan_result.get("content", "")
                if "```" in raw:
                    raw = raw.split("```")[1]
                    if raw.startswith("json"):
                        raw = raw[4:]
                parsed = json.loads(raw)
                agent_order = parsed.get("order", [])
            except (json.JSONDecodeError, IndexError):
                pass

        if not agent_order:
            agent_order = ["cto_manager", "cso_manager"]

    # ìœ íš¨í•œ ì—ì´ì „íŠ¸ë§Œ í•„í„°ë§
    valid_agents = set(_AGENT_NAMES.keys())
    agent_order = [a for a in agent_order if a in valid_agents]
    if not agent_order:
        agent_order = ["chief_of_staff"]

    # ìˆœì°¨ ì‹¤í–‰
    chain_context = f"CEO ì›ë³¸ ëª…ë ¹: {text}"
    results = []
    total_cost = 0.0
    total_time = 0.0

    for i, agent_id in enumerate(agent_order):
        agent_name = _AGENT_NAMES.get(agent_id, agent_id)
        step_label = f"[{i+1}/{len(agent_order)}]"

        await _broadcast_status("chief_of_staff", "working", (i + 0.5) / len(agent_order),
                                f"ìˆœì°¨ í˜‘ì—… {step_label} {agent_name} ì‘ì—… ì¤‘...")

        # ì´ì „ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨í•˜ì—¬ í˜¸ì¶œ
        if i == 0:
            agent_input = text
        else:
            prev_results = "\n\n".join(
                f"[{r['name']}ì˜ ì‘ì—… ê²°ê³¼]\n{r['content'][:500]}"
                for r in results
            )
            agent_input = (
                f"{text}\n\n"
                f"## ì´ì „ ë‹¨ê³„ ì‘ì—… ê²°ê³¼ (ì°¸ê³ í•˜ì—¬ ì‘ì—…í•˜ì„¸ìš”)\n{prev_results}"
            )

        result = await _manager_with_delegation(agent_id, agent_input)

        if isinstance(result, Exception):
            results.append({"agent_id": agent_id, "name": agent_name, "content": f"ì˜¤ë¥˜: {result}", "cost_usd": 0})
        elif "error" in result:
            results.append({"agent_id": agent_id, "name": agent_name, "content": f"ì˜¤ë¥˜: {result['error']}", "cost_usd": 0})
        else:
            results.append(result)
            total_cost += result.get("cost_usd", 0)
            total_time += result.get("time_seconds", 0)

    # ë¹„ì„œì‹¤ì¥ ì¢…í•©
    await _broadcast_status("chief_of_staff", "working", 0.9, "ìˆœì°¨ í˜‘ì—… ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì¤‘...")

    chain_summary = "\n\n---\n\n".join(
        f"### {i+1}ë‹¨ê³„: {r.get('name', r.get('agent_id', '?'))}\n{r.get('content', 'ê²°ê³¼ ì—†ìŒ')}"
        for i, r in enumerate(results)
    )

    synthesis_prompt = (
        f"CEO ëª…ë ¹: {text}\n\n"
        f"ì•„ë˜ëŠ” {len(results)}ê°œ ë¶€ì„œê°€ ìˆœì°¨ì ìœ¼ë¡œ ì‘ì—…í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\n"
        f"ì´ì „ ë‹¨ê³„ì˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ê°€ ì°¸ê³ í•˜ì—¬ ì‘ì—…í–ˆìŠµë‹ˆë‹¤.\n\n"
        f"{chain_summary}\n\n"
        f"ìœ„ ìˆœì°¨ í˜‘ì—… ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ CEOì—ê²Œ ê°„ê²°í•œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
    )

    soul = _load_agent_prompt("chief_of_staff")
    override = _get_model_override("chief_of_staff")
    model = select_model(synthesis_prompt, override=override)
    synthesis = await ask_ai(synthesis_prompt, system_prompt=soul, model=model)

    await _broadcast_status("chief_of_staff", "done", 1.0, "ìˆœì°¨ í˜‘ì—… ì™„ë£Œ")

    if "error" in synthesis:
        chief_content = f"âš ï¸ ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì‹¤íŒ¨\n\n{chain_summary}"
    else:
        chief_content = synthesis.get("content", "")
        total_cost += synthesis.get("cost_usd", 0)

    order_names = " â†’ ".join(_AGENT_NAMES.get(a, a) for a in agent_order)
    final_content = (
        f"ğŸ”— **ìˆœì°¨ í˜‘ì—… ë³´ê³ ** ({order_names})\n\n"
        f"{chief_content}\n\n---\n\n"
        f"ğŸ“‚ ìƒì„¸ ë³´ê³ ì„œ {len(results)}ê±´ì´ ê¸°ë°€ë¬¸ì„œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
    )

    # ì•„ì¹´ì´ë¸Œ ì €ì¥
    now_str = datetime.now(KST).strftime("%Y%m%d_%H%M%S")
    save_archive(
        division="secretary",
        filename=f"sequential_collab_{now_str}.md",
        content=f"# [ìˆœì°¨ í˜‘ì—…] {text[:50]}\n\nì‘ì—… ìˆœì„œ: {order_names}\n\n{chain_summary}",
        agent_id="chief_of_staff",
    )

    update_task(task_id, status="completed",
                result_summary=f"ìˆœì°¨ í˜‘ì—… ì™„ë£Œ ({order_names})",
                result_data=final_content,
                success=1, cost_usd=total_cost,
                time_seconds=round(total_time, 2),
                agent_id="chief_of_staff")

    return {
        "content": final_content,
        "agent_id": "chief_of_staff",
        "handled_by": f"ë¹„ì„œì‹¤ì¥ â†’ {order_names}",
        "delegation": f"ìˆœì°¨ í˜‘ì—…: {order_names}",
        "total_cost_usd": round(total_cost, 6),
        "time_seconds": round(total_time, 2),
        "model": "multi-agent-sequential",
        "routing_method": "ìˆœì°¨ í˜‘ì—…",
    }


# ìˆœì°¨ í˜‘ì—… íŠ¸ë¦¬ê±° í‚¤ì›Œë“œ
_SEQUENTIAL_KEYWORDS = ["ìˆœì°¨", "í˜‘ì—…", "ìˆœì„œëŒ€ë¡œ", "ë‹¨ê³„ë³„", "ë¦´ë ˆì´", "ì—°ê³„"]


def _is_sequential_command(text: str) -> bool:
    """ìˆœì°¨ í˜‘ì—… ëª…ë ¹ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    return any(kw in text for kw in _SEQUENTIAL_KEYWORDS)


def _classify_by_keywords(text: str) -> str | None:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ë¶„ë¥˜. ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ None ë°˜í™˜."""
    for agent_id, keywords in _ROUTING_KEYWORDS.items():
        if any(kw in text for kw in keywords):
            return agent_id
    return None


async def _route_task(text: str) -> dict:
    """CEO ëª…ë ¹ì„ ì í•©í•œ ì—ì´ì „íŠ¸ì—ê²Œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤.

    1ë‹¨ê³„: í‚¤ì›Œë“œ ë§¤ì¹­ (ë¬´ë£Œ, ì¦‰ì‹œ)
    2ë‹¨ê³„: AI ë¶„ë¥˜ (Haiku, ~$0.001)
    3ë‹¨ê³„: í´ë°± â†’ ë¹„ì„œì‹¤ì¥
    """
    # 1ë‹¨ê³„: í‚¤ì›Œë“œ ë¶„ë¥˜
    agent_id = _classify_by_keywords(text)
    if agent_id:
        return {
            "agent_id": agent_id,
            "method": "í‚¤ì›Œë“œ",
            "cost_usd": 0.0,
            "reason": "í‚¤ì›Œë“œ ë§¤ì¹­",
        }

    # 2ë‹¨ê³„: AI ë¶„ë¥˜ (í‚¤ì›Œë“œ ì‹¤íŒ¨ ì‹œ)
    result = await classify_task(text)
    if result.get("agent_id") and result["agent_id"] != "chief_of_staff":
        return {
            "agent_id": result["agent_id"],
            "method": "AIë¶„ë¥˜",
            "cost_usd": result.get("cost_usd", 0),
            "reason": result.get("reason", "AI ë¶„ë¥˜"),
        }

    # 3ë‹¨ê³„: í´ë°± â€” ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬
    return {
        "agent_id": "chief_of_staff",
        "method": "ì§ì ‘",
        "cost_usd": result.get("cost_usd", 0),
        "reason": result.get("reason", "ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬"),
    }


def _get_tool_descriptions(agent_id: str) -> str:
    """ì—ì´ì „íŠ¸ì— í• ë‹¹ëœ ë„êµ¬ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    detail = _AGENTS_DETAIL.get(agent_id, {})
    allowed = detail.get("allowed_tools", [])
    if not allowed:
        return ""

    # ë„êµ¬ ID â†’ ì„¤ëª… ë§¤í•‘
    tool_map = {t.get("tool_id"): t for t in _TOOLS_LIST}
    descs = []
    for tid in allowed:
        t = tool_map.get(tid)
        if t:
            name = t.get("name_ko") or t.get("name", tid)
            desc = t.get("description", "")[:150]
            descs.append(f"- **{name}**: {desc}")

    if not descs:
        return ""

    return (
        "\n\n## ì‚¬ìš© ê°€ëŠ¥í•œ ì „ë¬¸ ë„êµ¬\n"
        "ì•„ë˜ ë„êµ¬ì˜ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n"
        + "\n".join(descs)
    )


def _load_agent_prompt(agent_id: str, *, include_tools: bool = True) -> str:
    """ì—ì´ì „íŠ¸ì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(ì†Œìš¸) + ë„êµ¬ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„: DB ì˜¤ë²„ë¼ì´ë“œ > souls/*.md íŒŒì¼ > agents.yaml system_prompt > ê¸°ë³¸ê°’
    include_tools=Trueì´ë©´ ë§ˆì§€ë§‰ì— í• ë‹¹ëœ ë„êµ¬ ì„¤ëª…ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    ë°°ì¹˜ ëª¨ë“œì—ì„œëŠ” include_tools=Falseë¡œ í˜¸ì¶œí•˜ì—¬ ë„êµ¬ ì„¤ëª…ì„ ì œì™¸í•©ë‹ˆë‹¤.
    """
    prompt = ""

    # 1ìˆœìœ„: DB ì˜¤ë²„ë¼ì´ë“œ
    soul = load_setting(f"soul_{agent_id}")
    if soul:
        prompt = soul
    else:
        # 2ìˆœìœ„: souls íŒŒì¼
        soul_path = Path(BASE_DIR).parent / "souls" / "agents" / f"{agent_id}.md"
        if soul_path.exists():
            try:
                prompt = soul_path.read_text(encoding="utf-8")
            except Exception as e:
                logger.debug("ì†Œìš¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ (%s): %s", agent_id, e)

    if not prompt:
        # 3ìˆœìœ„: agents.yamlì˜ system_prompt
        detail = _AGENTS_DETAIL.get(agent_id, {})
        if detail.get("system_prompt"):
            prompt = detail["system_prompt"]

    if not prompt:
        # 4ìˆœìœ„: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        name = _AGENT_NAMES.get(agent_id, _SPECIALIST_NAMES.get(agent_id, agent_id))
        prompt = (
            f"ë‹¹ì‹ ì€ CORTHEX HQì˜ {name}ì…ë‹ˆë‹¤. "
            "CEOì˜ ì—…ë¬´ ì§€ì‹œë¥¼ ë°›ì•„ ì²˜ë¦¬í•˜ê³ , ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. "
            "í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
        )

    if include_tools:
        # ë„êµ¬ ì„¤ëª… ì¶”ê°€ (ì—ì´ì „íŠ¸ê°€ ìì‹ ì˜ ë„êµ¬ë¥¼ ì¸ì§€í•˜ê³  í™œìš©í•  ìˆ˜ ìˆê²Œ)
        tools_desc = _get_tool_descriptions(agent_id)
        if tools_desc:
            prompt += tools_desc

    return prompt


# ë°°ì¹˜ ëª¨ë“œ ì „ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ‘ë¯¸ì‚¬ (ë„êµ¬ í˜¸ì¶œ ë°©ì§€)
_BATCH_MODE_SUFFIX = (
    "\n\n[ë°°ì¹˜ ëª¨ë“œ ì•ˆë‚´] ì´ ìš”ì²­ì€ ë°°ì¹˜ ì²˜ë¦¬ì…ë‹ˆë‹¤. "
    "ë„êµ¬(í•¨ìˆ˜)ë¥¼ ì§ì ‘ í˜¸ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
    "ë³´ìœ í•œ ì§€ì‹ê³¼ ë¶„ì„ ëŠ¥ë ¥ë§Œìœ¼ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. "
    "ì½”ë“œ ë¸”ë¡ì´ë‚˜ í•¨ìˆ˜ í˜¸ì¶œ í˜•íƒœ(ì˜ˆ: await, function() ë“±)ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”."
)


# app_state.chief_prompt â†’ app_state.chief_prompt ì§ì ‘ ì‚¬ìš©


def _load_chief_prompt() -> None:
    """ë¹„ì„œì‹¤ì¥ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤ (ì„œë²„ ì‹œì‘ ì‹œ ìºì‹œ)."""

    app_state.chief_prompt = _load_agent_prompt("chief_of_staff")
    _log("[AI] ë¹„ì„œì‹¤ì¥ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")


def _get_model_override(agent_id: str) -> str | None:
    """ì—ì´ì „íŠ¸ì— ì§€ì •ëœ ëª¨ë¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    ìš°ì„ ìˆœìœ„:
    1. agent_overrides DB (CEO ìˆ˜ë™ ì„¤ì • / ê¶Œì¥ ë²„íŠ¼ / ì¼ê´„ ë³€ê²½)
    2. _AGENTS_DETAIL (agents.yaml ê¸°ë³¸ê°’)
    3. AGENTS ë¦¬ìŠ¤íŠ¸ í´ë°±
    4. ê¸€ë¡œë²Œ ì˜¤ë²„ë¼ì´ë“œ
    """
    # 1. DB ì˜¤ë²„ë¼ì´ë“œ (CEO ìˆ˜ë™ ì„¤ì • â€” ê°€ì¥ ìš°ì„ !)
    overrides = _load_data("agent_overrides", {})
    if agent_id in overrides and "model_name" in overrides[agent_id]:
        return overrides[agent_id]["model_name"]
    # 2. ì—ì´ì „íŠ¸ë³„ ê°œë³„ ì§€ì • ëª¨ë¸ (agents.yaml ê¸°ë³¸ê°’)
    detail = _AGENTS_DETAIL.get(agent_id, {})
    agent_model = detail.get("model_name")
    if agent_model:
        return agent_model
    # 3. AGENTS ë¦¬ìŠ¤íŠ¸ í´ë°±
    for a in AGENTS:
        if a["agent_id"] == agent_id and a.get("model_name"):
            return a["model_name"]
    # 4. ê¸€ë¡œë²Œ ì˜¤ë²„ë¼ì´ë“œ (í…”ë ˆê·¸ë¨ /models ë˜ëŠ” ì›¹ ëŒ€ì‹œë³´ë“œì—ì„œ ì„¤ì •í•œ ì „ì²´ ëª¨ë¸)
    global_override = load_setting("model_override")
    if global_override:
        return global_override
    return None


def _get_agent_reasoning_effort(agent_id: str) -> str:
    """ì—ì´ì „íŠ¸ì˜ reasoning_effortë¥¼ agent_overrides DB â†’ AGENTS ëª©ë¡ ìˆœì„œë¡œ ì¡°íšŒ."""
    overrides = _load_data("agent_overrides", {})
    if agent_id in overrides and "reasoning_effort" in overrides[agent_id]:
        return overrides[agent_id]["reasoning_effort"]
    for a in AGENTS:
        if a["agent_id"] == agent_id:
            return a.get("reasoning_effort", "")
    return ""


def _build_conv_history(conversation_id: str | None, current_text: str) -> list | None:
    """ëŒ€í™” ì„¸ì…˜ì—ì„œ AI conversation_historyë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.

    conversation_idê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì„¸ì…˜ë§Œ, ì—†ìœ¼ë©´ ì „ì²´(ë ˆê±°ì‹œ) ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        if conversation_id:
            recent = load_conversation_messages_by_id(conversation_id, limit=200)
        else:
            recent = load_conversation_messages(limit=100)

        # ìµœê·¼ 20ê°œ ë©”ì‹œì§€ (ì•½ 10í„´)
        tail = recent[-20:] if len(recent) > 20 else recent
        if not tail:
            return None

        conv_history = []
        for m in tail:
            if m["type"] == "user" and m.get("text"):
                conv_history.append({"role": "user", "content": m["text"][:2000]})
            elif m["type"] == "result" and m.get("content"):
                conv_history.append({"role": "assistant", "content": m["content"][:2000]})

        # í˜„ì¬ ë©”ì‹œì§€ì™€ ë™ì¼í•œ ë§ˆì§€ë§‰ user ë©”ì‹œì§€ëŠ” ì œê±° (ì¤‘ë³µ ë°©ì§€)
        if (conv_history and conv_history[-1].get("role") == "user"
                and conv_history[-1].get("content", "").strip() == current_text[:2000].strip()):
            conv_history.pop()

        return conv_history if conv_history else None
    except Exception as e:
        logger.debug("ëŒ€í™” ê¸°ë¡ ë¡œë“œ ì‹¤íŒ¨ (ë¬´ì‹œ): %s", e)
        return None


async def _process_ai_command(text: str, task_id: str, target_agent_id: str | None = None,
                              conversation_id: str | None = None) -> dict:
    """CEO ëª…ë ¹ì„ ì í•©í•œ ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„í•˜ê³  AI ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    íë¦„:
      ì˜ˆì‚° í™•ì¸ â†’ ë¸Œë¡œë“œìºìŠ¤íŠ¸ í™•ì¸ â†’ ë¼ìš°íŒ…(ë¶„ë¥˜) â†’ ìƒíƒœ ì „ì†¡
      â†’ íŒ€ì¥+ì „ë¬¸ê°€ í’€ ì²´ì¸ ìœ„ì„ â†’ ê²€ìˆ˜ â†’ DB ì €ì¥

    ë¸Œë¡œë“œìºìŠ¤íŠ¸ ëª¨ë“œ: "ì „ì²´", "ì¶œì„ì²´í¬" ë“± â†’ ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ… (Level 1~4)
    ë‹¨ì¼ ìœ„ì„ ëª¨ë“œ: í‚¤ì›Œë“œ/AI ë¶„ë¥˜ â†’ íŒ€ì¥+ì „ë¬¸ê°€ ì²´ì¸ í˜¸ì¶œ
    ì§ì ‘ ì²˜ë¦¬: ë¹„ì„œì‹¤ì¥ì´ ì§ì ‘ ë‹µë³€ (ë‹¨ìˆœ ì§ˆë¬¸)
    target_agent_id: CEOê°€ íŠ¹ì • ì—ì´ì „íŠ¸ë¥¼ ì§ì ‘ ì§€ì •í•œ ê²½ìš°
    """
    # 1) ì˜ˆì‚° í™•ì¸
    limit = float(load_setting("daily_budget_usd") or 7.0)
    today = get_today_cost()
    if today >= limit:
        update_task(task_id, status="failed",
                    result_summary=f"ì¼ì¼ ì˜ˆì‚° ì´ˆê³¼ (${today:.2f}/${limit:.0f})",
                    success=0)
        return {"error": f"ì¼ì¼ ì˜ˆì‚°ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ (${today:.2f}/${limit:.0f})"}

    # â”€â”€ ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´ ì‹œìŠ¤í…œ â”€â”€
    text_lower = text.strip().lower()
    text_stripped = text.strip()

    # /ëª…ë ¹ì–´ ë˜ëŠ” /ë„ì›€ë§ â€” ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´ ëª©ë¡
    if text_lower in ("/ëª…ë ¹ì–´", "/ë„ì›€ë§", "/help", "/commands"):
        content = (
            "ğŸ“‹ **ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´**\n\n"
            "| ëª…ë ¹ì–´ | ì„¤ëª… |\n"
            "|--------|------|\n"
            "| `/í† ë¡  [ì£¼ì œ]` | ì„ì› í† ë¡  (2ë¼ìš´ë“œ: ë…ë¦½ ì˜ê²¬ â†’ ì¬ë°˜ë°•) |\n"
            "| `/ì‹¬ì¸µí† ë¡  [ì£¼ì œ]` | ì‹¬ì¸µ ì„ì› í† ë¡  (3ë¼ìš´ë“œ: ë” ê¹Šì€ ë°˜ë°•) |\n"
            "| `/ì „ì²´ [ë©”ì‹œì§€]` | 29ëª… ì—ì´ì „íŠ¸ ë™ì‹œ ê°€ë™ (ë¸Œë¡œë“œìºìŠ¤íŠ¸) |\n"
            "| `/ìˆœì°¨ [ë©”ì‹œì§€]` | ì—ì´ì „íŠ¸ ë¦´ë ˆì´ (ìˆœì„œëŒ€ë¡œ ì‘ì—…) |\n"
            f"| `/ë„êµ¬ì ê²€` | {len(_TOOLS_LIST)}ê°œ ë„êµ¬ ìƒíƒœ ì ê²€ |\n"
            "| `/ë°°ì¹˜ì‹¤í–‰` | ëŒ€ê¸° ì¤‘ì¸ ë°°ì¹˜ ì‘ì—… ì‹¤í–‰ |\n"
            "| `/ë°°ì¹˜ìƒíƒœ` | ë°°ì¹˜ ì²˜ë¦¬ í˜„í™© |\n"
            "| `/ëª…ë ¹ì–´` | ì´ ë„ì›€ë§ |\n\n"
            "**ì¼ë°˜ ë©”ì‹œì§€**ëŠ” ë¹„ì„œì‹¤ì¥ì´ ìë™ìœ¼ë¡œ ì í•©í•œ ë¶€ì„œì— ìœ„ì„í•©ë‹ˆë‹¤."
        )
        update_task(task_id, status="completed", result_summary=content[:500], success=1)
        return {"content": content, "handled_by": "ë¹„ì„œì‹¤ì¥", "agent_id": "chief_of_staff"}

    # /í† ë¡  [ì£¼ì œ] â€” ì„ì› í† ë¡  (2ë¼ìš´ë“œ: ë…ë¦½ ì˜ê²¬ + ì¬ë°˜ë°•)
    if text_stripped.startswith("/í† ë¡ "):
        topic = text_stripped[len("/í† ë¡ "):].strip() or "CORTHEX ì „ëµ ë°©í–¥"
        result = await _broadcast_with_debate(topic, rounds=2)
        update_task(task_id, status="completed", result_summary=f"ì„ì› í† ë¡  ì™„ë£Œ (2ë¼ìš´ë“œ)", success=1)
        result["handled_by"] = result.get("handled_by", "ì„ì› í† ë¡ ")
        return result

    # /ì‹¬ì¸µí† ë¡  [ì£¼ì œ] â€” ì‹¬ì¸µ ì„ì› í† ë¡  (3ë¼ìš´ë“œ: ë” ê¹Šì€ ë°˜ë°•)
    if text_stripped.startswith("/ì‹¬ì¸µí† ë¡ "):
        topic = text_stripped[len("/ì‹¬ì¸µí† ë¡ "):].strip() or "CORTHEX ì „ëµ ë°©í–¥"
        result = await _broadcast_with_debate(topic, rounds=3)
        update_task(task_id, status="completed", result_summary=f"ì‹¬ì¸µ ì„ì› í† ë¡  ì™„ë£Œ (3ë¼ìš´ë“œ)", success=1)
        result["handled_by"] = result.get("handled_by", "ì‹¬ì¸µ ì„ì› í† ë¡ ")
        return result

    # /ì „ì²´ [ë©”ì‹œì§€] â€” ë¸Œë¡œë“œìºìŠ¤íŠ¸ (29ëª… ë™ì‹œ ê°€ë™) â€” í•­ìƒ Level 4 ì „ì› í˜¸ì¶œ
    if text_stripped.startswith("/ì „ì²´"):
        broadcast_text = text_stripped[len("/ì „ì²´"):].strip()
        if not broadcast_text:
            broadcast_text = "ì „ì²´ ì¶œì„ ë³´ê³ "
        return await _broadcast_to_managers_all(broadcast_text, task_id)

    # /ìˆœì°¨ [ë©”ì‹œì§€] â€” ìˆœì°¨ í˜‘ì—… (ì—ì´ì „íŠ¸ ë¦´ë ˆì´)
    if text_stripped.startswith("/ìˆœì°¨"):
        seq_text = text_stripped[len("/ìˆœì°¨"):].strip()
        if not seq_text:
            content = "âš ï¸ `/ìˆœì°¨` ë’¤ì— ì‘ì—… ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\n\nì˜ˆ: `/ìˆœì°¨ CORTHEX ì›¹ì‚¬ì´íŠ¸ ê¸°ìˆ â†’ë³´ì•ˆâ†’ì‚¬ì—…ì„± ë¶„ì„`"
            update_task(task_id, status="completed", result_summary=content[:500], success=1)
            return {"content": content, "handled_by": "ë¹„ì„œì‹¤ì¥", "agent_id": "chief_of_staff"}
        return await _sequential_collaboration(seq_text, task_id)

    # /ë„êµ¬ì ê²€ â€” ë„êµ¬ ê±´ê°• ì ê²€
    if text_lower in ("/ë„êµ¬ì ê²€", "/ë„êµ¬ìƒíƒœ", "/tools_health", "ì „ì²´ ë„êµ¬ ì ê²€", "ë„êµ¬ ì ê²€", "ë„êµ¬ ìƒíƒœ"):
        import urllib.request as _ur
        try:
            req = _ur.Request("http://127.0.0.1:8000/api/tools/health")
            with _ur.urlopen(req, timeout=10) as resp:
                health = json.loads(resp.read().decode("utf-8"))
        except Exception as e:
            health = {"total": 0, "ready": 0, "missing_key": 0, "not_loaded": 0, "tools": [], "error": str(e)}

        content = f"ğŸ”§ **ì „ì²´ ë„êµ¬ ì ê²€ ê²°ê³¼**\n\n"
        content += f"| í•­ëª© | ìˆ˜ëŸ‰ |\n|------|------|\n"
        content += f"| ì „ì²´ ë„êµ¬ | {health.get('total', 0)}ê°œ |\n"
        content += f"| ì •ìƒ (ready) | {health.get('ready', 0)}ê°œ |\n"
        content += f"| API í‚¤ ë¯¸ì„¤ì • | {health.get('missing_key', 0)}ê°œ |\n"
        content += f"| ë¯¸ë¡œë“œ | {health.get('not_loaded', 0)}ê°œ |\n"
        content += f"| ToolPool | {health.get('pool_status', 'unknown')} |\n\n"

        missing = [t for t in health.get("tools", []) if t.get("status") == "missing_key"]
        if missing:
            content += "### âš ï¸ API í‚¤ í•„ìš”í•œ ë„êµ¬\n"
            for t in missing[:10]:
                content += f"- **{t['name']}** (`{t['tool_id']}`) â€” í™˜ê²½ë³€ìˆ˜: `{t.get('api_key_env', '?')}`\n"

        ready = [t for t in health.get("tools", []) if t.get("status") == "ready"]
        if ready:
            content += f"\n### âœ… ì •ìƒ ì‘ë™ ë„êµ¬ ({len(ready)}ê°œ ì¤‘ ìƒìœ„ 10ê°œ)\n"
            for t in ready[:10]:
                content += f"- {t['name']} (`{t['tool_id']}`)\n"

        update_task(task_id, status="completed", result_summary=content[:500], success=1)
        return {"content": content, "handled_by": "ë¹„ì„œì‹¤ì¥", "agent_id": "chief_of_staff"}

    # /ë°°ì¹˜ì‹¤í–‰ â€” ë°°ì¹˜ ì‘ì—… ì‹¤í–‰
    if text_lower in ("/ë°°ì¹˜ì‹¤í–‰", "/batch_flush", "ë°°ì¹˜ì‹¤í–‰", "ë°°ì¹˜ ì‹¤í–‰"):
        result = await _flush_batch_api_queue()
        content = f"ğŸ“¦ **ë°°ì¹˜ ì‹¤í–‰ ê²°ê³¼**\n\n"
        if "error" in result:
            content += f"âŒ ì‹¤íŒ¨: {result['error']}"
        elif result.get("batch_id"):
            content += f"âœ… Batch API ì œì¶œ ì™„ë£Œ\n- batch_id: `{result['batch_id']}`\n- ê±´ìˆ˜: {result.get('count', 0)}ê±´\n- í”„ë¡œë°”ì´ë”: {result.get('provider', '?')}"
        else:
            content += result.get("message", "ì²˜ë¦¬ ì™„ë£Œ")
        update_task(task_id, status="completed", result_summary=content[:500], success=1)
        return {"content": content, "handled_by": "ë¹„ì„œì‹¤ì¥", "agent_id": "chief_of_staff"}

    # /ë°°ì¹˜ìƒíƒœ â€” ë°°ì¹˜ í˜„í™©
    if text_lower in ("/ë°°ì¹˜ìƒíƒœ", "/batch_status", "ë°°ì¹˜ìƒíƒœ", "ë°°ì¹˜ ìƒíƒœ"):
        pending_batches = load_setting("pending_batches") or []
        active = [b for b in pending_batches if b.get("status") in ("pending", "processing")]
        queue_count = len(_batch_api_queue)
        content = f"ğŸ“¦ **ë°°ì¹˜ ìƒíƒœ**\n\n"
        content += f"- ëŒ€ê¸°ì—´: {queue_count}ê±´\n"
        content += f"- ì²˜ë¦¬ ì¤‘ì¸ ë°°ì¹˜: {len(active)}ê±´\n"
        for b in active:
            prog = b.get("progress", {})
            content += f"  - `{b['batch_id'][:20]}...` ({b['provider']}) â€” {prog.get('completed', '?')}/{prog.get('total', '?')} ì™„ë£Œ\n"
        update_task(task_id, status="completed", result_summary=content[:500], success=1)
        return {"content": content, "handled_by": "ë¹„ì„œì‹¤ì¥", "agent_id": "chief_of_staff"}

    # â”€â”€ ìŠ¬ë˜ì‹œ ëª…ë ¹ì–´ ì´ì™¸: í‚¤ì›Œë“œ ê¸°ë°˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸ (ê¸°ì¡´ í˜¸í™˜) â”€â”€
    # "ì „ì²´", "ì¶œì„ì²´í¬" ë“±ì˜ í‚¤ì›Œë“œëŠ” ìŠ¤ë§ˆíŠ¸ ë¼ìš°íŒ… ì ìš© (Level 1~4)
    if _is_broadcast_command(text):
        return await _broadcast_to_managers(text, task_id, target_agent_id=target_agent_id, conversation_id=conversation_id)

    # 3) CEOê°€ @ì—ì´ì „íŠ¸ë¡œ ì§ì ‘ ì§€ì •í•œ ê²½ìš° â†’ ìë™ ë¼ìš°íŒ… ê±´ë„ˆëœ€
    if target_agent_id:
        logger.info("CEO ì§ì ‘ ì§€ì •: â†’ %s", target_agent_id)
        target_id = target_agent_id
        routing = {"agent_id": target_id, "method": "ceo_direct", "cost_usd": 0}
        routing_cost = 0

        # íŒ€ì¥ì´ë“  ì „ë¬¸ê°€ë“  â€” ë¹„ì„œì‹¤ì¥ ìœ„ì„ ì—†ì´ ì§ì ‘ í˜¸ì¶œ
        is_specialist = target_id in _SPECIALIST_NAMES
        if is_specialist or target_id not in _AGENT_NAMES:
            # ì „ë¬¸ê°€ì´ê±°ë‚˜ íŒ€ì¥ë„ ì•„ë‹Œ ì—ì´ì „íŠ¸ â†’ ë°”ë¡œ _call_agent()
            direct_result = await _call_agent(target_id, text, conversation_id=conversation_id)
            direct_name = _SPECIALIST_NAMES.get(target_id, _AGENT_NAMES.get(target_id, target_id))
            if "error" in direct_result:
                update_task(task_id, status="failed",
                            result_summary=f"ì˜¤ë¥˜: {direct_result['error'][:100]}",
                            success=0, agent_id=target_id)
                direct_result["handled_by"] = direct_name
                return direct_result
            total_cost = routing_cost + direct_result.get("cost_usd", 0)
            update_task(task_id, status="completed",
                        result_summary=direct_result.get("content", "")[:500],
                        result_data=direct_result.get("content", ""),
                        success=1, cost_usd=total_cost,
                        tokens_used=direct_result.get("input_tokens", 0) + direct_result.get("output_tokens", 0),
                        time_seconds=direct_result.get("time_seconds", 0),
                        agent_id=target_id)
            direct_result["handled_by"] = direct_name
            direct_result["delegation"] = ""
            direct_result["agent_id"] = target_id
            direct_result["routing_method"] = "ceo_direct"
            direct_result["total_cost_usd"] = total_cost
            return direct_result
        # íŒ€ì¥ì´ë©´ ì•„ë˜ ê¸°ì¡´ ìœ„ì„ ë¡œì§ìœ¼ë¡œ ì§„í–‰
    else:
        # ë¼ìš°íŒ… â€” ì í•©í•œ ì—ì´ì „íŠ¸ ê²°ì •
        routing = await _route_task(text)
        target_id = routing["agent_id"]
        routing_cost = routing.get("cost_usd", 0)

    # 4) ë¹„ì„œì‹¤ì¥ ì§ì ‘ ì²˜ë¦¬ (ì¼ë°˜ ì§ˆë¬¸, ì¸ì‚¬ ë“±)
    if target_id == "chief_of_staff":
        await _broadcast_status("chief_of_staff", "working", 0.2, "ì§ì ‘ ì²˜ë¦¬ ì¤‘...")
        soul = app_state.chief_prompt if app_state.chief_prompt else _load_agent_prompt("chief_of_staff")
        override = _get_model_override("chief_of_staff")
        model = select_model(text, override=override)
        # ëŒ€í™” ë§¥ë½ ë¡œë“œ
        _chief_history = _build_conv_history(conversation_id, text)
        result = await ask_ai(text, system_prompt=soul, model=model,
                              conversation_history=_chief_history)

        await _broadcast_status("chief_of_staff", "done", 1.0, "ì™„ë£Œ")

        if "error" in result:
            update_task(task_id, status="failed",
                        result_summary=f"AI ì˜¤ë¥˜: {result['error'][:100]}",
                        success=0, agent_id="chief_of_staff")
            result["handled_by"] = "ë¹„ì„œì‹¤ì¥"
            return result

        total_cost = routing_cost + result.get("cost_usd", 0)
        update_task(task_id, status="completed",
                    result_summary=result["content"][:500],
                    result_data=result["content"],
                    success=1, cost_usd=total_cost,
                    tokens_used=result.get("input_tokens", 0) + result.get("output_tokens", 0),
                    time_seconds=result.get("time_seconds", 0),
                    agent_id="chief_of_staff")
        result["handled_by"] = "ë¹„ì„œì‹¤ì¥"
        result["delegation"] = ""
        result["agent_id"] = "chief_of_staff"
        result["routing_method"] = routing["method"]
        result["total_cost_usd"] = total_cost
        return result

    # 5) ë¶€ì„œ ìœ„ì„ â€” ë¹„ì„œì‹¤ì¥ì´ ì í•©í•œ íŒ€ì¥ì—ê²Œ ì „ë‹¬
    target_name = _AGENT_NAMES.get(target_id, target_id)
    await _broadcast_status("chief_of_staff", "working", 0.1, f"{target_name}ì—ê²Œ ìœ„ì„ ì¤‘...")

    # íŒ€ì¥ì´ ìê¸° ì „ë¬¸ê°€ë¥¼ í˜¸ì¶œ â†’ ê²°ê³¼ ê²€ìˆ˜ â†’ ì¢…í•© ë³´ê³ ì„œ
    delegation_result = await _manager_with_delegation(target_id, text, conversation_id=conversation_id)
    await _broadcast_status("chief_of_staff", "done", 1.0, "ìœ„ì„ ì™„ë£Œ")

    if "error" in delegation_result:
        update_task(task_id, status="failed",
                    result_summary=f"ìœ„ì„ ì˜¤ë¥˜: {delegation_result['error'][:100]}",
                    success=0, agent_id=target_id)
        delegation_result["handled_by"] = target_name
        return delegation_result

    # 6) ê²°ê³¼ ì •ë¦¬
    total_cost = routing_cost + delegation_result.get("cost_usd", 0)
    specs_used = delegation_result.get("specialists_used", 0)
    delegation_label = f"ë¹„ì„œì‹¤ì¥ â†’ {target_name}"
    if specs_used:
        delegation_label += f" â†’ ì „ë¬¸ê°€ {specs_used}ëª…"

    content = delegation_result.get("content", "")
    header = f"ğŸ“‹ **{target_name}** ë³´ê³ "
    if specs_used:
        header += f" (ì†Œì† ì „ë¬¸ê°€ {specs_used}ëª… ë™ì›)"
    content = f"{header}\n\n---\n\n{content}"

    update_task(task_id, status="completed",
                result_summary=content[:500],
                result_data=content,
                success=1, cost_usd=total_cost,
                time_seconds=delegation_result.get("time_seconds", 0),
                agent_id=target_id)

    return {
        "content": content,
        "agent_id": target_id,
        "handled_by": target_name,
        "delegation": delegation_label,
        "total_cost_usd": round(total_cost, 6),
        "time_seconds": delegation_result.get("time_seconds", 0),
        "model": delegation_result.get("model", ""),
        "routing_method": routing["method"],
    }


# â”€â”€ ë„êµ¬ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸ â”€â”€

def _init_tool_pool():
    """ToolPool ì´ˆê¸°í™” â€” src/tools/ ëª¨ë“ˆì„ ë™ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.

    ask_ai()ë¥¼ ModelRouter ì¸í„°í˜ì´ìŠ¤ë¡œ ê°ì‹¸ëŠ” ì–´ëŒ‘í„°ë¥¼ ë§Œë“¤ì–´,
    ê¸°ì¡´ ë„êµ¬ ì½”ë“œë¥¼ ìˆ˜ì • ì—†ì´ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """

    if app_state.tool_pool is not None:
        return app_state.tool_pool if app_state.tool_pool else None

    try:
        from src.tools.pool import ToolPool
        from src.llm.base import LLMResponse

        class _MiniModelRouter:
            """ask_ai()ë¥¼ ModelRouter.complete() ì¸í„°í˜ì´ìŠ¤ë¡œ ê°ì‹¸ëŠ” ì–´ëŒ‘í„°."""

            class cost_tracker:
                """ë”ë¯¸ ë¹„ìš© ì¶”ì ê¸° (arm_serverëŠ” ìì²´ ë¹„ìš© ì¶”ì  ì‚¬ìš©)."""
                @staticmethod
                def record(*args, **kwargs):
                    pass

            async def complete(self, model_name="", messages=None,
                             temperature=0.3, max_tokens=32768,
                             agent_id="", reasoning_effort=None,
                             use_batch=False):
                messages = messages or []
                system_prompt = ""
                user_message = ""
                for msg in messages:
                    if msg.get("role") == "system":
                        system_prompt = msg["content"]
                    elif msg.get("role") == "user":
                        user_message = msg["content"]

                result = await ask_ai(user_message, system_prompt, model_name)

                if "error" in result:
                    return LLMResponse(
                        content=f"[ë„êµ¬ LLM ì˜¤ë¥˜] {result['error']}",
                        model=model_name,
                        input_tokens=0, output_tokens=0,
                        cost_usd=0.0, provider="unknown",
                    )
                return LLMResponse(
                    content=result["content"],
                    model=result.get("model", model_name),
                    input_tokens=result.get("input_tokens", 0),
                    output_tokens=result.get("output_tokens", 0),
                    cost_usd=result.get("cost_usd", 0.0),
                    provider=result.get("provider", "unknown"),
                )

            async def close(self):
                pass

        router = _MiniModelRouter()
        pool = ToolPool(router)

        tools_config = _load_config("tools")
        pool.build_from_config(tools_config)

        loaded = len(pool._tools)
        app_state.tool_pool = pool
        # AGENTS ì´ˆê¸° ëª¨ë¸ì„ í’€ì— ë“±ë¡ (Skill ë„êµ¬ê°€ caller ì—ì´ì „íŠ¸ ëª¨ë¸ì„ ë”°ë¼ê°€ë„ë¡)
        for a in AGENTS:
            _temp = _AGENTS_DETAIL.get(a["agent_id"], {}).get("temperature", 0.7)
            pool.set_agent_model(a["agent_id"], a.get("model_name", "claude-sonnet-4-6"), temperature=_temp)
        # DBì— ì €ì¥ëœ ì—ì´ì „íŠ¸ ëª¨ë¸ ë®ì–´ì”Œìš°ê¸° (ì‚¬ìš©ìê°€ UIì—ì„œ ë³€ê²½í•œ ê°’ ìš°ì„ )
        try:
            overrides = _load_data("agent_overrides", {})
            for agent_id, vals in overrides.items():
                if "model_name" in vals:
                    _temp = _AGENTS_DETAIL.get(agent_id, {}).get("temperature", 0.7)
                    pool.set_agent_model(agent_id, vals["model_name"], temperature=_temp)
        except Exception as e:
            logger.debug("ì—ì´ì „íŠ¸ ëª¨ë¸ ì˜¤ë²„ë¼ì´ë“œ ì‹¤íŒ¨: %s", e)
        _log(f"[TOOLS] ToolPool ì´ˆê¸°í™” ì™„ë£Œ: {loaded}ê°œ ë„êµ¬ ë¡œë“œ âœ…")
        return pool

    except Exception as e:
        _log(f"[TOOLS] ToolPool ì´ˆê¸°í™” ì‹¤íŒ¨ (ë„êµ¬ ëª©ë¡ë§Œ í‘œì‹œ): {e}")
        app_state.tool_pool = False
        return None


# â”€â”€ ë„êµ¬ ì‹¤í–‰/ìƒíƒœ/ê±´ê°• â†’ handlers/tools_handler.pyë¡œ ë¶„ë¦¬ â”€â”€


# â”€â”€ ì§„í™” ë¡œê·¸ ì‹¤ì‹œê°„ ë¸Œë¡œë“œìºìŠ¤íŠ¸ + REST API â”€â”€

async def _broadcast_evolution_log(message: str, level: str = "info"):
    """ì§„í™” ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ WebSocketìœ¼ë¡œ ì‹¤ì‹œê°„ ë¸Œë¡œë“œìºìŠ¤íŠ¸."""
    from datetime import datetime, timezone, timedelta
    _KST = timezone(timedelta(hours=9))
    now = datetime.now(_KST)
    await wm.broadcast("evolution_log", {
        "message": message,
        "level": level,
        "time": now.strftime("%H:%M:%S"),
        "timestamp": now.isoformat(),
    })


@app.get("/api/evolution/logs")
async def api_evolution_logs(limit: int = 50):
    """ìµœê·¼ ì§„í™” ì‹œìŠ¤í…œ ë¡œê·¸ ì¡°íšŒ (activity_logsì—ì„œ Soul Gym / Soul Evolution í•„í„°)."""
    try:
        conn = get_connection()
        rows = conn.execute(
            """SELECT agent_id, message, level, timestamp
               FROM activity_logs
               WHERE (message LIKE '%Soul Gym%' OR message LIKE '%Soul Evolution%' OR message LIKE '%ì§„í™”%')
               ORDER BY id DESC LIMIT ?""",
            (limit,),
        ).fetchall()
        conn.close()
        logs = [{"agent_id": r[0], "message": r[1], "level": r[2], "timestamp": r[3]} for r in rows]
        return {"logs": logs}
    except Exception as e:
        return {"logs": [], "error": str(e)}


# â”€â”€ Soul Gym 24/7 ìƒì‹œ ë£¨í”„ â”€â”€

_soul_gym_lock = asyncio.Lock()  # ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ Lock

async def _soul_gym_loop():
    """Soul Gym ìƒì‹œ ì§„í™” ë£¨í”„ â€” í•œ ë¼ìš´ë“œ ëë‚˜ë©´ 5ë¶„ ì‰¬ê³  ë‹¤ìŒ ë¼ìš´ë“œ.

    ë¹„ìœ : 24ì‹œê°„ ìš´ì˜ í—¬ìŠ¤ì¥. ì„ ìˆ˜ê°€ ìš´ë™ ëë‚˜ë©´ 5ë¶„ ì‰¬ê³  ë‹¤ì‹œ ì‹œì‘.
    """
    if _soul_gym_lock.locked():
        logger.warning("[SOUL GYM] ì´ë¯¸ ë£¨í”„ ì‹¤í–‰ ì¤‘ â€” ì¤‘ë³µ ë°©ì§€")
        return
    async with _soul_gym_lock:
        INTERVAL_SECONDS = 1800  # ë¼ìš´ë“œ ê°„ ëŒ€ê¸° (30ë¶„, 6íŒ€ì¥ ìˆœì°¨ ì‹¤í–‰ ê³ ë ¤)

        try:
            from soul_gym_engine import evolve_all as _evolve_all
        except ImportError:
            logger.error("[SOUL GYM] soul_gym_engine ì„í¬íŠ¸ ì‹¤íŒ¨")
            return

        round_num = 0
        while True:
            try:
                round_num += 1
                _evo_msg = f"ğŸ§¬ Soul Gym ë¼ìš´ë“œ #{round_num} ì‹œì‘"
                logger.info(_evo_msg)
                save_activity_log("system", _evo_msg, "info")
                await _broadcast_evolution_log(_evo_msg, "info")
                result = await _evolve_all()
                _evo_msg = f"ğŸ§¬ Soul Gym ë¼ìš´ë“œ #{round_num} ì™„ë£Œ â€” {result.get('status', '')}"
                logger.info("ğŸ§¬ Soul Gym ë¼ìš´ë“œ #%d ì™„ë£Œ: %s", round_num, result.get("status", "unknown"))
                save_activity_log("system", _evo_msg, "info")
                await _broadcast_evolution_log(_evo_msg, "info")
            except Exception as e:
                _evo_msg = f"ğŸ§¬ Soul Gym ë¼ìš´ë“œ #{round_num} ì—ëŸ¬: {e}"
                logger.error(_evo_msg)
                save_activity_log("system", _evo_msg, "error")
                await _broadcast_evolution_log(_evo_msg, "error")

            await asyncio.sleep(INTERVAL_SECONDS)


@app.on_event("startup")
async def on_startup():
    """ì„œë²„ ì‹œì‘ ì‹œ DB ì´ˆê¸°í™” + AI í´ë¼ì´ì–¸íŠ¸ + í…”ë ˆê·¸ë¨ ë´‡ + í¬ë¡  ì—”ì§„ + ë„êµ¬ í’€ ì‹œì‘."""
    init_db()
    _sync_agent_defaults_to_db()
    _load_chief_prompt()
    ai_ok = init_ai_client()
    _log(f"[AI] í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {'ì„±ê³µ âœ…' if ai_ok else 'ì‹¤íŒ¨ âŒ (ANTHROPIC_API_KEY ë¯¸ì„¤ì •?)'}")
    try:
        await _start_telegram_bot()
    except Exception as tg_err:
        _log(f"[TG] âŒ ë´‡ ì‹œì‘ ì¤‘ ë¯¸ì²˜ë¦¬ ì˜ˆì™¸: {tg_err}")
        _diag["tg_error"] = f"startup ì˜ˆì™¸: {tg_err}"
    # í¬ë¡  ì‹¤í–‰ ì—”ì§„ ì‹œì‘

    app_state.cron_task = asyncio.create_task(_cron_loop())
    _log("[CRON] í¬ë¡  ì‹¤í–‰ ì—”ì§„ ì‹œì‘ âœ…")
    # ê¸°ë³¸ ìŠ¤ì¼€ì¤„ ìë™ ë“±ë¡ (ì—†ìœ¼ë©´ ìƒì„±)
    _register_default_schedules()
    # í’ˆì§ˆê²€ìˆ˜ ê²Œì´íŠ¸ ì´ˆê¸°í™”
    _init_quality_gate()
    # ë„êµ¬ ì‹¤í–‰ ì—”ì§„ ì´ˆê¸°í™” (ë¹„ë™ê¸° ì•„ë‹Œ ë™ê¸° â€” ì²« ìš”ì²­ ì‹œ lazy ë¡œë“œë„ ì§€ì›)
    _init_tool_pool()
    # cross_agent_protocol ì‹¤ì‹œê°„ ì½œë°± ë“±ë¡
    try:
        from src.tools.cross_agent_protocol import register_call_agent, register_sse_broadcast, register_valid_agents, register_collaboration_log_callback
        register_call_agent(_call_agent)
        register_sse_broadcast(_broadcast_comms)
        register_valid_agents([{
            "agent_id": a["agent_id"],
            "division": a.get("division", ""),
            "superior_id": a.get("superior_id", ""),
            "dormant": a.get("dormant", False),
        } for a in AGENTS])
        # Phase 12: ë¶€ì„œ ê°„ í˜‘ì—… ë¡œê·¸ ì½œë°±
        register_collaboration_log_callback(
            lambda **kw: save_collaboration_log(**kw)
        )
        _log("[P2P] cross_agent_protocol ì½œë°± ë“±ë¡ ì™„ë£Œ âœ… (ì—ì´ì „íŠ¸ í˜¸ì¶œ + SSE + í˜‘ì—…ë¡œê·¸)")
    except Exception as e:
        _log(f"[P2P] cross_agent_protocol ì½œë°± ë“±ë¡ ì‹¤íŒ¨: {e}")
    # PENDING ë°°ì¹˜ ë˜ëŠ” ì§„í–‰ ì¤‘ì¸ ì²´ì¸ì´ ìˆìœ¼ë©´ í´ëŸ¬ ì‹œì‘
    pending_batches = load_setting("pending_batches") or []
    active_batches = [b for b in pending_batches if b.get("status") in ("pending", "processing")]
    chains = load_setting("batch_chains") or []
    active_chains = [c for c in chains if c.get("status") in ("running", "pending")]
    if active_batches or active_chains:
        _ensure_batch_poller()
        _log(f"[BATCH] ë¯¸ì™„ë£Œ ë°°ì¹˜ {len(active_batches)}ê°œ + ì²´ì¸ {len(active_chains)}ê°œ ê°ì§€ â€” í´ëŸ¬ ìë™ ì‹œì‘")
    # ìë™ë§¤ë§¤ ë´‡ ìƒíƒœ DBì—ì„œ ë³µì› (ë°°í¬/ì¬ì‹œì‘ í›„ì—ë„ ìœ ì§€)

    app_state.trading_bot_active = bool(load_setting("trading_bot_active", False))
    if app_state.trading_bot_active:
        app_state.trading_bot_task = asyncio.create_task(_trading_bot_loop())
        _log("[TRADING] ìë™ë§¤ë§¤ ë´‡ DB ìƒíƒœ ë³µì› â†’ ìë™ ì¬ì‹œì‘ âœ…")
    # ê´€ì‹¬ì¢…ëª© ì‹œì„¸ 1ë¶„ ìë™ ê°±ì‹  íƒœìŠ¤í¬ ì‹œì‘
    asyncio.create_task(_auto_refresh_prices())
    _log("[PRICE] ì‹œì„¸ ìë™ ê°±ì‹  íƒœìŠ¤í¬ ì‹œì‘ âœ… (1ë¶„ ê°„ê²©)")
    # KIS í† í° ë§¤ì¼ ì˜¤ì „ 7ì‹œ ìë™ ê°±ì‹  ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    from kis_client import start_daily_token_renewal
    asyncio.create_task(start_daily_token_renewal())
    _log("[KIS] í† í° ìë™ ê°±ì‹  ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ âœ… (ë§¤ì¼ KST 07:00)")
    asyncio.create_task(_cio_prediction_verifier())
    _log("[CIO] ì˜ˆì¸¡ ì‚¬í›„ê²€ì¦ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ âœ… (ë§¤ì¼ KST 03:00)")
    asyncio.create_task(_cio_weekly_soul_update())
    _log("[CIO] ì£¼ê°„ soul ìë™ ì—…ë°ì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ âœ… (ë§¤ì£¼ ì¼ìš”ì¼ KST 02:00)")
    asyncio.create_task(_shadow_trading_alert())
    _log("[Shadow] Shadow Trading ì•Œë¦¼ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ âœ… (ë§¤ì¼ KST 09:00, +5% ê¸°ì¤€)")
    # ë©”ëª¨ë¦¬ ì •ë¦¬ íƒœìŠ¤í¬ (10ë¶„ë§ˆë‹¤ bg_results, notion_log ì •ë¦¬)
    app_state._cleanup_task = asyncio.create_task(app_state.periodic_cleanup())
    _log("[CLEANUP] ë©”ëª¨ë¦¬ ìë™ ì •ë¦¬ íƒœìŠ¤í¬ ì‹œì‘ âœ… (10ë¶„ ê°„ê²©)")
    # Soul Gym 24/7 ìƒì‹œ ë£¨í”„ (ëŒ€í‘œë‹˜ ì§€ì‹œ 2026-02-25: "24ì‹œê°„ 7ì¼ ë‚´ë‚´")
    asyncio.create_task(_soul_gym_loop())
    _log("[SOUL GYM] 24/7 ìƒì‹œ ì§„í™” ë£¨í”„ ì‹œì‘ âœ… (ë¼ìš´ë“œë‹¹ ~$0.012)")


@app.on_event("shutdown")
async def on_shutdown():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì •ë¦¬ + í…”ë ˆê·¸ë¨ ë´‡ ì¢…ë£Œ."""
    cancelled = await app_state.cancel_all_bg_tasks()
    _log(f"[SHUTDOWN] ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ {cancelled}ê°œ ì·¨ì†Œ")
    await _stop_telegram_bot()
    _log("[SHUTDOWN] ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
