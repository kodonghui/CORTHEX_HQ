# prompt_tester — 프롬프트 테스터 도구 가이드

## 이 도구는 뭔가요?
AI에게 주는 지시문(프롬프트)의 품질을 테스트하고 개선하는 도구입니다.
같은 프롬프트를 여러 AI 모델에 동시에 보내서 응답을 비교하거나, 두 가지 프롬프트 중 어떤 것이 더 효과적인지 A/B 테스트를 하거나, 프롬프트 자체의 품질(명확성, 구체성 등)을 점수로 평가해줍니다.

## 어떤 API를 쓰나요?
- **OpenAI API, Anthropic API, Google AI API** — 여러 모델로 프롬프트를 실행하여 비교
- 비용: **유료** (각 모델 호출마다 토큰 비용 발생. test 액션은 여러 모델을 호출하므로 비용이 배수로 늘어남)
- 필요한 키: `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY` (테스트 대상 모델에 따라)

## 사용법

### action=test (다중 모델 테스트)
```
action=test, prompt="테스트할 프롬프트 내용", models="gpt-5-mini,claude-haiku-4-5-20251001", system_prompt="시스템 지시문(선택)"
```
- 하나의 프롬프트를 여러 AI 모델에 동시에 보내서 결과를 비교합니다
- models: 쉼표로 구분된 모델 ID 목록 (기본값: gpt-5-mini, claude-haiku-4-5-20251001)
- 반환: 모델별 응답 시간, 성공 여부, 각 모델의 실제 응답 내용

**예시:**
- `action=test, prompt="한국 경제 전망을 3줄로 요약해줘", models="gpt-5-mini,claude-haiku-4-5-20251001"` → 두 모델의 응답 비교

### action=compare (프롬프트 A/B 비교)
```
action=compare, prompt_a="첫 번째 프롬프트", prompt_b="두 번째 프롬프트", model="claude-sonnet-4-6"
```
- 두 개의 프롬프트를 같은 모델에 보내서 어떤 프롬프트가 더 효과적인지 AI가 평가합니다
- 평가 기준: 정확성, 구체성, 유용성, 간결성
- 반환: 각 프롬프트의 응답 요약, AI의 비교 평가

**예시:**
- `action=compare, prompt_a="주식 분석해줘", prompt_b="삼성전자의 2026년 1분기 실적 전망을 기술적 분석과 펀더멘털 분석 관점에서 각각 3줄로 정리해줘"` → 구체적인 프롬프트가 더 낫다는 평가 예상

### action=evaluate (프롬프트 품질 평가)
```
action=evaluate, prompt="평가받을 프롬프트"
```
- 프롬프트 자체의 품질을 5가지 기준으로 점수(0~100점)를 매깁니다
- 평가 기준: 명확성, 구체성, 맥락 제공, 제약 조건, 종합 점수
- 각 항목에 대한 개선 방안과 개선된 프롬프트 예시도 함께 제공
- 반환: 5가지 항목별 점수 + 이유 + 개선된 프롬프트 예시

**예시:**
- `action=evaluate, prompt="좋은 글 써줘"` → 명확성 낮음, 구체성 낮음 등 평가 + "어떤 주제의 어떤 형식의 글인지 명시하세요" 같은 개선 제안

## 이 도구를 쓰는 에이전트들

### 1. 기술개발처장 (CTO, cto_manager)
**언제 쓰나?** CORTHEX HQ 에이전트들의 시스템 프롬프트를 개선하거나, 새 에이전트 프롬프트를 테스트할 때
**어떻게 쓰나?**
- test로 에이전트 프롬프트를 여러 모델에 돌려보고 최적의 모델 선택
- evaluate로 기존 에이전트 시스템 프롬프트의 품질을 점검하고 개선

**실전 시나리오:**
> CEO가 "CIO 에이전트 응답이 좀 부실한데 개선해줘" 라고 하면:
> 1. `action=evaluate`로 CIO의 현재 시스템 프롬프트 품질 평가
> 2. 평가 결과에서 부족한 부분 파악 (예: 구체성 점수 낮음)
> 3. 개선된 프롬프트를 `action=compare`로 기존 것과 비교 테스트
> 4. 더 나은 프롬프트를 agents.yaml에 반영

### 2. AI 모델 Specialist (ai_model_specialist)
**언제 쓰나?** 새 AI 모델이 나왔을 때 기존 모델과 성능 비교, 프롬프트 엔지니어링 최적화
**어떻게 쓰나?**
- test로 동일 프롬프트를 신규/기존 모델에 돌려서 응답 품질과 속도 비교
- compare로 프롬프트 튜닝 시 A/B 테스트 수행
- evaluate로 팀 내 에이전트 프롬프트를 정기적으로 품질 감사

**실전 시나리오:**
> CEO가 "새 GPT-5.2 모델이 나왔는데 써볼까?" 라고 하면:
> 1. `action=test, models="gpt-5,gpt-5.2,claude-sonnet-4-6"`로 주요 업무 프롬프트 비교
> 2. 모델별 응답 속도, 품질, 비용 비교표 작성
> 3. CTO에게 모델 교체 추천 여부 보고

## 주의사항
- test 액션은 모델 수만큼 API를 호출하므로, 3개 모델이면 비용이 3배입니다
- compare 액션은 2번의 AI 호출 + 1번의 평가 호출 = 총 3번 API를 호출합니다
- evaluate 액션은 1번의 AI 호출만 합니다 (비용 가장 저렴)
- 테스트 시 temperature=0.2로 고정되어 있어, 창의적 응답보다는 일관된 응답 비교에 적합합니다
