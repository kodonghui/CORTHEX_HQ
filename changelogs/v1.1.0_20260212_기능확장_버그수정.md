# v1.1.0 — 기능 확장 + 버그 수정 (2026-02-12)

> **버전**: v1.1.0
> **날짜**: 2026-02-12
> **유형**: 기능 추가 (Minor) + 버그 수정
> **이전 버전**: v1.0.0 (2026-02-08)

---

## 요약

6가지 주요 변경사항:
1. 비서실장 보고서 중복 버그 수정
2. 모델 선택지 확대 + 추론 깊이 제어
3. 중간 보고서 아카이브 시스템
4. Batch API 절약 모드 (50% 할인)
5. GitHub 자동 동기화
6. 모바일 접근 (Telegram Bot + REST API)

---

## 1. 버그 수정: 비서실장 보고서 중복 (Phase 1)

### 증상
비서실장에게 명령하면 같은 보고서가 2번 표시됨. 작업내역은 1건인데 내용이 반복.

### 원인 A — `_synthesize_results()` 불필요한 재합성
- `ManagerAgent._synthesize_results()`가 부하 에이전트 결과가 1개여도 LLM에게 "종합하라"고 재요청
- 이미 완성된 보고서를 다시 쓰면서 내용이 중복됨

### 원인 B — WebSocket 좀비 연결
- 작업이 오래 걸리면 WebSocket 연결이 끊기고 재연결 시 이전 연결을 close하지 않음
- 서버가 좀비 연결에도 broadcast하여 클라이언트에서 같은 메시지 2번 수신

### 수정 내용
| 파일 | 변경 |
|------|------|
| `src/core/agent.py` | `_synthesize_results()`: 결과 1개이면 LLM 호출 없이 바로 반환 (`if len(results) == 1: return results[0].result_data`) |
| `web/templates/index.html` | `connectWebSocket()`: 재연결 전에 이전 WS의 `onclose`/`onmessage`/`onerror` 핸들러를 null로 설정 후 `close()` 호출 |

---

## 2. 모델 선택 + 추론 깊이 제어 (Phase 2)

### 새로 추가된 모델

| 모델 | Provider | Input (1M) | Output (1M) | 용도 |
|------|----------|-----------|------------|------|
| `gpt-5-mini` | OpenAI | $0.25 | $2.00 | Worker (저렴) |
| `gpt-5` | OpenAI | $1.25 | $10.00 | Manager |
| `gpt-5-2` | OpenAI | $1.75 | $14.00 | Manager |
| `gpt-5-2-pro` | OpenAI | $21.00 | $168.00 | Executive (비쌈) |
| `claude-opus-4-6` | Anthropic | $5.00 | $25.00 | Executive |

기존 모델 (`gpt-4o`, `gpt-4o-mini`, `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`, `o3-mini`, `claude-sonnet-4-5`, `claude-haiku-4-5`, `claude-opus-4-0`)도 유지.

### 추론 깊이 제어 (reasoning effort)

에이전트별로 추론 깊이를 설정 가능:
- **기본**: 일반 응답 (빠르고 저렴)
- **낮음**: 가벼운 추론 (Anthropic: 2048 budget tokens)
- **중간**: 중간 추론 (Anthropic: 8192 budget tokens)
- **높음**: 깊은 추론 (Anthropic: 32768 budget tokens)

OpenAI GPT-5.2/o-시리즈: `reasoning_effort` 파라미터로 전달
Anthropic Claude: `thinking.budget_tokens`로 extended thinking 활성화

### 수정 파일
| 파일 | 변경 |
|------|------|
| `config/models.yaml` | GPT-5.x, Claude 4.6 모델 정보 추가 |
| `src/llm/base.py` | `LLMProvider.complete()` 시그니처에 `reasoning_effort`, `is_batch` 파라미터 추가 |
| `src/llm/openai_provider.py` | GPT-5.x 가격 추가, `reasoning_effort` 파라미터 지원 |
| `src/llm/anthropic_provider.py` | Claude 4.6 가격 추가, extended thinking (`budget_tokens`) 지원, thinking 블록 파싱 |
| `src/llm/router.py` | `reasoning_effort` + `use_batch` 전달, `o5-*` 프리픽스 라우팅, `batch_collector` 인스턴스 지원 |
| `src/core/agent.py` | `AgentConfig.reasoning_effort` 필드 추가, `think()`에서 reasoning_effort 전달 |
| `web/app.py` | `PUT /api/agents/{agent_id}/reasoning` 엔드포인트 추가 |
| `web/templates/index.html` | 에이전트 상세 패널에 추론 수준 드롭다운 UI 추가 |

---

## 3. 중간 보고서 아카이브 (Phase 3)

### 목적
CEO가 최종 보고서뿐 아니라 **각 에이전트가 실제로 무슨 작업을 했는지** 확인 가능.

### 아카이브 구조
```
archive/
├── secretary/          # 비서실
├── tech/               # 기술부
├── finance/            # 재무부
├── strategy/           # 전략부
├── legal/              # 법무부
├── marketing/          # 마케팅부
└── publishing/         # 출판부
```

각 파일은 마크다운 형식으로 저장:
- 작성자, 소속, 보고 대상, 지시 내용, 시각, 소요 시간, 상관 작업 ID

### API
- `GET /api/archive` — 전체 아카이브 파일 목록
- `GET /api/archive/{division}/{filename}` — 개별 보고서 조회
- `GET /api/archive/by-correlation/{correlation_id}` — 특정 작업의 모든 중간 보고서

### UI
작업내역 상세 화면에 "중간 보고서 (에이전트별 원본)" 펼치기/접기 섹션 추가.

### 수정 파일
| 파일 | 변경 |
|------|------|
| `src/core/message.py` | `TaskResult`에 `task_description` 필드 추가 |
| `src/core/agent.py` | `handle_task()`에서 result 생성 시 `task_description` 포함 |
| `web/app.py` | `_archive_agent_report()` 함수 + `_handle_message_event()` 훅 + 아카이브 조회 API 3개 |
| `web/templates/index.html` | 작업 상세에 중간 보고서 아카이브 UI 추가 |

---

## 4. Batch API 절약 모드 (Phase 4)

### 작동 방식
CEO가 명령 시 **실시간 / 절약** 모드 선택:
- **실시간**: 기존과 동일, 즉시 API 호출
- **절약**: Batch API로 50% 할인 적용

| 모드 | Opus 4.6 Input (1M) | Opus 4.6 Output (1M) | 절감 |
|------|---------------------|----------------------|------|
| 실시간 | $5.00 | $25.00 | - |
| Batch | **$2.50** | **$12.50** | **50%** |

### 핵심 구현: `BatchCollector`

`src/llm/batch_collector.py` (신규 파일):
- 여러 에이전트의 LLM 요청을 0.5초 디바운스로 모아서 한번에 제출
- OpenAI: JSONL 파일 업로드 → Batch 생성 → 폴링 → 결과 다운로드
- Anthropic: Message Batches API → 폴링 → 결과 스트리밍
- 각 요청은 `asyncio.Future`로 대기하며, Batch 응답이 오면 resolve

### UI
관제실 입력창에 "실시간 / 절약 (-50%)" 선택 드롭다운 추가.

### 수정/생성 파일
| 파일 | 변경 |
|------|------|
| `src/llm/batch_collector.py` | **신규** — BatchCollector 클래스 |
| `src/llm/router.py` | `use_batch` 분기 + `batch_collector` 주입 포인트 |
| `src/llm/openai_provider.py` | `_calculate_cost()` batch 50% 할인 |
| `src/llm/anthropic_provider.py` | `_calculate_cost()` batch 50% 할인 |
| `src/core/agent.py` | `think()`에서 `use_batch` 전달, `handle_task()`에서 context의 batch 플래그 읽기 |
| `web/app.py` | WebSocket에서 batch 모드 파싱 + `_run_background_task()`에 전달 + BatchCollector 초기화 |
| `web/templates/index.html` | 실시간/절약 모드 선택 UI |

---

## 5. GitHub 자동 동기화 (Phase 5)

### 작동 방식
에이전트 작업 완료 시 자동으로:
1. `git pull --rebase --autostash` (충돌 방지)
2. `git add archive/ output/`
3. 변경사항이 있으면 `git commit -m "auto: 보고서 동기화 (2026-02-12 14:35)"`
4. `git push`

### 목적
노트북 2대에서 작업 시 보고서가 자동으로 GitHub에 올라가서 양쪽에서 공유 가능.

### 수정 파일
| 파일 | 변경 |
|------|------|
| `web/app.py` | `_auto_sync_to_github()` 함수 추가 + task 완료 후 비동기 호출 |

---

## 6. 모바일 접근 — Telegram Bot + REST API (Phase 6)

### Telegram Bot
아이폰/아이패드에서 텔레그램 앱을 통해 CORTHEX HQ에 명령.

- Polling 방식 (공개 URL 불필요, 방화벽 뒤에서도 동작)
- `TELEGRAM_BOT_TOKEN` + `TELEGRAM_CHAT_ID`로 CEO만 접근 가능
- 일반 메시지 → 실시간 모드
- `/batch <명령>` → 절약 모드
- `/status` → 시스템 상태 확인
- 결과는 4096자 제한에 맞게 자동 분할 전송

### REST API
외부 클라이언트(OpenClaw, 커스텀 앱 등)에서 HTTP로 명령 제출:

```
POST /api/command
{
  "text": "시장 분석 해줘",
  "depth": 3,
  "batch": false
}
```

응답: `{"task_id": "...", "status": "accepted"}`
결과 확인: `GET /api/tasks/{task_id}`

### 수정/생성 파일
| 파일 | 변경 |
|------|------|
| `src/integrations/__init__.py` | **신규** — 패키지 초기화 |
| `src/integrations/telegram_bot.py` | **신규** — CorthexTelegramBot 클래스 |
| `web/app.py` | `POST /api/command` REST 엔드포인트 + `_execute_command_for_api()` + Telegram bot startup |

### 환경변수 (.env)
```
TELEGRAM_BOT_TOKEN=your_token_here
TELEGRAM_CHAT_ID=your_chat_id
```

---

## 변경된 파일 전체 목록

### 수정
| 파일 | Phase |
|------|-------|
| `src/core/agent.py` | 1, 2, 3, 4 |
| `src/core/message.py` | 3 |
| `src/llm/base.py` | 2 |
| `src/llm/openai_provider.py` | 2, 4 |
| `src/llm/anthropic_provider.py` | 2, 4 |
| `src/llm/router.py` | 2, 4 |
| `config/models.yaml` | 2 |
| `web/app.py` | 2, 3, 4, 5, 6 |
| `web/templates/index.html` | 1, 2, 3, 4 |

### 신규
| 파일 | Phase |
|------|-------|
| `src/llm/batch_collector.py` | 4 |
| `src/integrations/__init__.py` | 6 |
| `src/integrations/telegram_bot.py` | 6 |
| `changelogs/v1.0.0_20260208_초기시스템.md` | — |
| `changelogs/v1.1.0_20260212_기능확장_버그수정.md` | — |

---

## 호환성

- **하위 호환**: 기존 `agents.yaml`에 `reasoning_effort` 필드가 없어도 기본값 `""`으로 동작
- **Telegram**: `python-telegram-bot` 미설치 시 자동 비활성화 (경고 로그만)
- **Batch**: `batch_collector`가 None이면 항상 실시간 모드로 fallback
- **GitHub 동기화**: git이 없거나 remote가 없어도 에러 무시

---

## 알려진 이슈

1. Batch 모드 사용 시 OpenAI Batch API는 결과 반환까지 수분~수십분 소요될 수 있음
2. 아카이브 파일이 많아지면 `GET /api/archive` 응답이 느려질 수 있음 (향후 페이지네이션 필요)
3. GitHub 자동 동기화는 git remote가 설정되어 있어야 함
4. Telegram Bot은 polling 방식이므로 서버가 꺼지면 메시지를 받을 수 없음
