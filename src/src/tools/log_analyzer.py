"""
ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ê¸° ë„êµ¬ (Log Analyzer).

ë¡œê·¸ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì—ëŸ¬ ìœ í˜•, ë¹ˆë„, íŒ¨í„´ì„ ìë™ìœ¼ë¡œ
í†µê³„ ë‚´ê³ , ì‹œê°„ëŒ€ë³„ ë¶„í¬ë¥¼ í…ìŠ¤íŠ¸ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•:
  - action="analyze": ë¡œê·¸ íŒŒì¼ ì „ì²´ ë¶„ì„ (log_file, level, hours)
  - action="top_errors": ê°€ì¥ ë§ì´ ë°œìƒí•˜ëŠ” ì—ëŸ¬ Top N (top_n)
  - action="timeline": ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ë°œìƒ ë¹ˆë„ (log_file, hours)
  - action="activity": DB í™œë™ ë¡œê·¸ ì¡°íšŒ (agent_id, level, keyword, limit)
  - action="trading": ìë™ë§¤ë§¤ ê´€ë ¨ í™œë™ ë¡œê·¸ë§Œ í•„í„° ë¶„ì„ (hours, limit)

í•„ìš” í™˜ê²½ë³€ìˆ˜: ì—†ìŒ
ì˜ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬: ì—†ìŒ (ìˆœìˆ˜ íŒŒì´ì¬)
"""
from __future__ import annotations

import logging
import re
from collections import Counter, defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any

from src.tools.base import BaseTool

logger = logging.getLogger("corthex.tools.log_analyzer")

KST = timezone(timedelta(hours=9))

DEFAULT_LOG_FILE = "logs/corthex.log"

# í‘œì¤€ íŒŒì´ì¬ ë¡œê·¸ í˜•ì‹ íŒŒì‹±
LOG_PATTERN = re.compile(
    r"(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}),?\d*\s*[-â€“]\s*"
    r"([\w.]+)\s*[-â€“]\s*(DEBUG|INFO|WARNING|ERROR|CRITICAL)\s*[-â€“]\s*(.*)"
)

# ë©”ì‹œì§€ ì •ê·œí™”ìš© íŒ¨í„´ (ë³€ìˆ˜ ë¶€ë¶„ì„ ì¹˜í™˜í•˜ì—¬ ê·¸ë£¹í•‘)
NORMALIZE_PATTERNS = [
    (re.compile(r"\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}[.\d]*"), "{TIMESTAMP}"),
    (re.compile(r"https?://\S+"), "{URL}"),
    (re.compile(r"\b\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\b"), "{IP}"),
    (re.compile(r"\b[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\b", re.I), "{UUID}"),
    (re.compile(r"\b\d+\b"), "{N}"),
]


@dataclass
class LogEntry:
    """íŒŒì‹±ëœ ë¡œê·¸ í•œ ì¤„."""
    timestamp: datetime
    logger_name: str
    level: str
    message: str


class LogAnalyzerTool(BaseTool):
    """ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ê¸° â€” ë¡œê·¸ íŒŒì¼ì—ì„œ ì—ëŸ¬ íŒ¨í„´ê³¼ ë¹ˆë„ë¥¼ ìë™ ë¶„ì„í•©ë‹ˆë‹¤."""

    async def execute(self, **kwargs: Any) -> str:
        action = kwargs.get("action", "analyze")

        if action == "analyze":
            return await self._analyze(kwargs)
        elif action == "top_errors":
            return await self._top_errors(kwargs)
        elif action == "timeline":
            return self._timeline(kwargs)
        elif action == "activity":
            return await self._activity_logs(kwargs)
        elif action == "trading":
            return await self._trading_logs(kwargs)
        else:
            return (
                f"ì•Œ ìˆ˜ ì—†ëŠ” action: {action}. "
                "analyze, top_errors, timeline, activity, trading ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            )

    # â”€â”€ ë¡œê·¸ íŒŒì‹± â”€â”€

    @staticmethod
    def _parse_log_file(log_file: str, level: str = "ALL", hours: int = 24) -> list[LogEntry]:
        """ë¡œê·¸ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ LogEntry ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        path = Path(log_file)
        if not path.exists():
            return []

        cutoff = datetime.now(KST) - timedelta(hours=hours)
        entries: list[LogEntry] = []

        for line in path.read_text(encoding="utf-8", errors="replace").splitlines():
            m = LOG_PATTERN.match(line.strip())
            if not m:
                continue

            ts_str, logger_name, log_level, message = m.groups()
            try:
                ts = datetime.strptime(ts_str, "%Y-%m-%d %H:%M:%S").replace(tzinfo=KST)
            except ValueError:
                continue

            if ts < cutoff:
                continue

            if level != "ALL" and log_level != level:
                continue

            entries.append(LogEntry(
                timestamp=ts,
                logger_name=logger_name,
                level=log_level,
                message=message.strip(),
            ))

        return entries

    @staticmethod
    def _normalize_message(message: str) -> str:
        """ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ë³€ìˆ˜ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ íŒ¨í„´í™”í•©ë‹ˆë‹¤."""
        result = message
        for pattern, replacement in NORMALIZE_PATTERNS:
            result = pattern.sub(replacement, result)
        return result

    # â”€â”€ action êµ¬í˜„ â”€â”€

    @staticmethod
    def _ensure_log_file(log_file: str) -> None:
        """ë¡œê·¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë””ë ‰í† ë¦¬ì™€ ë¹ˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        path = Path(log_file)
        path.parent.mkdir(parents=True, exist_ok=True)
        if not path.exists():
            path.touch()
            logger.info("[LogAnalyzer] ë¡œê·¸ íŒŒì¼ ìƒì„±: %s", log_file)

    async def _analyze(self, kwargs: dict[str, Any]) -> str:
        """ë¡œê·¸ íŒŒì¼ ì „ì²´ ë¶„ì„."""
        log_file = kwargs.get("log_file", DEFAULT_LOG_FILE)
        level = kwargs.get("level", "ERROR").upper()
        hours = int(kwargs.get("hours", 24))

        # ë¡œê·¸ íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        self._ensure_log_file(log_file)

        # ì „ì²´ ë ˆë²¨ ì¹´ìš´íŠ¸ë¥¼ ìœ„í•´ ALLë¡œ ë¨¼ì € íŒŒì‹±
        all_entries = self._parse_log_file(log_file, "ALL", hours)

        if not all_entries:
            return f"ìµœê·¼ {hours}ì‹œê°„ ë‚´ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. (íŒŒì¼: {log_file})"

        # ë ˆë²¨ë³„ ê±´ìˆ˜
        level_counts = Counter(e.level for e in all_entries)

        # ìš”ì²­ëœ ë ˆë²¨ë§Œ í•„í„°
        if level != "ALL":
            filtered = [e for e in all_entries if e.level == level]
        else:
            filtered = all_entries

        # ëª¨ë“ˆë³„ ë¶„í¬
        module_counts = Counter(e.logger_name for e in filtered)

        # ì—ëŸ¬ ë©”ì‹œì§€ ê·¸ë£¹í•‘
        msg_patterns = Counter(self._normalize_message(e.message) for e in filtered)

        lines = [
            f"## ë¡œê·¸ ë¶„ì„ ê²°ê³¼",
            f"íŒŒì¼: {log_file} | ê¸°ê°„: ìµœê·¼ {hours}ì‹œê°„ | í•„í„°: {level}",
            "",
            "### ë ˆë²¨ë³„ ê±´ìˆ˜",
        ]
        for lvl in ["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"]:
            cnt = level_counts.get(lvl, 0)
            if cnt > 0:
                lines.append(f"  {lvl}: {cnt:,}ê±´")

        lines.append(f"\n### í•„í„°ëœ ë¡œê·¸ ê±´ìˆ˜: {len(filtered):,}ê±´")

        if module_counts:
            lines.append("\n### ëª¨ë“ˆë³„ ë¶„í¬")
            for mod, cnt in module_counts.most_common(10):
                lines.append(f"  {mod}: {cnt:,}ê±´")

        if msg_patterns:
            lines.append("\n### ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ (ìƒìœ„ 10ê°œ)")
            for pattern, cnt in msg_patterns.most_common(10):
                lines.append(f"  [{cnt:,}ê±´] {pattern[:100]}")

        result = "\n".join(lines)

        # LLM ë¶„ì„
        analysis = await self._llm_call(
            system_prompt=(
                "ë‹¹ì‹ ì€ ì‹œìŠ¤í…œ ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
                "ë¡œê·¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë³´ê³  ë‹¤ìŒì„ ì •ë¦¬í•˜ì„¸ìš”:\n"
                "1. ì—ëŸ¬ ê·¼ë³¸ ì›ì¸ ì¶”ì • (ê°€ëŠ¥í•œ ì›ì¸ 3ê°€ì§€)\n"
                "2. ìˆ˜ì • ìš°ì„ ìˆœìœ„ (ê°€ì¥ ì‹œê¸‰í•œ ê²ƒë¶€í„°)\n"
                "3. êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²• ì œì•ˆ\n"
                "í•œêµ­ì–´ë¡œ, ë¹„ê°œë°œìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”."
            ),
            user_prompt=result,
        )

        return f"{result}\n\n---\n\n## ì›ì¸ ë¶„ì„\n\n{analysis}"

    async def _top_errors(self, kwargs: dict[str, Any]) -> str:
        """ê°€ì¥ ë§ì´ ë°œìƒí•˜ëŠ” ì—ëŸ¬ Top N."""
        log_file = kwargs.get("log_file", DEFAULT_LOG_FILE)
        top_n = int(kwargs.get("top_n", 10))
        hours = int(kwargs.get("hours", 24))

        # ë¡œê·¸ íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        self._ensure_log_file(log_file)

        entries = self._parse_log_file(log_file, "ERROR", hours)
        if not entries:
            return f"ìµœê·¼ {hours}ì‹œê°„ ë‚´ ERROR ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. (íŒŒì¼: {log_file})"

        msg_patterns = Counter(self._normalize_message(e.message) for e in entries)

        lines = [f"## ì—ëŸ¬ ë¹ˆë„ Top {top_n} (ìµœê·¼ {hours}ì‹œê°„)", ""]
        for rank, (pattern, cnt) in enumerate(msg_patterns.most_common(top_n), 1):
            lines.append(f"{rank}. **[{cnt:,}ê±´]** {pattern[:150]}")

        result = "\n".join(lines)

        analysis = await self._llm_call(
            system_prompt=(
                "ë‹¹ì‹ ì€ ì‹œìŠ¤í…œ ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
                "ìì£¼ ë°œìƒí•˜ëŠ” ì—ëŸ¬ ëª©ë¡ì„ ë³´ê³  ë‹¤ìŒì„ ì •ë¦¬í•˜ì„¸ìš”:\n"
                "1. ê° ì—ëŸ¬ì˜ ê°€ëŠ¥í•œ ì›ì¸\n"
                "2. í•´ê²° ìš°ì„ ìˆœìœ„ (ë¹ˆë„ì™€ ì‹¬ê°ë„ ê³ ë ¤)\n"
                "3. êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²•\n"
                "í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."
            ),
            user_prompt=result,
        )

        return f"{result}\n\n---\n\n## ë¶„ì„\n\n{analysis}"

    def _timeline(self, kwargs: dict[str, Any]) -> str:
        """ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ë°œìƒ ë¹ˆë„ë¥¼ í…ìŠ¤íŠ¸ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œí˜„í•©ë‹ˆë‹¤."""
        log_file = kwargs.get("log_file", DEFAULT_LOG_FILE)
        hours = int(kwargs.get("hours", 24))

        # ë¡œê·¸ íŒŒì¼/ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
        self._ensure_log_file(log_file)

        entries = self._parse_log_file(log_file, "ERROR", hours)
        if not entries:
            return f"ìµœê·¼ {hours}ì‹œê°„ ë‚´ ERROR ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. (íŒŒì¼: {log_file})"

        # ì‹œê°„ëŒ€ë³„ ì§‘ê³„
        hour_counts: dict[int, int] = defaultdict(int)
        for e in entries:
            hour_counts[e.timestamp.hour] += 1

        max_count = max(hour_counts.values()) if hour_counts else 1
        bar_max = 30  # ìµœëŒ€ ë§‰ëŒ€ ê¸¸ì´

        lines = [
            f"## ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ë¹ˆë„ (ìµœê·¼ {hours}ì‹œê°„)",
            f"ì´ ì—ëŸ¬: {len(entries):,}ê±´",
            "",
        ]

        for h in range(24):
            cnt = hour_counts.get(h, 0)
            bar_len = int((cnt / max_count) * bar_max) if max_count > 0 else 0
            bar = "â–ˆ" * bar_len
            lines.append(f"{h:02d}ì‹œ: {bar} ({cnt}ê±´)")

        # í”¼í¬ ì‹œê°„ëŒ€
        if hour_counts:
            peak_hour = max(hour_counts, key=hour_counts.get)  # type: ignore[arg-type]
            lines.append(f"\nâš ï¸ í”¼í¬ ì‹œê°„ëŒ€: {peak_hour:02d}ì‹œ ({hour_counts[peak_hour]}ê±´)")

        return "\n".join(lines)

    # â”€â”€ DB í™œë™ ë¡œê·¸ ë¶„ì„ â”€â”€

    @staticmethod
    def _get_activity_logs(
        agent_id: str | None = None,
        level: str | None = None,
        keyword: str | None = None,
        limit: int = 200,
        hours: int | None = None,
    ) -> list[dict]:
        """DB activity_logs í…Œì´ë¸”ì—ì„œ ë¡œê·¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            from web.db import get_connection
        except ImportError:
            try:
                import sys
                from pathlib import Path as _P
                sys.path.insert(0, str(_P(__file__).resolve().parents[2]))
                from web.db import get_connection
            except ImportError:
                return []

        conn = get_connection()
        try:
            query = (
                "SELECT agent_id, message, level, time, timestamp, created_at "
                "FROM activity_logs"
            )
            conditions: list[str] = []
            params: list[Any] = []

            if agent_id:
                conditions.append("agent_id = ?")
                params.append(agent_id)
            if level:
                conditions.append("level = ?")
                params.append(level.lower())
            if hours:
                cutoff_ms = int(
                    (datetime.now(KST) - timedelta(hours=hours)).timestamp() * 1000
                )
                conditions.append("timestamp >= ?")
                params.append(cutoff_ms)

            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            query += " ORDER BY timestamp DESC LIMIT ?"
            params.append(limit)

            rows = conn.execute(query, params).fetchall()
            results = [dict(r) for r in rows]

            # í‚¤ì›Œë“œ í•„í„° (SQL LIKEë³´ë‹¤ ìœ ì—°í•œ Python í•„í„°)
            if keyword:
                kw_lower = keyword.lower()
                results = [r for r in results if kw_lower in r.get("message", "").lower()]

            return results
        finally:
            conn.close()

    async def _activity_logs(self, kwargs: dict[str, Any]) -> str:
        """DB í™œë™ ë¡œê·¸ë¥¼ ì¡°íšŒí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤."""
        agent_id = kwargs.get("agent_id")
        level = kwargs.get("level")
        keyword = kwargs.get("keyword")
        limit = int(kwargs.get("limit", 100))
        hours = int(kwargs.get("hours", 24)) if kwargs.get("hours") else None

        logs = self._get_activity_logs(
            agent_id=agent_id, level=level, keyword=keyword,
            limit=limit, hours=hours,
        )

        if not logs:
            filter_desc = []
            if agent_id:
                filter_desc.append(f"ì—ì´ì „íŠ¸={agent_id}")
            if level:
                filter_desc.append(f"ë ˆë²¨={level}")
            if keyword:
                filter_desc.append(f"í‚¤ì›Œë“œ={keyword}")
            if hours:
                filter_desc.append(f"ìµœê·¼ {hours}ì‹œê°„")
            return f"í™œë™ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. (í•„í„°: {', '.join(filter_desc) or 'ì—†ìŒ'})"

        # ë ˆë²¨ë³„ ê±´ìˆ˜
        level_counts = Counter(log.get("level", "info") for log in logs)

        # ì—ì´ì „íŠ¸ë³„ ê±´ìˆ˜
        agent_counts = Counter(log.get("agent_id", "unknown") for log in logs)

        lines = [
            "## í™œë™ ë¡œê·¸ ë¶„ì„",
            f"ì¡°íšŒ ê±´ìˆ˜: {len(logs):,}ê±´",
        ]
        if agent_id:
            lines.append(f"ì—ì´ì „íŠ¸ í•„í„°: {agent_id}")
        if keyword:
            lines.append(f"í‚¤ì›Œë“œ í•„í„°: {keyword}")
        if hours:
            lines.append(f"ê¸°ê°„: ìµœê·¼ {hours}ì‹œê°„")

        lines.append("\n### ë ˆë²¨ë³„ ê±´ìˆ˜")
        for lvl in ["error", "warning", "info"]:
            cnt = level_counts.get(lvl, 0)
            if cnt > 0:
                emoji = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(lvl, "âšª")
                lines.append(f"  {emoji} {lvl}: {cnt:,}ê±´")

        if len(agent_counts) > 1:
            lines.append("\n### ì—ì´ì „íŠ¸ë³„ ê±´ìˆ˜")
            for aid, cnt in agent_counts.most_common(10):
                lines.append(f"  {aid}: {cnt:,}ê±´")

        # ìµœê·¼ ë¡œê·¸ ëª©ë¡ (ìµœëŒ€ 30ê±´)
        lines.append(f"\n### ìµœê·¼ ë¡œê·¸ (ìµœëŒ€ 30ê±´)")
        for log in logs[:30]:
            lvl_icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(
                log.get("level", ""), "âšª"
            )
            lines.append(
                f"  {lvl_icon} [{log.get('time', '')}] "
                f"({log.get('agent_id', '')}) {log.get('message', '')[:120]}"
            )

        result = "\n".join(lines)

        # LLM ë¶„ì„
        analysis = await self._llm_call(
            system_prompt=(
                "ë‹¹ì‹ ì€ CORTHEX HQ ì‹œìŠ¤í…œ ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
                "í™œë™ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì •ë¦¬í•˜ì„¸ìš”:\n"
                "1. ì „ì²´ íë¦„ ìš”ì•½ (ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€)\n"
                "2. ì—ëŸ¬ë‚˜ ê²½ê³ ê°€ ìˆë‹¤ë©´ ì›ì¸ ì¶”ì •\n"
                "3. ê°œì„  ì œì•ˆ\n"
                "í•œêµ­ì–´ë¡œ, CEO(ë¹„ê°œë°œì)ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”."
            ),
            user_prompt=result,
        )

        return f"{result}\n\n---\n\n## AI ë¶„ì„\n\n{analysis}"

    async def _trading_logs(self, kwargs: dict[str, Any]) -> str:
        """ìë™ë§¤ë§¤ ê´€ë ¨ í™œë™ ë¡œê·¸ë§Œ í•„í„°í•˜ì—¬ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤."""
        hours = int(kwargs.get("hours", 24))
        limit = int(kwargs.get("limit", 200))

        # CIO ì—ì´ì „íŠ¸ + ì‹œìŠ¤í…œì˜ ë§¤ë§¤ ê´€ë ¨ ë¡œê·¸ ìˆ˜ì§‘
        cio_logs = self._get_activity_logs(
            agent_id="cio_manager", limit=limit, hours=hours,
        )
        system_trading_logs = self._get_activity_logs(
            agent_id="system", keyword="ë§¤ë§¤", limit=limit, hours=hours,
        )
        system_trading_logs += self._get_activity_logs(
            agent_id="system", keyword="trading", limit=limit, hours=hours,
        )

        # ì¤‘ë³µ ì œê±° (timestamp ê¸°ì¤€)
        seen_ts = set()
        all_logs = []
        for log in cio_logs + system_trading_logs:
            ts = log.get("timestamp", 0)
            if ts not in seen_ts:
                seen_ts.add(ts)
                all_logs.append(log)

        # ì‹œê°„ìˆœ ì •ë ¬ (ì˜¤ë˜ëœ ìˆœ â†’ íë¦„ íŒŒì•… ìš©ì´)
        all_logs.sort(key=lambda x: x.get("timestamp", 0))

        if not all_logs:
            return f"ìµœê·¼ {hours}ì‹œê°„ ë‚´ ìë™ë§¤ë§¤ ê´€ë ¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ë¶„ë¥˜
        errors = [l for l in all_logs if l.get("level") == "error"]
        warnings = [l for l in all_logs if l.get("level") == "warning"]
        orders = [l for l in all_logs if any(
            kw in l.get("message", "") for kw in ["KIS ì£¼ë¬¸", "ë§¤ìˆ˜ ì„±ê³µ", "ë§¤ë„ ì„±ê³µ", "ì£¼ë¬¸ ì‹¤íŒ¨", "ì£¼ë¬¸ ì „ì†¡"]
        )]
        skipped = [l for l in all_logs if "ê±´ë„ˆëœ€" in l.get("message", "") or "ë¶€ì¡±" in l.get("message", "")]
        analysis_starts = [l for l in all_logs if "ë¶„ì„ ì‹œì‘" in l.get("message", "")]

        lines = [
            f"## ìë™ë§¤ë§¤ ë¡œê·¸ ë¶„ì„ (ìµœê·¼ {hours}ì‹œê°„)",
            f"ì „ì²´ ë¡œê·¸: {len(all_logs):,}ê±´",
            f"  - ğŸ”´ ì—ëŸ¬: {len(errors)}ê±´",
            f"  - ğŸŸ¡ ê²½ê³ : {len(warnings)}ê±´",
            f"  - ğŸ“Š ë¶„ì„ ì‹œì‘: {len(analysis_starts)}ê±´",
            f"  - ğŸ¯ ì£¼ë¬¸ ì‹œë„: {len(orders)}ê±´",
            f"  - â­ï¸ ê±´ë„ˆëœ€: {len(skipped)}ê±´",
        ]

        if errors:
            lines.append("\n### ğŸ”´ ì—ëŸ¬ ëª©ë¡ (ë§¤ë§¤ ì‹¤íŒ¨ ì›ì¸)")
            for log in errors:
                lines.append(
                    f"  [{log.get('time', '')}] {log.get('message', '')[:150]}"
                )

        if warnings:
            lines.append("\n### ğŸŸ¡ ê²½ê³  ëª©ë¡")
            for log in warnings[:10]:
                lines.append(
                    f"  [{log.get('time', '')}] {log.get('message', '')[:150]}"
                )

        if skipped:
            lines.append("\n### â­ï¸ ê±´ë„ˆë›´ ì‹œê·¸ë„ (ì™œ ë§¤ë§¤ê°€ ì•ˆ ëëŠ”ì§€)")
            for log in skipped:
                lines.append(
                    f"  [{log.get('time', '')}] {log.get('message', '')[:150]}"
                )

        if orders:
            lines.append("\n### ğŸ¯ ì‹¤ì œ ì£¼ë¬¸ ë‚´ì—­")
            for log in orders:
                lines.append(
                    f"  [{log.get('time', '')}] {log.get('message', '')[:150]}"
                )

        # ì „ì²´ ì‹œê°„ìˆœ íë¦„ (ìµœëŒ€ 50ê±´)
        lines.append(f"\n### ğŸ“‹ ì „ì²´ íë¦„ (ì‹œê°„ìˆœ, ìµœëŒ€ 50ê±´)")
        for log in all_logs[:50]:
            lvl_icon = {"error": "ğŸ”´", "warning": "ğŸŸ¡", "info": "ğŸ”µ"}.get(
                log.get("level", ""), "âšª"
            )
            lines.append(
                f"  {lvl_icon} [{log.get('time', '')}] {log.get('message', '')[:120]}"
            )

        result = "\n".join(lines)

        # LLM ë¶„ì„ â€” ë§¤ë§¤ ì‹¤íŒ¨ ì›ì¸ íŠ¹í™”
        analysis = await self._llm_call(
            system_prompt=(
                "ë‹¹ì‹ ì€ CORTHEX HQ ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
                "ì•„ë˜ ë§¤ë§¤ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì •ë¦¬í•˜ì„¸ìš”:\n"
                "1. **ë§¤ë§¤ê°€ ì‹¤í–‰ëëŠ”ì§€ ì—¬ë¶€** â€” ì‹¤ì œ ì£¼ë¬¸ì´ ë‚˜ê°”ëŠ”ì§€\n"
                "2. **ì‹¤íŒ¨ ì›ì¸** â€” ì™œ ë§¤ë§¤ê°€ ì•ˆ ëëŠ”ì§€ (ì—ëŸ¬, ì‹ ë¢°ë„ ë¶€ì¡±, KIS ë¯¸ì—°ê²° ë“±)\n"
                "3. **íë¦„ ì¬êµ¬ì„±** â€” ë²„íŠ¼ í´ë¦­ â†’ ë¶„ì„ â†’ ì‹œê·¸ë„ â†’ ì£¼ë¬¸ê¹Œì§€ ì–´ë””ì„œ ëŠê²¼ëŠ”ì§€\n"
                "4. **í•´ê²° ë°©ë²•** â€” êµ¬ì²´ì  ì¡°ì¹˜ ì‚¬í•­\n"
                "í•œêµ­ì–´ë¡œ, CEO(ë¹„ê°œë°œì)ë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n"
                "ê¸°ìˆ  ìš©ì–´ëŠ” ê´„í˜¸ ì•ˆì— ì„¤ëª…ì„ ë„£ìœ¼ì„¸ìš”."
            ),
            user_prompt=result,
        )

        return f"{result}\n\n---\n\n## AI ì§„ë‹¨\n\n{analysis}"
