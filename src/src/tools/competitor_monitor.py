"""
ê²½ìŸì‚¬ ì›¹ì‚¬ì´íŠ¸ ë³€í™” ê°ì§€ê¸° Tool.

ê²½ìŸì‚¬ ì›¹ì‚¬ì´íŠ¸ì˜ ë³€ê²½ì‚¬í•­ì„ ìžë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì•Œë ¤ì¤ë‹ˆë‹¤.
ê°ì‹œ ëŒ€ìƒ URLì„ ë“±ë¡í•˜ë©´, ì´ì „ ìŠ¤ëƒ…ìƒ·ê³¼ í˜„ìž¬ ìƒíƒœë¥¼ ë¹„êµí•˜ì—¬
ë³€ê²½ ì—¬ë¶€ë¥¼ íŒë³„í•˜ê³  ë³€ê²½ ë‚´ìš©ì˜ ì‚¬ì—…ì  ì˜ë¯¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

ì‚¬ìš© ë°©ë²•:
  - action="add": ê°ì‹œ ëŒ€ìƒ ì›¹ì‚¬ì´íŠ¸ ì¶”ê°€
  - action="remove": ê°ì‹œ ëŒ€ìƒ í•´ì œ
  - action="check": ë“±ë¡ëœ ëª¨ë“  ì‚¬ì´íŠ¸ ë³€ê²½ì‚¬í•­ í™•ì¸
  - action="list": í˜„ìž¬ ê°ì‹œ ì¤‘ì¸ ì‚¬ì´íŠ¸ ëª©ë¡
  - action="diff": íŠ¹ì • ì‚¬ì´íŠ¸ì˜ ì´ì „/í˜„ìž¬ ì°¨ì´ì  ìƒì„¸ ë³´ê¸°

í•„ìš” í™˜ê²½ë³€ìˆ˜: ì—†ìŒ
"""
from __future__ import annotations

import hashlib
import json
import logging
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Any

import httpx

from src.tools.base import BaseTool

logger = logging.getLogger("corthex.tools.competitor_monitor")

WATCHLIST_PATH = Path("data/competitor_watchlist.json")  # ë ˆê±°ì‹œ â€” ë§ˆì´ê·¸ë ˆì´ì…˜ìš©
SNAPSHOT_DIR = Path("data/competitor_snapshots")
_WATCHLIST_KEY = "competitor_watchlist"

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)


class CompetitorMonitorTool(BaseTool):
    """ê²½ìŸì‚¬ ì›¹ì‚¬ì´íŠ¸ ë³€ê²½ì‚¬í•­ ìžë™ ê°ì§€ ë„êµ¬."""

    async def execute(self, **kwargs: Any) -> str:
        action = kwargs.get("action", "list")

        if action == "add":
            return await self._add_site(kwargs)
        elif action == "remove":
            return self._remove_site(kwargs)
        elif action == "check":
            return await self._check_all(kwargs)
        elif action == "list":
            return self._list_sites()
        elif action == "diff":
            return await self._show_diff(kwargs)
        else:
            return (
                f"ì•Œ ìˆ˜ ì—†ëŠ” action: {action}. "
                "add, remove, check, list, diff ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            )

    # â”€â”€ ë‚´ë¶€: ê°ì‹œ ëª©ë¡ ê´€ë¦¬ (SQLite DB) â”€â”€

    def _load_watchlist(self) -> list[dict]:
        """ê°ì‹œ ëª©ë¡ì„ DBì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤. DBì— ì—†ìœ¼ë©´ ë ˆê±°ì‹œ JSON ë§ˆì´ê·¸ë ˆì´ì…˜."""
        try:
            from web.db import load_setting
            result = load_setting(_WATCHLIST_KEY, None)
            if result is not None:
                return result
        except Exception:
            pass
        # ë ˆê±°ì‹œ JSON ë§ˆì´ê·¸ë ˆì´ì…˜
        if WATCHLIST_PATH.exists():
            try:
                data = json.loads(WATCHLIST_PATH.read_text(encoding="utf-8"))
                self._save_watchlist(data)  # DBë¡œ ì´ì „
                return data
            except (json.JSONDecodeError, OSError):
                pass
        return []

    def _save_watchlist(self, watchlist: list[dict]) -> None:
        """ê°ì‹œ ëª©ë¡ì„ DBì— ì €ìž¥í•©ë‹ˆë‹¤."""
        try:
            from web.db import save_setting
            save_setting(_WATCHLIST_KEY, watchlist)
        except Exception as e:
            logger.warning("ê²½ìŸì‚¬ ê°ì‹œ ëª©ë¡ DB ì €ìž¥ ì‹¤íŒ¨: %s", e)

    @staticmethod
    def _url_hash(url: str) -> str:
        return hashlib.md5(url.encode()).hexdigest()[:12]

    # â”€â”€ action: add â”€â”€

    async def _add_site(self, kwargs: dict[str, Any]) -> str:
        url = kwargs.get("url", "").strip()
        name = kwargs.get("name", "").strip()
        selector = kwargs.get("selector", "").strip()

        if not url:
            return "ê°ì‹œí•  URLì„ ìž…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: url='https://www.megastudy.net'"
        if not name:
            return "ê²½ìŸì‚¬ ì´ë¦„(name)ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”. ì˜ˆ: name='ë©”ê°€ë¡œìŠ¤ì¿¨'"

        watchlist = self._load_watchlist()

        # ì´ë¯¸ ë“±ë¡ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸
        for item in watchlist:
            if item["url"] == url:
                return f"ì´ë¯¸ ê°ì‹œ ì¤‘ì¸ URLìž…ë‹ˆë‹¤: {url} ({item['name']})"

        # ì´ˆê¸° ìŠ¤ëƒ…ìƒ· ì €ìž¥
        text = await self._fetch_page_text(url, selector)
        if text is None:
            return f"URLì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}\nURLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."

        text_hash = hashlib.md5(text.encode()).hexdigest()
        now = datetime.now().isoformat()

        # ìŠ¤ëƒ…ìƒ· íŒŒì¼ ì €ìž¥
        self._save_snapshot(url, text, now)

        entry = {
            "url": url,
            "name": name,
            "selector": selector,
            "last_hash": text_hash,
            "last_check": now,
            "last_text": text[:5000],
        }
        watchlist.append(entry)
        self._save_watchlist(watchlist)

        return (
            f"## ê°ì‹œ ëŒ€ìƒ ì¶”ê°€ ì™„ë£Œ\n\n"
            f"- **ê²½ìŸì‚¬**: {name}\n"
            f"- **URL**: {url}\n"
            f"- **ê°ì‹œ ì˜ì—­**: {selector if selector else 'ì „ì²´ íŽ˜ì´ì§€'}\n"
            f"- **ë“±ë¡ ì‹œê°**: {now[:19]}\n"
            f"- **ì´ˆê¸° ìŠ¤ëƒ…ìƒ·**: ì €ìž¥ ì™„ë£Œ (í…ìŠ¤íŠ¸ {len(text)}ìž)"
        )

    # â”€â”€ action: remove â”€â”€

    def _remove_site(self, kwargs: dict[str, Any]) -> str:
        url = kwargs.get("url", "").strip()
        if not url:
            return "í•´ì œí•  URLì„ ìž…ë ¥í•´ì£¼ì„¸ìš”."

        watchlist = self._load_watchlist()
        new_list = [item for item in watchlist if item["url"] != url]

        if len(new_list) == len(watchlist):
            return f"ê°ì‹œ ëª©ë¡ì— ì—†ëŠ” URLìž…ë‹ˆë‹¤: {url}"

        self._save_watchlist(new_list)
        return f"ê°ì‹œ í•´ì œ ì™„ë£Œ: {url}"

    # â”€â”€ action: list â”€â”€

    def _list_sites(self) -> str:
        watchlist = self._load_watchlist()
        if not watchlist:
            return (
                "í˜„ìž¬ ê°ì‹œ ì¤‘ì¸ ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
                "action='add'ë¡œ ê°ì‹œ ëŒ€ìƒì„ ì¶”ê°€í•˜ì„¸ìš”."
            )

        lines = ["## í˜„ìž¬ ê°ì‹œ ì¤‘ì¸ ì‚¬ì´íŠ¸ ëª©ë¡\n"]
        for i, item in enumerate(watchlist, 1):
            selector_info = item.get("selector") or "ì „ì²´ íŽ˜ì´ì§€"
            last_check = item.get("last_check", "")[:19]
            lines.append(
                f"**{i}. {item['name']}**\n"
                f"   - URL: {item['url']}\n"
                f"   - ê°ì‹œ ì˜ì—­: {selector_info}\n"
                f"   - ë§ˆì§€ë§‰ í™•ì¸: {last_check}"
            )

        lines.append(f"\nì´ {len(watchlist)}ê°œ ì‚¬ì´íŠ¸ ê°ì‹œ ì¤‘")
        return "\n\n".join(lines)

    # â”€â”€ action: check â”€â”€

    async def _check_all(self, kwargs: dict[str, Any]) -> str:
        watchlist = self._load_watchlist()
        if not watchlist:
            return "ê°ì‹œ ì¤‘ì¸ ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. action='add'ë¡œ ì¶”ê°€í•˜ì„¸ìš”."

        results = []
        changed_count = 0

        for item in watchlist:
            url = item["url"]
            name = item["name"]
            selector = item.get("selector", "")
            old_hash = item.get("last_hash", "")

            text = await self._fetch_page_text(url, selector)
            if text is None:
                results.append(f"- **{name}**: âš  ì ‘ê·¼ ì‹¤íŒ¨ ({url})")
                continue

            new_hash = hashlib.md5(text.encode()).hexdigest()
            now = datetime.now().isoformat()

            if new_hash != old_hash:
                changed_count += 1
                results.append(f"- **{name}**: ðŸ”´ ë³€ê²½ ê°ì§€! ({url})")

                # ìŠ¤ëƒ…ìƒ· ì €ìž¥ + ê°ì‹œ ëª©ë¡ ê°±ì‹ 
                self._save_snapshot(url, text, now)
                item["last_hash"] = new_hash
                item["last_check"] = now
                item["last_text"] = text[:5000]
            else:
                results.append(f"- **{name}**: ë³€ê²½ ì—†ìŒ ({url})")
                item["last_check"] = now

        self._save_watchlist(watchlist)

        summary = (
            f"## ê²½ìŸì‚¬ ì›¹ì‚¬ì´íŠ¸ ë³€ê²½ ê°ì§€ ê²°ê³¼\n\n"
            f"í™•ì¸ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n"
            f"ê°ì‹œ ëŒ€ìƒ: {len(watchlist)}ê°œ | ë³€ê²½ ê°ì§€: {changed_count}ê°œ\n\n"
            + "\n".join(results)
        )

        # ë³€ê²½ì´ ìžˆìœ¼ë©´ LLM ë¶„ì„
        if changed_count > 0:
            analysis = await self._llm_call(
                system_prompt=(
                    "ë‹¹ì‹ ì€ ê²½ìŸì‚¬ ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.\n"
                    "ê²½ìŸì‚¬ ì›¹ì‚¬ì´íŠ¸ì˜ ë³€ê²½ ê°ì§€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì •ë¦¬í•˜ì„¸ìš”:\n"
                    "1. ì–´ë–¤ ë³€ê²½ì´ ì‚¬ì—…ì ìœ¼ë¡œ ì˜ë¯¸ ìžˆëŠ”ì§€\n"
                    "2. ê²½ìŸì‚¬ì˜ ì „ëžµ ë³€í™” ê°€ëŠ¥ì„±\n"
                    "3. ìš°ë¦¬ê°€ ëŒ€ì‘í•´ì•¼ í•  ì‚¬í•­\n"
                    "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë³´ê³ í•˜ì„¸ìš”."
                ),
                user_prompt=summary,
            )
            summary += f"\n\n---\n\n## ë¶„ì„\n\n{analysis}"

        return summary

    # â”€â”€ action: diff â”€â”€

    async def _show_diff(self, kwargs: dict[str, Any]) -> str:
        url = kwargs.get("url", "").strip()
        if not url:
            return "ë¹„êµí•  ì‚¬ì´íŠ¸ URLì„ ìž…ë ¥í•´ì£¼ì„¸ìš”."

        watchlist = self._load_watchlist()
        entry = next((item for item in watchlist if item["url"] == url), None)
        if not entry:
            return f"ê°ì‹œ ëª©ë¡ì— ì—†ëŠ” URLìž…ë‹ˆë‹¤: {url}"

        selector = entry.get("selector", "")
        old_text = entry.get("last_text", "")

        # í˜„ìž¬ íŽ˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        new_text = await self._fetch_page_text(url, selector)
        if new_text is None:
            return f"í˜„ìž¬ íŽ˜ì´ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}"

        if not old_text:
            return "ì´ì „ ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤. action='check'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."

        # difflibë¡œ ì°¨ì´ì  ìƒì„±
        import difflib

        old_lines = old_text.splitlines()
        new_lines = new_text[:5000].splitlines()

        diff = list(difflib.unified_diff(
            old_lines, new_lines,
            fromfile="ì´ì „ ë²„ì „",
            tofile="í˜„ìž¬ ë²„ì „",
            lineterm="",
        ))

        if not diff:
            return f"**{entry['name']}**: ì´ì „ ìŠ¤ëƒ…ìƒ·ê³¼ ë™ì¼í•©ë‹ˆë‹¤. ë³€ê²½ ì‚¬í•­ ì—†ìŒ."

        diff_text = "\n".join(diff[:200])  # ìµœëŒ€ 200ì¤„

        # LLM ë¶„ì„
        analysis = await self._llm_call(
            system_prompt=(
                "ë‹¹ì‹ ì€ ê²½ìŸì‚¬ ë¶„ì„ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.\n"
                "ì›¹ì‚¬ì´íŠ¸ì˜ ì´ì „/í˜„ìž¬ ì°¨ì´ì (diff)ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì •ë¦¬í•˜ì„¸ìš”:\n"
                "1. ì£¼ìš” ë³€ê²½ ë‚´ìš© ìš”ì•½\n"
                "2. ë³€ê²½ì˜ ì‚¬ì—…ì  ì˜ë¯¸ (ê°€ê²© ë³€ë™, ì‹ ì œí’ˆ, ì´ë²¤íŠ¸ ë“±)\n"
                "3. ëŒ€ì‘ ì „ëžµ ì œì•ˆ\n"
                "í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë³´ê³ í•˜ì„¸ìš”."
            ),
            user_prompt=(
                f"ê²½ìŸì‚¬: {entry['name']}\n"
                f"URL: {url}\n\n"
                f"ë³€ê²½ ë‚´ì—­ (diff):\n```\n{diff_text}\n```"
            ),
        )

        return (
            f"## {entry['name']} ë³€ê²½ ìƒì„¸ ë¹„êµ\n\n"
            f"URL: {url}\n\n"
            f"### ë³€ê²½ ë‚´ì—­ (diff)\n```\n{diff_text}\n```\n\n"
            f"---\n\n### ë¶„ì„\n\n{analysis}"
        )

    # â”€â”€ ìœ í‹¸: íŽ˜ì´ì§€ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° â”€â”€

    async def _fetch_page_text(self, url: str, selector: str = "") -> str | None:
        """URLì˜ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜."""
        try:
            from bs4 import BeautifulSoup
        except ImportError:
            logger.warning("beautifulsoup4ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
            return None

        try:
            async with httpx.AsyncClient(follow_redirects=True) as client:
                resp = await client.get(
                    url,
                    headers={"User-Agent": USER_AGENT},
                    timeout=20,
                )
                if resp.status_code != 200:
                    logger.warning("HTTP %d: %s", resp.status_code, url)
                    return None
        except httpx.HTTPError as e:
            logger.warning("HTTP ì˜¤ë¥˜: %s â€” %s", url, e)
            return None

        soup = BeautifulSoup(resp.text, "html.parser")

        # script, style ì œê±°
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()

        if selector:
            target = soup.select_one(selector)
            if target:
                text = target.get_text(separator="\n", strip=True)
            else:
                text = soup.body.get_text(separator="\n", strip=True) if soup.body else ""
        else:
            text = soup.body.get_text(separator="\n", strip=True) if soup.body else ""

        # ê³µë°± ì •ë¦¬
        text = re.sub(r"\n{3,}", "\n\n", text)
        return text.strip()

    # â”€â”€ ìœ í‹¸: ìŠ¤ëƒ…ìƒ· ì €ìž¥ â”€â”€

    def _save_snapshot(self, url: str, text: str, timestamp: str) -> None:
        SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)
        url_h = self._url_hash(url)
        ts = timestamp.replace(":", "").replace("-", "")[:15]
        filename = f"{url_h}_{ts}.txt"
        filepath = SNAPSHOT_DIR / filename
        filepath.write_text(text[:10000], encoding="utf-8")
        logger.debug("ìŠ¤ëƒ…ìƒ· ì €ìž¥: %s", filepath)
