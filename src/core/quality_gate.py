"""
Quality Gate: ë§¤ë‹ˆì €ê¸‰ ì—ì´ì „íŠ¸ê°€ ë¶€í•˜ì˜ ë³´ê³ ì„œë¥¼ ê²€ìˆ˜í•˜ëŠ” ì‹œìŠ¤í…œ.

í•˜ì´ë¸Œë¦¬ë“œ ë£¨ë¸Œë¦­ (2026-02-22 ì¬ì„¤ê³„):
1. ê·œì¹™ ê¸°ë°˜ ì‚¬ì „ í•„í„° (LLM í˜¸ì¶œ ì—†ìŒ, ë¹„ìš© 0ì›)
   - ë¹ˆ ì‘ë‹µ/ì—ëŸ¬/ê¸¸ì´ ë¶€ì¡± â†’ ì¦‰ì‹œ ë¶ˆí•©ê²©
2. LLM í•˜ì´ë¸Œë¦¬ë“œ ê²€ìˆ˜ (ë§¤ë‹ˆì € ìê¸° ëª¨ë¸ 1íšŒ í˜¸ì¶œ)
   - ì²´í¬ë¦¬ìŠ¤íŠ¸: í•„ìˆ˜(required) ì „ë¶€ í†µê³¼ í•„ìš”
   - ì ìˆ˜: 1/3/5 ê°€ì¤‘ í‰ê·  â‰¥ 3.0 í•„ìš”
3. ë¶ˆí•©ê²© ì‹œ ìµœëŒ€ 2íšŒ ì¬ì‘ì—… (í”¼ë“œë°± í¬í•¨)
"""
from __future__ import annotations

import json
import logging
import re
from dataclasses import dataclass, field
from typing import Any, TYPE_CHECKING

import yaml
from pathlib import Path

if TYPE_CHECKING:
    from src.core.quality_rules_manager import QualityRulesManager
    from src.llm.router import ModelRouter

logger = logging.getLogger("corthex.quality_gate")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° êµ¬ì¡°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ChecklistItem:
    """ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© ê²°ê³¼."""
    id: str
    label: str
    passed: bool
    required: bool
    feedback: str = ""


@dataclass
class ScoreItem:
    """ì ìˆ˜ í•­ëª© ê²°ê³¼."""
    id: str
    label: str
    score: int  # 1, 3, 5
    weight: int  # ê°€ì¤‘ì¹˜ (%)
    critical: bool = False  # â˜…ì¹˜ëª…ì  í•­ëª© (1ì ì´ë©´ ì „ì²´ ì ìˆ˜ ì œí•œ)
    feedback: str = ""


@dataclass
class HybridReviewResult:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìˆ˜ ê²°ê³¼ (ì²´í¬ë¦¬ìŠ¤íŠ¸ + ì ìˆ˜)."""
    passed: bool
    checklist_results: list[ChecklistItem] = field(default_factory=list)
    score_results: list[ScoreItem] = field(default_factory=list)
    weighted_average: float = 0.0  # 1~5 ìŠ¤ì¼€ì¼
    feedback: str = ""
    rejection_reasons: list[str] = field(default_factory=list)
    reviewer_id: str = ""
    target_agent_id: str = ""
    review_model: str = ""

    def to_dict(self) -> dict[str, Any]:
        return {
            "passed": self.passed,
            "weighted_average": round(self.weighted_average, 2),
            "feedback": self.feedback,
            "rejection_reasons": self.rejection_reasons,
            "reviewer_id": self.reviewer_id,
            "target_agent_id": self.target_agent_id,
            "review_model": self.review_model,
            "checklist": [
                {"id": c.id, "label": c.label, "passed": c.passed,
                 "required": c.required, "feedback": c.feedback}
                for c in self.checklist_results
            ],
            "scores": [
                {"id": s.id, "label": s.label, "score": s.score,
                 "weight": s.weight, "critical": s.critical,
                 "feedback": s.feedback}
                for s in self.score_results
            ],
        }


@dataclass
class ReviewResult:
    """í’ˆì§ˆ ê²€ìˆ˜ ê²°ê³¼ (ë ˆê±°ì‹œ í˜¸í™˜)."""
    passed: bool
    score: int  # 0~100
    issues: list[str] = field(default_factory=list)
    reviewer_id: str = ""
    target_agent_id: str = ""
    task_description: str = ""
    rejection_reason: str = ""


@dataclass
class QualityStats:
    """í’ˆì§ˆ ê²Œì´íŠ¸ ëˆ„ì  í†µê³„."""
    total_reviewed: int = 0
    total_passed: int = 0
    total_rejected: int = 0
    total_retried: int = 0
    total_retry_passed: int = 0
    rejections_by_agent: dict[str, int] = field(default_factory=dict)
    rejections_by_reviewer: dict[str, int] = field(default_factory=dict)
    recent_rejections: list[dict[str, Any]] = field(default_factory=list)

    @property
    def pass_rate(self) -> float:
        if self.total_reviewed == 0:
            return 100.0
        return round(self.total_passed / self.total_reviewed * 100, 1)

    @property
    def retry_success_rate(self) -> float:
        if self.total_retried == 0:
            return 0.0
        return round(self.total_retry_passed / self.total_retried * 100, 1)

    def to_dict(self) -> dict[str, Any]:
        return {
            "total_reviewed": self.total_reviewed,
            "total_passed": self.total_passed,
            "total_rejected": self.total_rejected,
            "total_retried": self.total_retried,
            "total_retry_passed": self.total_retry_passed,
            "pass_rate": self.pass_rate,
            "retry_success_rate": self.retry_success_rate,
            "rejections_by_agent": self.rejections_by_agent,
            "rejections_by_reviewer": self.rejections_by_reviewer,
            "recent_rejections": self.recent_rejections[-10:],
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ í’ˆì§ˆ ê²Œì´íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class QualityGate:
    """ë§¤ë‹ˆì €ê°€ ë¶€í•˜ ê²°ê³¼ë¥¼ ê²€ìˆ˜í•˜ëŠ” í’ˆì§ˆ ê²Œì´íŠ¸."""

    def __init__(self, config_path: Path | None = None) -> None:
        self.stats = QualityStats()
        self._rules: dict[str, Any] = self._default_rules()
        self._rubrics: dict[str, Any] = {}
        self._common_checklist: dict[str, list[dict]] = {"required": [], "optional": []}
        self._pass_criteria: dict[str, Any] = {
            "all_required_pass": True,
            "min_average_score": 3.0,
        }
        self._rules_manager: QualityRulesManager | None = None
        self._config_path = config_path

        if config_path and config_path.exists():
            self._load_config(config_path)

    def _load_config(self, config_path: Path) -> None:
        """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ."""
        try:
            loaded = yaml.safe_load(config_path.read_text(encoding="utf-8"))
            if not loaded or not isinstance(loaded, dict):
                return
            self._rules.update(loaded.get("rules", {}))
            self._rubrics = loaded.get("rubrics", {})
            self._common_checklist = loaded.get("common_checklist", self._common_checklist)
            self._pass_criteria = loaded.get("pass_criteria", self._pass_criteria)
        except Exception as e:
            logger.warning("í’ˆì§ˆ ê·œì¹™ ë¡œë“œ ì‹¤íŒ¨: %s", e)

    def reload_config(self) -> None:
        """ì„¤ì • íŒŒì¼ì„ ë‹¤ì‹œ ë¡œë“œí•œë‹¤ (ì›¹ UIì—ì„œ ìˆ˜ì • í›„ ë°˜ì˜)."""
        if self._config_path and self._config_path.exists():
            self._load_config(self._config_path)

    @staticmethod
    def _default_rules() -> dict[str, Any]:
        return {
            "min_length": 50,
            "max_retry": 2,
            "check_hallucination": True,
            "check_relevance": True,
        }

    @property
    def max_retry(self) -> int:
        return self._rules.get("max_retry", 2)

    def set_rules_manager(self, manager: QualityRulesManager) -> None:
        """QualityRulesManagerë¥¼ ì—°ê²°í•˜ì—¬ ë™ì  ë£¨ë¸Œë¦­/ëª¨ë¸ ì¡°íšŒë¥¼ í™œì„±í™”í•œë‹¤."""
        self._rules_manager = manager

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë£¨ë¸Œë¦­ ì¡°íšŒ
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_rubric(self, division: str) -> dict[str, Any]:
        """ë¶€ì„œë³„ ë£¨ë¸Œë¦­ ë°˜í™˜. ì—†ìœ¼ë©´ default."""
        if division and division in self._rubrics:
            return self._rubrics[division]
        return self._rubrics.get("default", {})

    def _build_checklist_items(self, division: str) -> list[dict]:
        """ê³µí†µ + ë¶€ì„œë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© ë³‘í•©."""
        items = []
        # ê³µí†µ í•„ìˆ˜
        for item in self._common_checklist.get("required", []):
            items.append({**item, "required": True, "source": "common"})
        # ê³µí†µ ì„ íƒ
        for item in self._common_checklist.get("optional", []):
            items.append({**item, "required": False, "source": "common"})
        # ë¶€ì„œë³„
        rubric = self.get_rubric(division)
        dept_cl = rubric.get("department_checklist", {})
        for item in dept_cl.get("required", []):
            items.append({**item, "required": True, "source": "department"})
        for item in dept_cl.get("optional", []):
            items.append({**item, "required": False, "source": "department"})
        return items

    def _build_scoring_items(self, division: str) -> list[dict]:
        """ë¶€ì„œë³„ ì ìˆ˜ í•­ëª© ë°˜í™˜."""
        rubric = self.get_rubric(division)
        return rubric.get("scoring", [])

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ê·œì¹™ ê¸°ë°˜ ì‚¬ì „ í•„í„°
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def rule_based_check(self, result_data: Any, task_description: str) -> ReviewResult:
        """ê·œì¹™ ê¸°ë°˜ ê²€ì‚¬ (LLM í˜¸ì¶œ ì—†ìŒ)."""
        issues = []
        text = str(result_data or "")

        # ë¹ˆ ì‘ë‹µ
        if not text.strip():
            issues.append("ë¹ˆ ì‘ë‹µ")
            return ReviewResult(
                passed=False, score=0, issues=issues,
                rejection_reason="ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            )

        # ë„ˆë¬´ ì§§ìŒ
        min_len = self._rules.get("min_length", 50)
        if len(text.strip()) < min_len:
            issues.append(f"ì‘ë‹µ ê¸¸ì´ ë¶€ì¡± ({len(text.strip())}ì < {min_len}ì)")

        # ì—ëŸ¬ ì‘ë‹µ
        if isinstance(result_data, dict) and "error" in result_data:
            issues.append("ì—ëŸ¬ ì‘ë‹µ")
            return ReviewResult(
                passed=False, score=10, issues=issues,
                rejection_reason="ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            )

        # ì˜ë¯¸ì—†ëŠ” ë°˜ë³µ íŒ¨í„´
        words = text.split()
        if len(words) > 10:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.3:
                issues.append("ë°˜ë³µ íŒ¨í„´ ê°ì§€")

        if issues:
            score = max(0, 60 - len(issues) * 20)
            return ReviewResult(
                passed=False, score=score, issues=issues,
                rejection_reason=" / ".join(issues) + ". ë³´ì™„í•˜ì—¬ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.",
            )

        return ReviewResult(passed=True, score=80, issues=[])

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í•˜ì´ë¸Œë¦¬ë“œ LLM ê²€ìˆ˜ (í•µì‹¬)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def hybrid_review(
        self,
        result_data: Any,
        task_description: str,
        model_router: "ModelRouter",
        reviewer_id: str = "",
        reviewer_model: str = "",
        division: str = "",
        target_agent_id: str = "",
    ) -> HybridReviewResult:
        """í•˜ì´ë¸Œë¦¬ë“œ í’ˆì§ˆ ê²€ìˆ˜: ì²´í¬ë¦¬ìŠ¤íŠ¸ + 1/3/5 ì ìˆ˜."""

        text = str(result_data or "")

        # 1. ê·œì¹™ ê¸°ë°˜ ì‚¬ì „ í•„í„°
        pre_check = self.rule_based_check(result_data, task_description)
        if not pre_check.passed:
            return HybridReviewResult(
                passed=False,
                weighted_average=0.0,
                feedback=pre_check.rejection_reason,
                rejection_reasons=pre_check.issues,
                reviewer_id=reviewer_id,
                target_agent_id=target_agent_id,
                review_model="rule_based",
            )

        # 2. ì²´í¬ë¦¬ìŠ¤íŠ¸ + ì ìˆ˜ í•­ëª© êµ¬ì„±
        checklist_items = self._build_checklist_items(division)
        scoring_items = self._build_scoring_items(division)

        # 3. LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_hybrid_prompt(
            task_description, text[:3000], checklist_items, scoring_items
        )

        # 4. ë§¤ë‹ˆì € ìê¸° ëª¨ë¸ë¡œ í˜¸ì¶œ
        model_to_use = reviewer_model or "claude-sonnet-4-6"
        try:
            response = await model_router.complete(
                model_name=model_to_use,
                messages=[
                    {"role": "system", "content": (
                        "ë‹¹ì‹ ì€ ë³´ê³ ì„œ í’ˆì§ˆ ê²€ìˆ˜ê´€ì…ë‹ˆë‹¤. "
                        "ë°˜ë“œì‹œ ìš”ì²­ëœ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. "
                        "ë³´ê³ ì„œì— ë„êµ¬(dart_api, kr_stock, ecos_macro, us_stock ë“±)ë¡œ "
                        "ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš°, í•´ë‹¹ ìˆ˜ì¹˜ëŠ” ì‹¤ì‹œê°„ APIì—ì„œ "
                        "ê²€ì¦ëœ ì •í™•í•œ ë°ì´í„°ì´ë¯€ë¡œ 'í™•ì¸ ë¶ˆê°€'ë¡œ ê°ì í•˜ì§€ ë§ˆì„¸ìš”."
                    )},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.0,
                agent_id=reviewer_id or "quality_gate",
            )
            result = self._parse_hybrid_response(
                response.content, checklist_items, scoring_items,
                reviewer_id, target_agent_id, model_to_use,
            )
            logger.info(
                "[QA] %s â†’ %s | ëª¨ë¸=%s | ì ìˆ˜=%.1f | %s",
                reviewer_id, target_agent_id, model_to_use,
                result.weighted_average,
                "í•©ê²©" if result.passed else "ë¶ˆí•©ê²©",
            )
            return result

        except Exception as e:
            logger.warning("[QA] LLM ê²€ìˆ˜ ì‹¤íŒ¨ (%sâ†’%s): %s", reviewer_id, target_agent_id, e)
            # LLM ì‹¤íŒ¨ ì‹œ í†µê³¼ ì²˜ë¦¬ (ì—…ë¬´ ì°¨ë‹¨ ë°©ì§€)
            return HybridReviewResult(
                passed=True,
                weighted_average=3.0,
                feedback=f"LLM ê²€ìˆ˜ ì‹¤íŒ¨ ({e}). ìë™ í†µê³¼ ì²˜ë¦¬ë¨.",
                reviewer_id=reviewer_id,
                target_agent_id=target_agent_id,
                review_model=model_to_use,
            )

    def _build_hybrid_prompt(
        self,
        task_description: str,
        text: str,
        checklist_items: list[dict],
        scoring_items: list[dict],
    ) -> str:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìˆ˜ í”„ë¡¬í”„íŠ¸ ìƒì„±."""

        # ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒŒíŠ¸
        cl_lines = []
        for item in checklist_items:
            req_tag = "[í•„ìˆ˜]" if item.get("required") else "[ì„ íƒ]"
            cl_lines.append(f"  - {item['id']}: {req_tag} {item['label']}")
        checklist_text = "\n".join(cl_lines)

        # ì ìˆ˜ íŒŒíŠ¸
        sc_lines = []
        for item in scoring_items:
            criteria = item.get("criteria", {})
            c_desc = ", ".join(f"{k}ì ={v}" for k, v in sorted(criteria.items()))
            critical_tag = " â˜…ì¹˜ëª…ì " if item.get("critical") else ""
            sc_lines.append(
                f"  - {item['id']}: {item['label']}{critical_tag} (ê°€ì¤‘ì¹˜ {item.get('weight', 33)}%)\n"
                f"    íŒì • ê¸°ì¤€: {c_desc}"
            )
        scoring_text = "\n".join(sc_lines)

        # ì „ì²´ í•­ëª© ID ëª©ë¡ (JSON ì˜ˆì‹œìš©)
        cl_ids = [item["id"] for item in checklist_items]
        sc_ids = [item["id"] for item in scoring_items]

        cl_example = ", ".join(f'"{cid}": true' for cid in cl_ids)
        sc_example = ", ".join(f'"{sid}": 3' for sid in sc_ids)

        # ë„êµ¬ ì‚¬ìš© ì—¬ë¶€ ê°ì§€ (ë³´ê³ ì„œ í•˜ë‹¨ì— 'ğŸ”§ **ì‚¬ìš©í•œ ë„êµ¬**:' íƒœê·¸ê°€ ìˆìŒ)
        tool_trust_instruction = ""
        if "ì‚¬ìš©í•œ ë„êµ¬" in text:
            tool_trust_instruction = (
                "\n## âš ï¸ ë„êµ¬ ë°ì´í„° ì‹ ë¢° ê·œì¹™\n"
                "ì´ ë³´ê³ ì„œëŠ” ë„êµ¬(dart_api, kr_stock, ecos_macro, us_stock ë“±)ë¥¼ ì‚¬ìš©í•˜ì—¬ "
                "ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ ë³´ê³ ì„œì…ë‹ˆë‹¤.\n"
                "- ë„êµ¬ë¥¼ í†µí•´ ê°€ì ¸ì˜¨ ìˆ˜ì¹˜(ì£¼ê°€, ì¬ë¬´ì œí‘œ, ê±°ì‹œê²½ì œ ì§€í‘œ ë“±)ëŠ” "
                "ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ì¦ëœ ì •í™•í•œ ë°ì´í„°ì´ë¯€ë¡œ, ë°ì´í„° ì •í™•ì„± ì±„ì  ì‹œ "
                "ë„êµ¬ ì‚¬ìš© ë°ì´í„°ëŠ” ì •í™•í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì„¸ìš”.\n"
                "- 'ìˆ˜ì¹˜ í™•ì¸ ë¶ˆê°€', 'ì¶œì²˜ ë¶ˆëª…', 'ê²€ì¦ ë¶ˆê°€'ë¥¼ ì´ìœ ë¡œ ê°ì í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "- ë„êµ¬ê°€ ë°˜í™˜í•œ ë°ì´í„° ìì²´ì˜ ì •í™•ì„±ì´ ì•„ë‹ˆë¼, í•´ë‹¹ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ "
                "ë¶„ì„ì˜ ë…¼ë¦¬ì„±ê³¼ ì™„ì„±ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”.\n"
            )

        return (
            f"## ì—…ë¬´ ì§€ì‹œ\n{task_description}\n\n"
            f"## ì œì¶œëœ ë³´ê³ ì„œ\n{text}\n\n"
            f"{tool_trust_instruction}"
            f"## ì²´í¬ë¦¬ìŠ¤íŠ¸ (í†µê³¼=true, ë¶ˆí†µê³¼=false)\n{checklist_text}\n\n"
            f"## ì ìˆ˜ í•­ëª© (1/3/5 ì¤‘ ì„ íƒ)\n{scoring_text}\n\n"
            "## ì‘ë‹µ í˜•ì‹\n"
            "ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”. JSON ì™¸ í…ìŠ¤íŠ¸ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.\n"
            "```json\n"
            "{\n"
            f'  "checklist": {{{cl_example}}},\n'
            f'  "scores": {{{sc_example}}},\n'
            '  "feedback": "ì¢…í•© í”¼ë“œë°± (1~2ë¬¸ì¥)"\n'
            "}\n"
            "```"
        )

    def _parse_hybrid_response(
        self,
        response: str,
        checklist_items: list[dict],
        scoring_items: list[dict],
        reviewer_id: str,
        target_agent_id: str,
        model_used: str,
    ) -> HybridReviewResult:
        """LLM í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ íŒŒì‹±."""

        parsed = self._extract_json(response)

        if parsed is None:
            logger.warning("[QA] JSON íŒŒì‹± ì‹¤íŒ¨, ì •ê·œì‹ í´ë°± ì‹œë„")
            parsed = self._regex_fallback(response, checklist_items, scoring_items)

        if parsed is None:
            logger.warning("[QA] ì •ê·œì‹ í´ë°±ë„ ì‹¤íŒ¨, ìë™ í†µê³¼ ì²˜ë¦¬")
            return HybridReviewResult(
                passed=True,
                weighted_average=3.0,
                feedback="ê²€ìˆ˜ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨. ìë™ í†µê³¼ ì²˜ë¦¬ë¨.",
                reviewer_id=reviewer_id,
                target_agent_id=target_agent_id,
                review_model=model_used,
            )

        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ê²°ê³¼ êµ¬ì„±
        cl_data = parsed.get("checklist", {})
        checklist_results = []
        for item in checklist_items:
            item_passed = cl_data.get(item["id"], True)
            if isinstance(item_passed, str):
                item_passed = item_passed.lower() in ("true", "yes", "í†µê³¼")
            checklist_results.append(ChecklistItem(
                id=item["id"],
                label=item["label"],
                passed=bool(item_passed),
                required=item.get("required", False),
            ))

        # ì ìˆ˜ ê²°ê³¼ êµ¬ì„±
        sc_data = parsed.get("scores", {})
        score_results = []
        for item in scoring_items:
            raw_score = sc_data.get(item["id"], 3)
            try:
                score_val = int(raw_score)
            except (ValueError, TypeError):
                score_val = 3
            # 1/3/5 ë²”ìœ„ ë³´ì •
            if score_val <= 1:
                score_val = 1
            elif score_val <= 3:
                score_val = 3
            else:
                score_val = 5
            score_results.append(ScoreItem(
                id=item["id"],
                label=item["label"],
                score=score_val,
                weight=item.get("weight", 33),
                critical=item.get("critical", False),
            ))

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_average = self._calc_weighted_average(score_results)

        # â˜… ì¹˜ëª…ì  í•­ëª© ì—°ë™: critical í•­ëª©ì´ 1ì ì´ë©´ ê°€ì¤‘ í‰ê·  ì œí•œ
        critical_cap = self._pass_criteria.get("critical_cap", 2.0)
        critical_failed = [
            s for s in score_results if s.critical and s.score == 1
        ]
        if critical_failed:
            original_avg = weighted_average
            weighted_average = min(weighted_average, critical_cap)
            failed_labels = ", ".join(f"[{s.id}] {s.label}" for s in critical_failed)
            logger.info(
                "[QA] â˜…ì¹˜ëª…ì  í•­ëª© 1ì  ê°ì§€: %s | ê°€ì¤‘í‰ê·  %.1f â†’ %.1f (cap=%.1f)",
                failed_labels, original_avg, weighted_average, critical_cap,
            )

        # í•©ê²© íŒì •
        feedback = parsed.get("feedback", "")
        rejection_reasons = []

        # ì¹˜ëª…ì  í•­ëª© ë¶ˆí•©ê²© ì‚¬ìœ  ì¶”ê°€
        for s in critical_failed:
            rejection_reasons.append(
                f"â˜…ì¹˜ëª…ì  í•­ëª© [{s.id}] {s.label}ì´(ê°€) 1ì  â†’ ì „ì²´ ì ìˆ˜ {critical_cap} ì´í•˜ë¡œ ì œí•œ"
            )

        # í•„ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ í™•ì¸
        all_required_pass = self._pass_criteria.get("all_required_pass", True)
        if all_required_pass:
            for cl in checklist_results:
                if cl.required and not cl.passed:
                    rejection_reasons.append(f"í•„ìˆ˜í•­ëª© ë¶ˆí†µê³¼: [{cl.id}] {cl.label}")

        # ì ìˆ˜ ê¸°ì¤€ í™•ì¸
        min_avg = self._pass_criteria.get("min_average_score", 3.0)
        if weighted_average < min_avg:
            rejection_reasons.append(
                f"ê°€ì¤‘ í‰ê·  {weighted_average:.1f} < ê¸°ì¤€ {min_avg}"
            )

        passed = len(rejection_reasons) == 0

        return HybridReviewResult(
            passed=passed,
            checklist_results=checklist_results,
            score_results=score_results,
            weighted_average=weighted_average,
            feedback=feedback,
            rejection_reasons=rejection_reasons,
            reviewer_id=reviewer_id,
            target_agent_id=target_agent_id,
            review_model=model_used,
        )

    @staticmethod
    def _extract_json(text: str) -> dict | None:
        """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ."""
        # ```json ... ``` ë¸”ë¡ ë¨¼ì € ì‹œë„
        match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass

        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì²« ë²ˆì§¸ { ... } ì‹œë„
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                pass

        return None

    @staticmethod
    def _regex_fallback(
        text: str,
        checklist_items: list[dict],
        scoring_items: list[dict],
    ) -> dict | None:
        """JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ìœ¼ë¡œ ê°’ ì¶”ì¶œ ì‹œë„."""
        result: dict[str, Any] = {"checklist": {}, "scores": {}, "feedback": ""}

        for item in checklist_items:
            # "C1: true" ë˜ëŠ” "C1: í†µê³¼" íŒ¨í„´
            pattern = rf'{item["id"]}\s*[:=]\s*(true|false|í†µê³¼|ë¶ˆí†µê³¼)'
            m = re.search(pattern, text, re.IGNORECASE)
            if m:
                val = m.group(1).lower()
                result["checklist"][item["id"]] = val in ("true", "í†µê³¼")

        for item in scoring_items:
            # "S1: 3" ë˜ëŠ” "S1 = 5" íŒ¨í„´
            pattern = rf'{item["id"]}\s*[:=]\s*(\d)'
            m = re.search(pattern, text)
            if m:
                result["scores"][item["id"]] = int(m.group(1))

        # ìµœì†Œí•œ í•˜ë‚˜ë¼ë„ ì°¾ì•˜ìœ¼ë©´ ì„±ê³µ
        if result["checklist"] or result["scores"]:
            return result
        return None

    @staticmethod
    def _calc_weighted_average(score_results: list[ScoreItem]) -> float:
        """ê°€ì¤‘ í‰ê·  ê³„ì‚° (1~5 ìŠ¤ì¼€ì¼)."""
        if not score_results:
            return 3.0
        total_weight = sum(s.weight for s in score_results)
        if total_weight == 0:
            return 3.0
        weighted_sum = sum(s.score * s.weight for s in score_results)
        return weighted_sum / total_weight

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ë ˆê±°ì‹œ LLM ê²€ìˆ˜ (í˜¸í™˜ìš©)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _get_rubric_prompt(self, division: str) -> str:
        """ë¶€ì„œë³„ ë£¨ë¸Œë¦­ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤ (ë ˆê±°ì‹œ)."""
        if self._rules_manager:
            return self._rules_manager.get_rubric_prompt(division)
        return (
            "ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ê²€ìˆ˜í•˜ì„¸ìš”:\n"
            "1. ê´€ë ¨ì„±: ì—…ë¬´ ì§€ì‹œì— ì‹¤ì œë¡œ ë‹µí•˜ê³  ìˆëŠ”ê°€? (0-30ì )\n"
            "2. êµ¬ì²´ì„±: ë§‰ì—°í•œ ì¼ë°˜ë¡ ì´ ì•„ë‹Œ êµ¬ì²´ì  ë‚´ìš©ì´ ìˆëŠ”ê°€? (0-30ì )\n"
            "3. ì‹ ë¢°ì„±: ì¶œì²˜ ì—†ëŠ” ìˆ«ì, ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ì‹¤ ë“± "
            "í• ë£¨ì‹œë„¤ì´ì…˜ ì§•í›„ê°€ ìˆëŠ”ê°€? (0-40ì )"
        )

    async def llm_review(
        self,
        result_data: Any,
        task_description: str,
        model_router: "ModelRouter",
        reviewer_id: str = "",
        division: str = "",
    ) -> ReviewResult:
        """LLM ê¸°ë°˜ í’ˆì§ˆ ê²€ìˆ˜ (ë ˆê±°ì‹œ â€” hybrid_review ì‚¬ìš© ê¶Œì¥)."""
        text = str(result_data or "")[:3000]

        if self._rules_manager:
            review_model = self._rules_manager.review_model
        else:
            review_model = self._rules.get("review_model", "claude-sonnet-4-6")

        rubric_text = self._get_rubric_prompt(division)

        prompt = (
            "ë‹¹ì‹ ì€ ë³´ê³ ì„œ í’ˆì§ˆ ê²€ìˆ˜ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ë³´ê³ ì„œë¥¼ ê²€ìˆ˜í•˜ì„¸ìš”.\n\n"
            f"## ì›ë˜ ì—…ë¬´ ì§€ì‹œ\n{task_description}\n\n"
            f"## ì œì¶œëœ ë³´ê³ ì„œ\n{text}\n\n"
            f"{rubric_text}\n\n"
            "ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”:\n"
            "ì ìˆ˜: [0-100]\n"
            "í†µê³¼: [ì˜ˆ/ì•„ë‹ˆì˜¤]\n"
            "ë¬¸ì œì : [ì—†ìœ¼ë©´ 'ì—†ìŒ', ìˆìœ¼ë©´ êµ¬ì²´ì  ì„¤ëª…]"
        )

        try:
            response = await model_router.complete(
                model_name=review_model,
                messages=[
                    {"role": "system", "content": "ë³´ê³ ì„œ í’ˆì§ˆ ê²€ìˆ˜ê´€. ê°„ê²°í•˜ê²Œ íŒì •."},
                    {"role": "user", "content": prompt},
                ],
                temperature=0.0,
                agent_id=reviewer_id or "quality_gate",
            )
            return self._parse_review(response.content, reviewer_id)
        except Exception as e:
            logger.warning("LLM í’ˆì§ˆ ê²€ìˆ˜ ì‹¤íŒ¨: %s", e)
            return ReviewResult(passed=True, score=70, issues=["LLM ê²€ìˆ˜ ë¶ˆê°€"])

    def _parse_review(self, response: str, reviewer_id: str) -> ReviewResult:
        """LLM ê²€ìˆ˜ ì‘ë‹µ íŒŒì‹± (ë ˆê±°ì‹œ)."""
        lines = response.strip().split("\n")
        score = 70
        passed = True
        issues = []

        for line in lines:
            line = line.strip()
            if line.startswith("ì ìˆ˜:"):
                try:
                    score = int(line.split(":")[1].strip().split()[0])
                except (ValueError, IndexError):
                    pass
            elif line.startswith("í†µê³¼:"):
                val = line.split(":")[1].strip()
                passed = val.startswith("ì˜ˆ")
            elif line.startswith("ë¬¸ì œì :"):
                issue_text = line.split(":", 1)[1].strip()
                if issue_text and issue_text != "ì—†ìŒ":
                    issues.append(issue_text)

        if score < 50:
            passed = False

        rejection_reason = ""
        if not passed:
            rejection_reason = " / ".join(issues) if issues else "í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬"

        return ReviewResult(
            passed=passed,
            score=score,
            issues=issues,
            reviewer_id=reviewer_id,
            rejection_reason=rejection_reason,
        )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # í†µê³„ ê¸°ë¡
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def record_review(
        self,
        review: ReviewResult | HybridReviewResult,
        reviewer_id: str,
        target_agent_id: str,
        task_desc: str,
        is_retry: bool = False,
    ) -> None:
        """ê²€ìˆ˜ ê²°ê³¼ë¥¼ í†µê³„ì— ê¸°ë¡."""
        self.stats.total_reviewed += 1

        if review.passed:
            self.stats.total_passed += 1
            if is_retry:
                self.stats.total_retry_passed += 1
        else:
            self.stats.total_rejected += 1
            self.stats.rejections_by_agent[target_agent_id] = (
                self.stats.rejections_by_agent.get(target_agent_id, 0) + 1
            )
            self.stats.rejections_by_reviewer[reviewer_id] = (
                self.stats.rejections_by_reviewer.get(reviewer_id, 0) + 1
            )
            # rejection_reason ì¶”ì¶œ
            if isinstance(review, HybridReviewResult):
                reason = " / ".join(review.rejection_reasons) if review.rejection_reasons else "í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬"
                score_val = review.weighted_average
            else:
                reason = review.rejection_reason
                score_val = review.score

            self.stats.recent_rejections.append({
                "reviewer": reviewer_id,
                "target": target_agent_id,
                "task": task_desc[:80],
                "score": score_val,
                "reason": reason,
            })
            if len(self.stats.recent_rejections) > 20:
                self.stats.recent_rejections = self.stats.recent_rejections[-20:]

        if is_retry:
            self.stats.total_retried += 1
